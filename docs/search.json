[
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "블로그 개발 방향",
    "section": "",
    "text": "우선 기본적인 구조 파악하기. 새로운 글을 작성하는 방식이나 정렬 순서. 내가 만든 css나 js 파일도 적용 시킬 수 있나? -> 공식 문서 참고할 것 https://quarto.org/docs/websites/website-listings.html https://quarto.org/docs/websites/website-blog.html\n포스팅 된 글들의 정렬 순서가 궁금함. 실험해본 결과 ‘date’ 날짜에 따라 우선 정렬을 하는 것 까지는 확인했는데 같은 날짜의 포스팅의 경우 어떤 우선순위로 정렬하는지 알 수 없었음. 문서를 작성한 순서도, 폴더 이름도, 제목도 아님… 뭐지? -> 공식 문서 찾음. 수정할 예정\n적절한 카테고리 설정도 필요해보임. 우선 필요한 카테고리들을 정리해보자면… ML/DL, 코딩테스트, 생각 정리, 기타"
  },
  {
    "objectID": "posts/bigleader-01-03/bigleader_01_week01_03.html",
    "href": "posts/bigleader-01-03/bigleader_01_week01_03.html",
    "title": "빅리더 1주차 02일 지도시각화",
    "section": "",
    "text": "#fig2.show(renderer='colab')\n#fig2.show()\n\n### Folium 라이브러리 - 지도 활용\n\nimport folium\n# 서울 지도 만들기\nseoul_map = folium.Map(location=[37.55,126.98], zoom_start=12)\nseoul_map2 = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', zoom_start=12)\nseoul_map3 = folium.Map(location=[37.55,126.98], tiles='Stamen Toner', zoom_start=15)\n# 지도를 HTML 파일로 저장하기 seoul_map.save('./seoul.html') seoul_map2.save('./seoul2.html') seoul_map2.save('./seoul2.html')\n\n\nimport folium\nm = folium.Map( location=[35.1804486,128.5523014], zoom_start=15)\nfolium.Marker( location=[35.1804486,128.5523014], popup='University of kyungnam',\n              icon=folium.Icon(color='red',icon='star')).add_to(m)\n\nm.save('kyungnam.html')\n\n\n지도에 마커 표시하기\n\nimport pandas as pd\n\n\ndf = pd.read_excel('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/서울지역 대학교 위치.xlsx')\ndf.set_index('학교명', inplace=True)\n# 서울 지도 만들기\nseoul_map = folium.Map(location=[37.55,126.98],\ntiles='Stamen Terrain', zoom_start=12)\n# 대학교 위치정보를 Marker로 표시\n\n#fig2.show(renderer='colab')\n\nfor name, lat, lng in zip(df.index, df.위도, df.경도):\n    folium.Marker([lat, lng], popup=name).add_to(seoul_map)\n# 지도보기\nseoul_map\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n지도에 마커 표시하기2\n\n# 서울 지도 만들기\nseoul_map = folium.Map(location=[37.55,126.98],\ntiles='Stamen Terrain', zoom_start=12)\n# 대학교 위치정보를 CircleMarker로 표시\nfor name, lat, lng in zip(df.index, df.위도, df.경도):\n    folium.CircleMarker([lat, lng],\n    radius=10, # 원의 반지름\n    ).add_to(seoul_map)\n    color='brown', # 원의 둘레 색상 fill=True,\n    fill_color='coral',# 원을 채우는 색\n    fill_opacity=0.7, # 투명도 popup=name\n# 지도보기\nseoul_map\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\nfrom folium.plugins import MarkerCluster\n\ndf = pd.read_excel('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/서울지역 대학교 위치.xlsx')\ndf.set_index('학교명', inplace=True)\n# 서울 지도 만들기\nseoul_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', zoom_start=12)\n\nmarker_cluster = MarkerCluster().add_to(seoul_map)\n\n# 대학교 위치정보를 Marker로 표시\nfor name, lat, lng in zip(df.index, df.위도, df.경도):\n    folium.Marker([lat, lng], icon=folium.Icon(color='red',icon='ok'), popup=name).add_to(marker_cluster)\n# 지도보기\nmarker_cluster\n\n<folium.plugins.marker_cluster.MarkerCluster at 0x7f3f845a4490>\n\n\n\ndf = pd.read_excel('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/서울지역 대학교 위치.xlsx')\n\n\nseoul_map = folium.Map(location=[37.55,126.98], tiles='Stamen Terrain', zoom_start=12)\nfolium.CircleMarker(\nlocation = df.loc[0, ['위도', '경도']],\n                     tooltip = df.loc[0, '학교명'],\n                     radius = 100).add_to(seoul_map)\nseoul_map\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nPolyLine\n\nlines = df[['위도', '경도']].values[:5].tolist()\nseoul_map = folium.Map(location=[37.55,126.98],\n                        tiles='Stamen Terrain', zoom_start=12)\nfolium.PolyLine( locations = lines,\n                #fill=True, # 안쪽을 칠해줌.\n                tooltip = 'PolyLine').add_to(seoul_map)\nseoul_map\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\n영역 구분(choropleth map) 표시\n\nimport json\n\nfile_path = '/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/경기도인구데이터.xlsx'\ndf = pd.read_excel(file_path, index_col='구분')\ndf.columns = df.columns.map(str)\ngeo_path = '/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/경기도행정구역경계.json'\ngeo_data = json.load(open(geo_path, encoding='utf-8'))\ng_map = folium.Map(location=[37.5502,126.982], tiles='Stamen Terrain', zoom_start=9)\n\n# 출력할 연도 선택 (2007 ~ 2017년 중에서 선택)\nyear = '2017'\n# Choropleth 클래스로 단계구분도 표시하기\nfolium.Choropleth(geo_data=geo_data, # 지도 경계\ndata = df[year], # 표시하려는 데이터\ncolumns = [df.index,\n           df[year]], # 열 지정\n            fill_color=\"YlOrRd\",\n            fill_opacity=0.7,\n            line_opacity=0.3,\n            threshold_scale=[10000, 100000, 300000, 500000, 700000],\n            key_on='feature.properties.name',\n            legend_name='인구수').add_to(g_map)\n# 지도를 HTML 파일로 저장하기\n#g_map.save('./gyonggi_population_' + year + '.html')\ng_map\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n\nChoroplethmap 주요속성\n\n'''\nfolium.Choropleth(\n    geo_data = \"지도 데이터 파일 경로 (.geojson, geopandas.DataFrame)\" data = \"시각화 하고자 하는 데이터파일. (pandas.DataFrame)\"\n    columns = (지도 데이터와 매핑할 값, 시각화 하고자하는 변수),\n    key_on = \"feature.데이터 파일과 매핑할 값\",\n    fill_color = \"시각화에 쓰일 색상\",\n    legend_name = \"칼라 범주 이름\"\n).add_to(m)\n'''"
  },
  {
    "objectID": "posts/bigleader-01-04/bigleader_01_week01_04.html",
    "href": "posts/bigleader-01-04/bigleader_01_week01_04.html",
    "title": "빅리더 1주차 02일 크롤링",
    "section": "",
    "text": "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\n\n\n#!pip install selenium\n\n\ndriver=webdriver.Chrome()\n\ndriver.get(\"http://www.opinet.co.kr\")\ndriver.get(\"http://www.opinet.co.kr/searRgSelect.do\")\n\n\n# 시/도 입력을 위한 xpath 확인\narea = driver.find_element(By.XPATH, '//*[@id=\"SIDO_NM0\"]')\n\n\n\narea.send_keys('서울')\n\n\n# 구/데이터 입력을 위한 xpath 확인\ngu_list_raw = driver.find_element(By.XPATH, '//*[@id=\"SIGUNGU_NM0\"]')\n# 구 리스트 확인 위해 find_elements_by_tag_name으로 option 태그 검색\ngu_list = gu_list_raw.find_elements(By.TAG_NAME, 'option')\n\n\n서울 주유소 가격 정보 비교\n\n#value 속성을 이용하여 구 리스트 획득\ngu_names = [option.get_attribute('value') for option in gu_list]\ngu_names.remove('')\ngu_names\n\n['강남구',\n '강동구',\n '강북구',\n '강서구',\n '관악구',\n '광진구',\n '구로구',\n '금천구',\n '노원구',\n '도봉구',\n '동대문구',\n '동작구',\n '마포구',\n '서대문구',\n '서초구',\n '성동구',\n '성북구',\n '송파구',\n '양천구',\n '영등포구',\n '용산구',\n '은평구',\n '종로구',\n '중구',\n '중랑구']\n\n\n\n#gu_names에서 리스트 첫번째 값 입력하여 테스트 진행\nelement = driver.find_element(By.ID, 'SIGUNGU_NM0')\nelement.send_keys(gu_names[0])\n\n\n#조회버튼의 Xpath를 찾아서 클릭\nxpath ='''//*[@id=\"searRgSelect\"]/span'''\nelement_sel_gu = driver.find_element(By.XPATH, xpath).click()\n\n\n#엑셀 저장 버튼 클릭하여 엑셀 내용 저장 테스트\nxpath = '''//*[@id=\"glopopd_excel\"]/span'''\nelement_get_excel = driver.find_element(By.XPATH, xpath).click()\n\n\n\n구 별 주유소 가격 정리\n\n!pip install tqdm\n\nCollecting tqdm\n  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.1/77.1 kB 1.1 MB/s eta 0:00:00a 0:00:01\nInstalling collected packages: tqdm\nSuccessfully installed tqdm-4.65.0\n\n\n\nimport time\nfrom tqdm import tqdm_notebook\n# 반복문을 이용하여 모든 구 엑셀파일 다운로드 진행\nfor gu in tqdm_notebook(gu_names):\n    element = driver.find_element(By.ID, 'SIGUNGU_NM0')\n    element.send_keys(gu)\n    time.sleep(2)# 데이터 획득 위한 지연 시간\n    xpath ='''//*[@id=\"searRgSelect\"]/span'''\n    element_sel_gu = driver.find_element(By.XPATH, xpath).click()\n    time.sleep(1)\n    xpath = '''//*[@id=\"glopopd_excel\"]/span'''\n    element_get_excel = driver.find_element(By.XPATH, xpath).click()\n\n    time.sleep(1)\n\n/var/folders/cf/_pxyh8n15f71szpgtc0c4m6w0000gn/T/ipykernel_5366/3591970159.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for gu in tqdm_notebook(gu_names):\n\n\n\n\n\n\n\n구별 주유 가격 정리\n\n#!pip install pandas\n\n\nimport pandas as pd\nfrom glob import glob\n\n\n# station_files 변수에 각 엑셀 파일의 경로와 이름을 리스트로 저장\nstations_files = glob('data/지역*.xls')\nstations_files\n\n['data/지역_위치별(주유소) (9).xls',\n 'data/지역_위치별(주유소) (12).xls',\n 'data/지역_위치별(주유소) (5).xls',\n 'data/지역_위치별(주유소) (4).xls',\n 'data/지역_위치별(주유소) (13).xls',\n 'data/지역_위치별(주유소) (8).xls',\n 'data/지역_위치별(주유소) (3).xls',\n 'data/지역_위치별(주유소) (18).xls',\n 'data/지역_위치별(주유소) (22).xls',\n 'data/지역_위치별(주유소) (14).xls',\n 'data/지역_위치별(주유소) (15).xls',\n 'data/지역_위치별(주유소) (23).xls',\n 'data/지역_위치별(주유소) (19).xls',\n 'data/지역_위치별(주유소) (2).xls',\n 'data/지역_위치별(주유소) (20).xls',\n 'data/지역_위치별(주유소) (1).xls',\n 'data/지역_위치별(주유소) (16).xls',\n 'data/지역_위치별(주유소) (17).xls',\n 'data/지역_위치별(주유소) (21).xls',\n 'data/지역_위치별(주유소) (10).xls',\n 'data/지역_위치별(주유소).xls',\n 'data/지역_위치별(주유소) (7).xls',\n 'data/지역_위치별(주유소) (6).xls',\n 'data/지역_위치별(주유소) (11).xls']\n\n\n\n#!pip install xlrd\n\n\n# concat 명령으로 합쳐본다.\ntmp_raw = []\n\nfor file_name in stations_files:\n    tmp = pd.read_excel(file_name, header=2)\n    tmp_raw.append(tmp)\n\nstation_raw = pd.concat(tmp_raw)\n\n\nstation_raw.info()\n\n<class 'pandas.core.frame.DataFrame'>\nIndex: 444 entries, 0 to 10\nData columns (total 10 columns):\n #   Column  Non-Null Count  Dtype \n---  ------  --------------  ----- \n 0   지역      444 non-null    object\n 1   상호      444 non-null    object\n 2   주소      444 non-null    object\n 3   상표      444 non-null    object\n 4   전화번호    444 non-null    object\n 5   셀프여부    444 non-null    object\n 6   고급휘발유   444 non-null    object\n 7   휘발유     444 non-null    int64 \n 8   경유      444 non-null    int64 \n 9   실내등유    444 non-null    object\ndtypes: int64(2), object(8)\nmemory usage: 38.2+ KB\n\n\n\n\n구별 주유 가격 정리\n\nstation_raw.head()\n\n\n\n\n\n  \n    \n      \n      지역\n      상호\n      주소\n      상표\n      전화번호\n      셀프여부\n      고급휘발유\n      휘발유\n      경유\n      실내등유\n    \n  \n  \n    \n      0\n      서울특별시\n      (주)자연에너지 국민주유소\n      서울 동대문구 답십리로 223 (답십리동)\n      현대오일뱅크\n      02-2216-7155\n      Y\n      1740\n      1509\n      1329\n      -\n    \n    \n      1\n      서울특별시\n      풍한주유소\n      서울 동대문구 안암로 168\n      SK에너지\n      02-924-5189\n      N\n      -\n      1515\n      1335\n      -\n    \n    \n      2\n      서울특별시\n      열린주유소\n      서울 동대문구 한천로 263 (휘경동)\n      S-OIL\n      02-2215-6543\n      N\n      -\n      1527\n      1329\n      -\n    \n    \n      3\n      서울특별시\n      재정제2주유소\n      서울 동대문구 사가정로 90 (전농동)\n      현대오일뱅크\n      02-2249-6682\n      Y\n      -\n      1530\n      1350\n      -\n    \n    \n      4\n      서울특별시\n      재정주유소\n      서울 동대문구 전농로 121 (전농동)\n      현대오일뱅크\n      070-8256-4617\n      Y\n      1789\n      1530\n      1350\n      -\n    \n  \n\n\n\n\n\n# 휘발유 데이터 저장.\nstations = pd.DataFrame({'Oil_store': station_raw['상호'],\n                        '주소': station_raw['주소'],\n                        '가격': station_raw['휘발유'],\n                        '셀프': station_raw['셀프여부'],\n                        '상표': station_raw['상표']\n})\nstations.head()\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n    \n  \n  \n    \n      0\n      (주)자연에너지 국민주유소\n      서울 동대문구 답십리로 223 (답십리동)\n      1509\n      Y\n      현대오일뱅크\n    \n    \n      1\n      풍한주유소\n      서울 동대문구 안암로 168\n      1515\n      N\n      SK에너지\n    \n    \n      2\n      열린주유소\n      서울 동대문구 한천로 263 (휘경동)\n      1527\n      N\n      S-OIL\n    \n    \n      3\n      재정제2주유소\n      서울 동대문구 사가정로 90 (전농동)\n      1530\n      Y\n      현대오일뱅크\n    \n    \n      4\n      재정주유소\n      서울 동대문구 전농로 121 (전농동)\n      1530\n      Y\n      현대오일뱅크\n    \n  \n\n\n\n\n\n#구 이름만 추출\nstations['구'] = [eachAddress.split()[1]for eachAddress in stations['주소']]\nstations.head()\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n    \n      0\n      (주)자연에너지 국민주유소\n      서울 동대문구 답십리로 223 (답십리동)\n      1509\n      Y\n      현대오일뱅크\n      동대문구\n    \n    \n      1\n      풍한주유소\n      서울 동대문구 안암로 168\n      1515\n      N\n      SK에너지\n      동대문구\n    \n    \n      2\n      열린주유소\n      서울 동대문구 한천로 263 (휘경동)\n      1527\n      N\n      S-OIL\n      동대문구\n    \n    \n      3\n      재정제2주유소\n      서울 동대문구 사가정로 90 (전농동)\n      1530\n      Y\n      현대오일뱅크\n      동대문구\n    \n    \n      4\n      재정주유소\n      서울 동대문구 전농로 121 (전농동)\n      1530\n      Y\n      현대오일뱅크\n      동대문구\n    \n  \n\n\n\n\n\n# unique() 이용해서 데이터 검사 수행\nstations['구'].unique()\n\narray(['동대문구', '서대문구', '관악구', '강북구', '서초구', '도봉구', '강동구', '영등포구', '중구',\n       '성동구', '성북구', '중랑구', '용산구', '강남구', '은평구', '송파구', '양천구', '종로구',\n       '동작구', '노원구', '금천구', '마포구'], dtype=object)\n\n\n\nstations['가격'].unique() #가격에 '-'가 있음\n\narray([1509, 1515, 1527, 1530, 1555, 1565, 1575, 1578, 1595, 1597, 1599,\n       1640, 1648, 1534, 1539, 1544, 1559, 1569, 1579, 1588, 1624, 1564,\n       1566, 1589, 1594, 1598, 1604, 1665, 1696, 1699, 1718, 1518, 1519,\n       1537, 1548, 1549, 1627, 1567, 1585, 1586, 1587, 1596, 1603, 1609,\n       1614, 1619, 1623, 1625, 1626, 1628, 1655, 1659, 1793, 1938, 1989,\n       1491, 1524, 1529, 1547, 1561, 1563, 1568, 1543, 1584, 1605, 1629,\n       1630, 1631, 1695, 1799, 1948, 1513, 1540, 1545, 1570, 1593, 1895,\n       1898, 2050, 2110, 2199, 1798, 1819, 1998, 2246, 2359, 2595, 1535,\n       1693, 1698, 1745, 1759, 1893, 1945, 2170, 1507, 1554, 1574, 1577,\n       1995, 1999, 2019, 2021, 2195, 2217, 2585, 1582, 1615, 1660, 1684,\n       1687, 1694, 1697, 1758, 1840, 1908, 1939, 1940, 2089, 2128, 2176,\n       2188, 2275, 2355, 2398, 2578, 1495, 1557, 1573, 1610, 1613, 1678,\n       1838, 1497, 1512, 1514, 1553, 1592, 1617, 1730, 2075, 2168, 2240,\n       2250, 1560, 1765, 1525, 1658, 1638, 1639])\n\n\n\n\n서울 주유소 가격 비교\n\n#가격에 '-' 값만 추출\nstations[stations['가격']=='-']\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n  \n\n\n\n\n\n# '-' 문자가 포함된 데이터 제외\nstations = stations[stations['가격'] != '-']\nstations.head()\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n    \n      0\n      (주)자연에너지 국민주유소\n      서울 동대문구 답십리로 223 (답십리동)\n      1509\n      Y\n      현대오일뱅크\n      동대문구\n    \n    \n      1\n      풍한주유소\n      서울 동대문구 안암로 168\n      1515\n      N\n      SK에너지\n      동대문구\n    \n    \n      2\n      열린주유소\n      서울 동대문구 한천로 263 (휘경동)\n      1527\n      N\n      S-OIL\n      동대문구\n    \n    \n      3\n      재정제2주유소\n      서울 동대문구 사가정로 90 (전농동)\n      1530\n      Y\n      현대오일뱅크\n      동대문구\n    \n    \n      4\n      재정주유소\n      서울 동대문구 전농로 121 (전농동)\n      1530\n      Y\n      현대오일뱅크\n      동대문구\n    \n  \n\n\n\n\n\n\n서울 주유소 가격 정보 비교\n\n# 가격 float 형 변환.\nstations['가격'] = [float(value) for value in stations['가격']]\n\n\n# reset_index 이용하여 index 재정의\nstations.reset_index(inplace=True)\ndel stations['index']# 기존 인덱스 삭제\n\n\nstations.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 444 entries, 0 to 443\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Oil_store  444 non-null    object \n 1   주소         444 non-null    object \n 2   가격         444 non-null    float64\n 3   셀프         444 non-null    object \n 4   상표         444 non-null    object \n 5   구          444 non-null    object \ndtypes: float64(1), object(5)\nmemory usage: 20.9+ KB\n\n\n\n\n시각화\n\n!pip install matplotlib\n!pip install seaborn\n\n\n# 한글문제 해결\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport platform\npath = 'c:/Windows/Fonts/malgun.ttf'\nfrom matplotlib import font_manager, rc\n\nif platform.system() == 'Darwin':\n    rc('font', family = 'AppleGothic')\nelif platform.system() == 'Windows':\n    font_name = font_manager.FontProperties(fname=path).get_name()\n    rc('font', family=font_name)\nelse:\n    print('Unknown system... sorry~~~')\n\n\n\n시각화\n\nstations.boxplot(column='가격', by='셀프', figsize=(12,8))\n\n<Axes: title={'center': '가격'}, xlabel='셀프'>\n\n\n\n\n\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x='상표', y='가격', hue='셀프', data=stations, palette='Set3')\nplt.show()\n\n\n\n\n\nplt.figure(figsize=(12,8))\nsns.boxplot(x='상표', y='가격', data=stations, palette='Set3')\nsns.swarmplot(x='상표', y='가격', data=stations, color='.6')\nplt.show()\n\n/Users/siina/anaconda3/envs/selenium/lib/python3.11/site-packages/seaborn/categorical.py:3544: UserWarning: 12.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\n#!pip install folium\n#!pip install googlemaps\n\n\nimport json\nimport folium\nimport googlemaps\n# 이제 서울시에서 가장 주유 가격이 비싼 주유소\nstations.sort_values(by='가격', ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n    \n      155\n      서남주유소\n      서울 중구 통일로 30\n      2595.0\n      N\n      SK에너지\n      중구\n    \n    \n      219\n      서계주유소\n      서울 용산구  청파로 367 (청파동)\n      2585.0\n      N\n      GS칼텍스\n      용산구\n    \n    \n      303\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n    \n    \n      406\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n    \n    \n      253\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n    \n    \n      302\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n    \n    \n      252\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n    \n    \n      405\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n    \n    \n      154\n      필동주유소\n      서울 중구 퇴계로 196 (필동2가)\n      2359.0\n      N\n      GS칼텍스\n      중구\n    \n    \n      251\n      SK논현주유소\n      서울 강남구 논현로 747 (논현동)\n      2355.0\n      N\n      SK에너지\n      강남구\n    \n  \n\n\n\n\n\n# 서울시에서 가장 주유 가격이 싼 주유소\nstations.sort_values(by='가격', ascending=True).head(10)\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n    \n      89\n      이케이에너지(주) 도선주유소\n      서울 도봉구 도봉로 941 (도봉동)\n      1491.0\n      Y\n      현대오일뱅크\n      도봉구\n    \n    \n      254\n      (주)디오티디 은평유니콘주유소\n      서울 은평구 통일로 1151 (진관동)\n      1495.0\n      N\n      현대오일뱅크\n      은평구\n    \n    \n      334\n      플라트(주)서호주유소\n      서울 양천구 남부순환로 317\n      1497.0\n      Y\n      GS칼텍스\n      양천구\n    \n    \n      333\n      현대주유소\n      서울 양천구 남부순환로 372 (신월동)\n      1497.0\n      Y\n      S-OIL\n      양천구\n    \n    \n      332\n      가로공원주유소\n      서울 양천구 가로공원로 165 (신월동)\n      1497.0\n      N\n      SK에너지\n      양천구\n    \n    \n      172\n      원천주유소\n      서울 성북구 돌곶이로 142 (장위동)\n      1507.0\n      N\n      알뜰주유소\n      성북구\n    \n    \n      49\n      (주)서울에너지 시민주유소\n      서울 강북구 인수봉로 185 (수유동)\n      1509.0\n      Y\n      현대오일뱅크\n      강북구\n    \n    \n      59\n      만남의광장주유소\n      서울 서초구 양재대로12길 73-71\n      1509.0\n      Y\n      알뜰(ex)\n      서초구\n    \n    \n      335\n      (주)타이거오일 신정주유소\n      서울 양천구 중앙로 226 (신정동)\n      1509.0\n      Y\n      SK에너지\n      양천구\n    \n    \n      90\n      (주)자연에너지 햇살주유소\n      서울 도봉구 방학로 142 (방학동)\n      1509.0\n      Y\n      현대오일뱅크\n      도봉구\n    \n  \n\n\n\n\n\n# pivot_table을 이용해서 구별 가격 정보로 변경하고 가격 평균값 정리.\nimport numpy as np\ngu_data = pd.pivot_table(stations, index=['구'], values=['가격'], aggfunc=np.mean)\ngu_data.head()\n\n\n\n\n\n  \n    \n      \n      가격\n    \n    \n      구\n      \n    \n  \n  \n    \n      강남구\n      1837.647059\n    \n    \n      강동구\n      1652.571429\n    \n    \n      강북구\n      1533.333333\n    \n    \n      관악구\n      1616.285714\n    \n    \n      금천구\n      1588.454545\n    \n  \n\n\n\n\n\n#서울시 구별 정보에 대해 지도로 표현\ngeo_path = 'data/skorea_municipalities_geo_simple.json'\ngeo_str = json.load(open(geo_path, encoding='utf-8'))\nmap = folium.Map(location=[37.5502, 126.982], zoom_start=10.5, tiles='Stamen Toner')\nmap.choropleth(geo_data = geo_str, data = gu_data,\n                columns=[gu_data.index, '가격'], fill_color='PuRd', #PuRd, YlGnBu\n                key_on='feature.id')\nmap\n\n/Users/siina/anaconda3/envs/selenium/lib/python3.11/site-packages/folium/folium.py:465: FutureWarning: The choropleth  method has been deprecated. Instead use the new Choropleth class, which has the same arguments. See the example notebook 'GeoJSON_and_choropleth' for how to do this.\n  warnings.warn(\n\n\nMake this Notebook Trusted to load map: File -> Trust Notebook\n\n\n\n# 주유 가격 상위 10개 주소 oil_price_top10 저장.\noil_price_top10 = stations.sort_values(by='가격', ascending=False).head(10)\noil_price_top10\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n    \n      155\n      서남주유소\n      서울 중구 통일로 30\n      2595.0\n      N\n      SK에너지\n      중구\n    \n    \n      219\n      서계주유소\n      서울 용산구  청파로 367 (청파동)\n      2585.0\n      N\n      GS칼텍스\n      용산구\n    \n    \n      303\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n    \n    \n      406\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n    \n    \n      253\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n    \n    \n      302\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n    \n    \n      252\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n    \n    \n      405\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n    \n    \n      154\n      필동주유소\n      서울 중구 퇴계로 196 (필동2가)\n      2359.0\n      N\n      GS칼텍스\n      중구\n    \n    \n      251\n      SK논현주유소\n      서울 강남구 논현로 747 (논현동)\n      2355.0\n      N\n      SK에너지\n      강남구\n    \n  \n\n\n\n\n\n# 하위 10개 oil_price_bottom10 저장\noil_price_bottom10 = stations.sort_values(by='가격',ascending=True).head(10)\noil_price_bottom10\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n    \n  \n  \n    \n      89\n      이케이에너지(주) 도선주유소\n      서울 도봉구 도봉로 941 (도봉동)\n      1491.0\n      Y\n      현대오일뱅크\n      도봉구\n    \n    \n      254\n      (주)디오티디 은평유니콘주유소\n      서울 은평구 통일로 1151 (진관동)\n      1495.0\n      N\n      현대오일뱅크\n      은평구\n    \n    \n      334\n      플라트(주)서호주유소\n      서울 양천구 남부순환로 317\n      1497.0\n      Y\n      GS칼텍스\n      양천구\n    \n    \n      333\n      현대주유소\n      서울 양천구 남부순환로 372 (신월동)\n      1497.0\n      Y\n      S-OIL\n      양천구\n    \n    \n      332\n      가로공원주유소\n      서울 양천구 가로공원로 165 (신월동)\n      1497.0\n      N\n      SK에너지\n      양천구\n    \n    \n      172\n      원천주유소\n      서울 성북구 돌곶이로 142 (장위동)\n      1507.0\n      N\n      알뜰주유소\n      성북구\n    \n    \n      49\n      (주)서울에너지 시민주유소\n      서울 강북구 인수봉로 185 (수유동)\n      1509.0\n      Y\n      현대오일뱅크\n      강북구\n    \n    \n      59\n      만남의광장주유소\n      서울 서초구 양재대로12길 73-71\n      1509.0\n      Y\n      알뜰(ex)\n      서초구\n    \n    \n      335\n      (주)타이거오일 신정주유소\n      서울 양천구 중앙로 226 (신정동)\n      1509.0\n      Y\n      SK에너지\n      양천구\n    \n    \n      90\n      (주)자연에너지 햇살주유소\n      서울 도봉구 방학로 142 (방학동)\n      1509.0\n      Y\n      현대오일뱅크\n      도봉구\n    \n  \n\n\n\n\n\n\ngoogle map api 이용\n\n# google maps API용 개인 key 입력\ngmap_key = '*********'\ngmaps = googlemaps.Client(key=gmap_key)\n\n\nfrom tqdm import tqdm_notebook\n\nlat=[]\nlng=[]\n\nfor n in tqdm_notebook(oil_price_top10.index):\n    try:\n        tmp_add = str(oil_price_top10['주소'][n]).split('(')[0]\n        tmp_map = gmaps.geocode(tmp_add)\n\n        tmp_loc = tmp_map[0].get('geometry')\n        lat.append(tmp_loc['location']['lat'])\n        lng.append(tmp_loc['location']['lng'])\n\n    except:\n        lat.append(np.nan)\n        lng.append(np.nan)\n        print('Here is nan !')\n\noil_price_top10['lat']=lat\noil_price_top10['lng']=lng\noil_price_top10\n\n/var/folders/cf/_pxyh8n15f71szpgtc0c4m6w0000gn/T/ipykernel_5366/2810043115.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for n in tqdm_notebook(oil_price_top10.index):\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n      lat\n      lng\n    \n  \n  \n    \n      155\n      서남주유소\n      서울 중구 통일로 30\n      2595.0\n      N\n      SK에너지\n      중구\n      37.558375\n      126.972094\n    \n    \n      219\n      서계주유소\n      서울 용산구  청파로 367 (청파동)\n      2585.0\n      N\n      GS칼텍스\n      용산구\n      37.552329\n      126.968946\n    \n    \n      303\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n      37.511521\n      127.047172\n    \n    \n      406\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n      37.511521\n      127.047172\n    \n    \n      253\n      (주)만정에너지 삼보주유소\n      서울 강남구 봉은사로 433 (삼성동)\n      2578.0\n      N\n      GS칼텍스\n      강남구\n      37.511521\n      127.047172\n    \n    \n      302\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n      37.517647\n      127.035735\n    \n    \n      252\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n      37.517647\n      127.035735\n    \n    \n      405\n      (주)제이제이네트웍스 제이제이주유소\n      서울 강남구 언주로 716\n      2398.0\n      N\n      현대오일뱅크\n      강남구\n      37.517647\n      127.035735\n    \n    \n      154\n      필동주유소\n      서울 중구 퇴계로 196 (필동2가)\n      2359.0\n      N\n      GS칼텍스\n      중구\n      37.560912\n      126.993713\n    \n    \n      251\n      SK논현주유소\n      서울 강남구 논현로 747 (논현동)\n      2355.0\n      N\n      SK에너지\n      강남구\n      37.518637\n      127.028238\n    \n  \n\n\n\n\n\nlat=[]\nlng=[]\nfor n in tqdm_notebook(oil_price_bottom10.index):\n    try:\n        tmp_add = str(oil_price_bottom10['주소'][n]).split('(')[0]\n        tmp_map = gmaps.geocode(tmp_add)\n        tmp_loc = tmp_map[0].get('geometry')\n        lat.append(tmp_loc['location']['lat'])\n        lng.append(tmp_loc['location']['lng'])\n    except:\n        lat.append(np.nan)\n        lng.append(np.nan)\n        print('Here is nan !')\noil_price_bottom10['lat']=lat\noil_price_bottom10['lng']=lng\noil_price_bottom10\n\n/var/folders/cf/_pxyh8n15f71szpgtc0c4m6w0000gn/T/ipykernel_5366/1722742016.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n  for n in tqdm_notebook(oil_price_bottom10.index):\n\n\n\n\n\n\n\n\n\n  \n    \n      \n      Oil_store\n      주소\n      가격\n      셀프\n      상표\n      구\n      lat\n      lng\n    \n  \n  \n    \n      89\n      이케이에너지(주) 도선주유소\n      서울 도봉구 도봉로 941 (도봉동)\n      1491.0\n      Y\n      현대오일뱅크\n      도봉구\n      37.688431\n      127.045337\n    \n    \n      254\n      (주)디오티디 은평유니콘주유소\n      서울 은평구 통일로 1151 (진관동)\n      1495.0\n      N\n      현대오일뱅크\n      은평구\n      37.643226\n      126.920007\n    \n    \n      334\n      플라트(주)서호주유소\n      서울 양천구 남부순환로 317\n      1497.0\n      Y\n      GS칼텍스\n      양천구\n      37.538661\n      126.826837\n    \n    \n      333\n      현대주유소\n      서울 양천구 남부순환로 372 (신월동)\n      1497.0\n      Y\n      S-OIL\n      양천구\n      37.533873\n      126.829262\n    \n    \n      332\n      가로공원주유소\n      서울 양천구 가로공원로 165 (신월동)\n      1497.0\n      N\n      SK에너지\n      양천구\n      37.537016\n      126.834285\n    \n    \n      172\n      원천주유소\n      서울 성북구 돌곶이로 142 (장위동)\n      1507.0\n      N\n      알뜰주유소\n      성북구\n      37.614895\n      127.052732\n    \n    \n      49\n      (주)서울에너지 시민주유소\n      서울 강북구 인수봉로 185 (수유동)\n      1509.0\n      Y\n      현대오일뱅크\n      강북구\n      37.636202\n      127.012822\n    \n    \n      59\n      만남의광장주유소\n      서울 서초구 양재대로12길 73-71\n      1509.0\n      Y\n      알뜰(ex)\n      서초구\n      37.460084\n      127.042169\n    \n    \n      335\n      (주)타이거오일 신정주유소\n      서울 양천구 중앙로 226 (신정동)\n      1509.0\n      Y\n      SK에너지\n      양천구\n      37.516802\n      126.854664\n    \n    \n      90\n      (주)자연에너지 햇살주유소\n      서울 도봉구 방학로 142 (방학동)\n      1509.0\n      Y\n      현대오일뱅크\n      도봉구\n      37.663002\n      127.036333\n    \n  \n\n\n\n\n\nmap = folium.Map(location=[37.5202, 126.975], zoom_start=10.5)\n\nfor n in oil_price_top10.index:\n    if pd.notnull(oil_price_top10['lat'][n]):\n        folium.CircleMarker([oil_price_top10['lat'][n], oil_price_top10['lng'][n]],\n                            radius=15, color='#CD3181', fill_color='#CD3181').add_to(map)\n    for n in oil_price_bottom10.index:\n        if pd.notnull(oil_price_bottom10['lat'][n]):\n            folium.CircleMarker([oil_price_bottom10['lat'][n], oil_price_bottom10['lng'][n]],\n                                radius=15, color='#3186cc', fill_color='#3186cc').add_to(map)\n\nmap\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "posts/dl-post-01/index.html",
    "href": "posts/dl-post-01/index.html",
    "title": "크롬 공룡 게임 AI",
    "section": "",
    "text": "Genetic Dino    https://github.com/kairess/genetic_dino\n크롬 공룡 게임 고득점을 위한 AI 알고리즘 테스트.\n우선 정답에 가까운 유전알고리즘 코드를 가져왔음.\n이후 수업 들으면서 알고리즘 부분만 잘 수정해보는게 목표.\n— 7월 10일 추가사항 — ML/DL 스터디그룹 친구들끼리 이야기 하다 나온 내용.\n나 : 공룡 게임 AI 만들어서 우리끼리 대회 열면 재밌을거 같지 않아?\n친구들 : 오 정말 미친생각인데? 바로 해보자!\n알고리즘 개발 기한은 24일까지. Start."
  },
  {
    "objectID": "posts/bigleader-01-05/index.html",
    "href": "posts/bigleader-01-05/index.html",
    "title": "빅리더 1주차 마무리 겸 생각정리",
    "section": "",
    "text": "마크다운 문법 지키는거 좀 귀찮음…\n분명 쉬운 방법이 있을탠데\nhtml tag 가 사용 가능하단건 분명한 장점.\njs 스크립트도 동작하나?\n여기까지는 자잘한 생각들.\n\n1주차 마무리한 소감\n빅디러 AI 혁신가드닝 줄여서 빅리더의 1주차가 종료되었다. 처음 들어온 금요일날은 OT 였고 바로 주말이었으니 1주차라기엔 애매한가?\nn주차를 어떻게 구분해야할지 조금 모호하긴하지만 내 마음대로 구분할랜다.\n내가 보낸 1주차 같이 공부하는 학생들이 70여명이 넘어가니 한 반에서 수업하는게 사실상 불가능했다. 그래서 A반 B반으로 나누어 수업을 진행하게 되었다.\n그래도 한반에 35여명이니 적은 인원은 아니다. 코드를 다루거나 익숙하지 않은 툴들을 다루다보니 강사님이 학생들 한명 한명을 신경써주시는게 사실살 불가능 했다.\n수업난이도나 진행에서도 아쉬운 점이 있긴했지만 교수님들께서도 정말 열정이 넘치시고 연구원분들도 쓰러지셔도 전혀 이상하지 않을 정도로 도와주고 계시니 점차 나아질 것이라 기대한다.\n1주차에는 기초 파이썬과 시각화 툴 중 하나인 PowerBI 수업을 들었는데 A 반과는 수업 내용이 많이 달랐나보다.\nA 반의 경우 파이썬 기초 내용은 생략하고 GUI를 이용해서 사진을 보여주고 편집된 사진을 보여주는 등 응용한 내용을 진행했다고 한다. B 반의 경우 너무 기초부터 진행하여서 거의 숙지하고 있던 내용이다보니 수업에 집중하기 애먹었다.\n기초적인 내용들은 사전에 LMS 교육을 통해 어느정도 숙지하고 여기서는 프로젝트를 진행한다고 들었기에 열심히 준비했는데… 비록 사전교육 기간이 상대적으로 적었던 학생도 있었을 것이고 학업 내지는 일등을 병행하고 있었기에 시간이 부족했을 수도 있었다곤 생각하지만 아쉬운건 사실이었다.\n시각화 툴인 PowerBI의 경우 UI도 정말 직관적이고 데이터를 처리하거나 표현하는데 있어서 대부분의 기능이 드래그 드롭 으로 해결되었기에 강력한 시각화 툴 이라는 생각이 들었다. 툴도 툴이지만 시각화 하는데 있어서 중요한 점들 현업에서의 이야기 그 외에도 진행하셨던 프로젝트 이야기등을 해주셔서 많은 도움이 되었다. 이런 시각화에 대한 내용들은 다음주에 진행될 Tableau 수업을 마친 후 정리하여 작성하고자 한다.\nOT 이야기도 적어야하고 만난 친구들 재미있는 스터디그룹 조원들 자잘한 사건사고 앞으로의 계획등 적어야할, 적고싶은 이야기는 정말 많은데 글 쓰는 재주도 없고 경험도 없고 시간도 없고(이건 핑계다) 그래도 최대한 열심히 블로그 포스팅을 이어나가보려고 한다.\n가독성이나 카테고리 정리 등도 신경쓰이긴 하지만 이건 차차 정리해나가는걸로 하며 이번 포스팅은 마무리해야지."
  },
  {
    "objectID": "posts/bigleader-01-02/bigleader_01_week01_02.html",
    "href": "posts/bigleader-01-02/bigleader_01_week01_02.html",
    "title": "빅리더 1주차 02일 타이타닉",
    "section": "",
    "text": "타이타닉 데이터 조작해보고 간단한 시각화 코드 보기\n\n실제 데이터 사용해보기\n\nimport pandas as pd\n\n\ndf = pd.read_csv('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/titanic.csv') # csv 파일 불러오기\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      887\n      0\n      2\n      Montvila, Rev. Juozas\n      male\n      27.0\n      0\n      0\n      211536\n      13.0000\n      NaN\n      S\n    \n    \n      887\n      888\n      1\n      1\n      Graham, Miss. Margaret Edith\n      female\n      19.0\n      0\n      0\n      112053\n      30.0000\n      B42\n      S\n    \n    \n      888\n      889\n      0\n      3\n      Johnston, Miss. Catherine Helen \"Carrie\"\n      female\n      NaN\n      1\n      2\n      W./C. 6607\n      23.4500\n      NaN\n      S\n    \n    \n      889\n      890\n      1\n      1\n      Behr, Mr. Karl Howell\n      male\n      26.0\n      0\n      0\n      111369\n      30.0000\n      C148\n      C\n    \n    \n      890\n      891\n      0\n      3\n      Dooley, Mr. Patrick\n      male\n      32.0\n      0\n      0\n      370376\n      7.7500\n      NaN\n      Q\n    \n  \n\n891 rows × 12 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.000000\n      891.000000\n      891.000000\n      714.000000\n      891.000000\n      891.000000\n      891.000000\n    \n    \n      mean\n      446.000000\n      0.383838\n      2.308642\n      29.699118\n      0.523008\n      0.381594\n      32.204208\n    \n    \n      std\n      257.353842\n      0.486592\n      0.836071\n      14.526497\n      1.102743\n      0.806057\n      49.693429\n    \n    \n      min\n      1.000000\n      0.000000\n      1.000000\n      0.420000\n      0.000000\n      0.000000\n      0.000000\n    \n    \n      25%\n      223.500000\n      0.000000\n      2.000000\n      20.125000\n      0.000000\n      0.000000\n      7.910400\n    \n    \n      50%\n      446.000000\n      0.000000\n      3.000000\n      28.000000\n      0.000000\n      0.000000\n      14.454200\n    \n    \n      75%\n      668.500000\n      1.000000\n      3.000000\n      38.000000\n      1.000000\n      0.000000\n      31.000000\n    \n    \n      max\n      891.000000\n      1.000000\n      3.000000\n      80.000000\n      8.000000\n      6.000000\n      512.329200\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n\n\n\ndf.isnull()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      1\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      2\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      3\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      4\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      886\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      887\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      888\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n      False\n      False\n      False\n      True\n      False\n    \n    \n      889\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n    \n    \n      890\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      False\n      True\n      False\n    \n  \n\n891 rows × 12 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf['Age'].isnull().sum()\n\n177\n\n\n\ndf.set_index('PassengerId', inplace=True)\n\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n    \n      PassengerId\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.0\n      1\n      0\n      A/5 21171\n      7.2500\n      NaN\n      S\n    \n    \n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n      1\n      0\n      PC 17599\n      71.2833\n      C85\n      C\n    \n    \n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n      0\n      0\n      STON/O2. 3101282\n      7.9250\n      NaN\n      S\n    \n    \n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n      1\n      0\n      113803\n      53.1000\n      C123\n      S\n    \n    \n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.0\n      0\n      0\n      373450\n      8.0500\n      NaN\n      S\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 보고 싶은 컬럼만 추출하기\ndf_new = df[['Name', 'Sex', 'Age']]\ndf_new.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Name\n      Sex\n      Age\n    \n    \n      PassengerId\n      \n      \n      \n    \n  \n  \n    \n      1\n      Braund, Mr. Owen Harris\n      male\n      22.0\n    \n    \n      2\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n    \n    \n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n    \n    \n      4\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n    \n    \n      5\n      Allen, Mr. William Henry\n      male\n      35.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 내가 원본 데이터를 안다는 가정하게 usecol 을 사용할수도 있음.\ndf_usecol = pd.read_csv('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/titanic.csv', usecols=['PassengerId', 'Name', 'Sex', 'Age'])\ndf_usecol.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Name\n      Sex\n      Age\n    \n  \n  \n    \n      0\n      1\n      Braund, Mr. Owen Harris\n      male\n      22.0\n    \n    \n      1\n      2\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n    \n    \n      2\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n    \n    \n      3\n      4\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n    \n    \n      4\n      5\n      Allen, Mr. William Henry\n      male\n      35.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 생존률과 상관관계 위주로 분석\ndf.corr()\n\nFutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  df.corr()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      Survived\n      1.000000\n      -0.338481\n      -0.077221\n      -0.035322\n      0.081629\n      0.257307\n    \n    \n      Pclass\n      -0.338481\n      1.000000\n      -0.369226\n      0.083081\n      0.018443\n      -0.549500\n    \n    \n      Age\n      -0.077221\n      -0.369226\n      1.000000\n      -0.308247\n      -0.189119\n      0.096067\n    \n    \n      SibSp\n      -0.035322\n      0.083081\n      -0.308247\n      1.000000\n      0.414838\n      0.159651\n    \n    \n      Parch\n      0.081629\n      0.018443\n      -0.189119\n      0.414838\n      1.000000\n      0.216225\n    \n    \n      Fare\n      0.257307\n      -0.549500\n      0.096067\n      0.159651\n      0.216225\n      1.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n결측치 처리\n결측치 처리에 관해선 정답이 없다는게 내 주관적인 생각.\n맞다고 생각되면 일단 해보는걸 추천함.\n\n# Age : 177 / Cabin : 687 / Embarked : 2\ndf.isnull().sum()\n\nSurvived      0\nPclass        0\nName          0\nSex           0\nAge         177\nSibSp         0\nParch         0\nTicket        0\nFare          0\nCabin       687\nEmbarked      2\ndtype: int64\n\n\n결측치를 무작정 제거하는건 좋은 생각이 아님\n\n#df.dropna(subset=['Age', 'Cabin'])\n\n탑승객들의 나이는 어떤식으로 결측치 처리 할 것인가\n\n# 그냥 평균으로 넣을 수도 있음.\n\n# df['Age'].fillna(df['Age'].mean())\n\n\n# 생존자 나이 평균\nmean1 = df[df['Survived'] == 1]['Age'].mean()\n\n# 사망자 나이 평균\nmean0 = df[df['Survived'] == 0]['Age'].mean()\n\nprint(mean1, mean0)\n\n28.343689655172415 30.62617924528302\n\n\n\ndf.loc[df['Survived'] == 1, 'Age']\n\nPassengerId\n2      38.0\n3      26.0\n4      35.0\n9      27.0\n10     14.0\n       ... \n876    15.0\n880    56.0\n881    25.0\n888    19.0\n890    26.0\nName: Age, Length: 342, dtype: float64\n\n\n\ndf[df['Survived'] == 1]['Age'] # 생존그룹에서 나이만 추출\n\nPassengerId\n2      38.0\n3      26.0\n4      35.0\n9      27.0\n10     14.0\n       ... \n876    15.0\n880    56.0\n881    25.0\n888    19.0\n890    26.0\nName: Age, Length: 342, dtype: float64\n\n\n\n# 앞에는 생존자 그룹을 분류 / 뒤에는\ndf.loc[df['Survived'] == 1, 'Age'] = df[df['Survived'] == 1]['Age'].fillna(mean1)\ndf.loc[df['Survived'] == 0, 'Age'] = df[df['Survived'] == 0]['Age'].fillna(mean0)\n\n\n\nPython 시각화\n\n%matplotlib inline\n\ns = [1,2,3,4,5,6,5,4,3,2,1]\n\ns1 = pd.Series(s)\ns1.plot()\n\n<Axes: >\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ns = [1,2,3,4,5,6,5,4,3,2,1]\n\nplt.plot(s)\nplt.show()\n\n\n\n\nchat gpt한태 물어보면 엄청 잘 알려줌\n데이터의 종류에 따라서 적절한 시각화 방법이 다른데 이걸 경험적으로 아는게 중요함\n\nimport matplotlib.pyplot as plt\n\n\np1 = ['a', 'b', 'c', 'd', 'e']\ns1 = [1, 2, 3, 4, 5]\ns2 = [2, 4, 3.5, 5, 2]\n\nplt.xlabel('Test')\nplt.ylabel('Value')\nplt.title('Chart')\nplt.plot(p1, s1, 'r')\nplt.bar(p1, s2) # 여기까지 실행하면 그래프가 안그려짐. 하지만 주피터 노트북 환경에선 그려지는데 인터프리터의 특성이 반영된 것임.\nplt.legend(['test1', 'test2']) # 범례\nplt.show()\n\n\n\n\n###데이터 크롤링 방법\n\nfrom bs4 import BeautifulSoup\nfrom urllib.request import urlopen"
  },
  {
    "objectID": "posts/new-post-02/index.html",
    "href": "posts/new-post-02/index.html",
    "title": "새로 추가한 포스팅22222",
    "section": "",
    "text": "새로 추가한 포스팅입니다."
  },
  {
    "objectID": "posts/bigleader-01-11/index.html",
    "href": "posts/bigleader-01-11/index.html",
    "title": "빅리더 시각화 체크리스트",
    "section": "",
    "text": "내가 멋있다고 생각하고 follow하는 개발자 분이 정리한 내용을 보고 영감을 얻어 이런 페이지를 한번쯤은 작성하고 있었는데 이번에 기회가 생겨 작성하게 되었다.\n시각화 할 일이 생기게 된다면 이 체크리스트를 펴두고 진행해보자.\n\n시각화 참고 노트\n\n시각화 할때는 색상도 중요함. 발표 대상(기업) 이라던가 주제의 메인 컬러를 잘 이용해야함. 근데 이부분은 디자인적인 영역이 아닌가?\n파이차트 보다는 도넛차트가 좋다. 도넛차트는 가운데 빈 공간도 활용할 수 있기에 정보를 많이 담을 수 있다.\n그래프나 글자의 배치도 신경써야한다. (선거 현수막? 이미지. 후보들 얼굴이 왼 오 왼 오 순으로 있어서 강조해주던 이미지)"
  },
  {
    "objectID": "posts/new-post-05/exercise_coordinate_reference_systems.html",
    "href": "posts/new-post-05/exercise_coordinate_reference_systems.html",
    "title": "Kaggle_exercise_02",
    "section": "",
    "text": "kaggle 문제풀이 02 https://www.kaggle.com/code/lsiina/exercise-coordinate-reference-systems/edit\n\n시작하기에 앞서\n당신은 조류 보존 전문가이며 보라색 제비의 이동 패턴을 이해하고자 합니다. 연구 결과 이 새들은 일반적으로 미국 동부에서 여름 번식기를 보내고 겨울에 남미로 이동한다는 것을 발견했습니다. 그러나 이 새는 멸종 위기 종으로 등재되어 있기 때문에, 이 새들이 더 자주 방문하는 위치를 자세히 살펴보고자 합니다.\n\n\n\n남미에는 몇 개의 보호 지역이 있으며, 이 지역은 이곳에서 이동하는 (또는 살아있는) 종이 최대한 번성할 수 있도록 특별한 규정에 따라 운영됩니다. 보라색 제비가 이 지역을 방문하는 경향이 있는지 알고 싶습니다. 이 질문에 대한 답변을 찾기 위해 최근 수집된 11종의 새들의 연중 위치를 추적하는 데이터를 사용할 것입니다.\n시작하기 전에 아래 코드 셀을 실행하여 모든 것을 설정하세요.\n\n!pip install geopandas\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting geopandas\n  Downloading geopandas-0.13.0-py3-none-any.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 21.8 MB/s eta 0:00:00\nCollecting fiona>=1.8.19 (from geopandas)\n  Downloading Fiona-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 36.7 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\nRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\nCollecting pyproj>=3.0.1 (from geopandas)\n  Downloading pyproj-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 67.9 MB/s eta 0:00:00\nRequirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (23.1.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (2022.12.7)\nRequirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (8.1.3)\nCollecting click-plugins>=1.0 (from fiona>=1.8.19->geopandas)\n  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\nCollecting cligj>=0.5 (from fiona>=1.8.19->geopandas)\n  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (1.22.4)\nInstalling collected packages: pyproj, cligj, click-plugins, fiona, geopandas\nSuccessfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.4 geopandas-0.13.0 pyproj-3.5.0\n\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nfrom shapely.geometry import LineString\n\n\n\nExercises\n\n1) 데이터 로드.\n다음 코드 셀을 실행하여 GPS 데이터를 pandas DataFrame birds_df로 로드하세요.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n# Load the data and print the first 5 rows\nbirds_df = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/purple_martin.csv\", parse_dates=['timestamp'])\nprint(\"There are {} different birds in the dataset.\".format(birds_df[\"tag-local-identifier\"].nunique()))\nbirds_df.head()\n\nThere are 11 different birds in the dataset.\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n이 데이터셋에는 “tag-local-identifier” 열에서 고유한 값을 가진 11마리의 새가 있습니다. 각 새는 연도의 다른 시기에 수집된 여러 측정치가 있습니다.\n다음 코드 셀을 사용하여 birds GeoDataFrame을 만드세요. - birds는 birds_df의 모든 열을 가져와야 하며, “geometry” 열에는 (경도, 위도) 위치를 포함하는 Point 객체가 있어야 합니다. - birds의 CRS는 {'init': 'epsg:4326'}로 설정하세요.\n\n# Your code here: Create the GeoDataFrame\nbirds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[\"location-long\"], birds_df[\"location-lat\"]))\n\n# Your code here: Set the CRS to {'init': 'epsg:4326'}\nbirds.crs = {'init': 'epsg:4326'}\n\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\nbirds.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      timestamp\n      location-long\n      location-lat\n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      2014-08-15 05:56:00\n      -88.146014\n      17.513049\n      30448\n      POINT (-88.14601 17.51305)\n    \n    \n      1\n      2014-09-01 05:59:00\n      -85.243501\n      13.095782\n      30448\n      POINT (-85.24350 13.09578)\n    \n    \n      2\n      2014-10-30 23:58:00\n      -62.906089\n      -7.852436\n      30448\n      POINT (-62.90609 -7.85244)\n    \n    \n      3\n      2014-11-15 04:59:00\n      -61.776826\n      -11.723898\n      30448\n      POINT (-61.77683 -11.72390)\n    \n    \n      4\n      2014-11-30 09:59:00\n      -61.241538\n      -11.612237\n      30448\n      POINT (-61.24154 -11.61224)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n2) 데이터 시각화.\n다음으로, GeoPandas에서 'naturalearth_lowres' 데이터 세트를 로드하고, americas를 북미와 남미의 모든 국가의 경계를 포함하는 GeoDataFrame으로 설정합니다. 변경하지 마시고 다음 코드 셀을 실행하세요.\n\n# Load a GeoDataFrame with country boundaries in North/South America, print the first 5 rows\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\namericas = world.loc[world['continent'].isin(['North America', 'South America'])]\namericas.head()\n\nFutureWarning: The geopandas.dataset module is deprecated and will be removed in GeoPandas 1.0. You can get the original 'naturalearth_lowres' data from https://www.naturalearthdata.com/downloads/110m-cultural-vectors/.\n  world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      16\n      11263077.0\n      North America\n      Haiti\n      HTI\n      14332\n      POLYGON ((-71.71236 19.71446, -71.62487 19.169...\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음 코드 셀을 사용하여 다음을 모두 보여주는 단일 플롯을 만듭니다. (1) americas GeoDataFrame의 국가 경계 및 (2) birds_gdf GeoDataFrame의 모든 지점입니다.\n여기서 특별한 스타일링에 대해 걱정할 필요가 없습니다. 모든 데이터가 제대로 로드되었는지 빠른 검증으로서 일단 기본 플롯을 만드는 것이 목적입니다. 특히 색상을 사용하여 새를 구분할 필요가 없으며 시작점과 끝점을 구분할 필요도 없습니다. 다음 단계에서 그것을 수행할 것입니다.\n\n# Your code here\n# Create a map\nax = americas.plot(figsize=(10,10), color='white', linestyle=':', edgecolor='gray')\nbirds.plot(ax=ax, markersize=10)\n\n# Uncomment to zoom in\nax.set_xlim([-110, -30])\nax.set_ylim([-30, 60])\n\n(-30.0, 60.0)\n\n\n\n\n\n\n\n3) 각 새가 여행을 시작하고 끝내는 위치는 어디인가요? (파트 1)\n경로를 더 자세히 살펴보기 위해, 각 새의 경로를 나타내는 LineString 객체를 담은 path_gdf와 출발점을 담은 start_gdf 두 개의 GeoDataFrame을 만듭니다:\n- path_gdf는 Point 객체의 목록에서 LineString 객체를 생성하기 위해 LineString() 메서드를 사용합니다.\n- start_gdf는 각 새의 출발점을 포함합니다.\n\n# GeoDataFrame showing path for each bird\npath_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: LineString(x)).reset_index()\npath_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry)\npath_gdf.crs = {'init' :'epsg:4326'}\n\n# GeoDataFrame showing starting point for each bird\nstart_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[0]).reset_index()\nstart_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry)\nstart_gdf.crs = {'init' :'epsg:4326'}\n\n# Show first five rows of GeoDataFrame\nstart_gdf.head()\n\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-90.12992 20.73242)\n    \n    \n      1\n      30054\n      POINT (-93.60861 46.50563)\n    \n    \n      2\n      30198\n      POINT (-80.31036 25.92545)\n    \n    \n      3\n      30263\n      POINT (-76.78146 42.99209)\n    \n    \n      4\n      30275\n      POINT (-76.78213 42.99207)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음 코드 셀을 사용하여 각 새의 최종 위치를 포함하는 GeoDataFrame end_gdf를 생성합니다.\n\n형식은 start_gdf와 동일해야하며 두 개의 열 (“tag-local-identifier” 및 “geometry”)이 있어야합니다. “geometry” 열에는 Point 객체가 포함되어야합니다.\nend_gdf의 CRS를 {'init': 'epsg:4326'}로 설정합니다.\n\n\nend_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1]).reset_index()\nend_gdf = gpd.GeoDataFrame(end_df, geometry=end_df.geometry)\nend_gdf.crs = {'init': 'epsg:4326'}\n\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\nend_gdf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      tag-local-identifier\n      geometry\n    \n  \n  \n    \n      0\n      30048\n      POINT (-47.53632 -4.43758)\n    \n    \n      1\n      30054\n      POINT (-62.47914 -5.03840)\n    \n    \n      2\n      30198\n      POINT (-57.46417 -2.77617)\n    \n    \n      3\n      30263\n      POINT (-50.19230 -5.70504)\n    \n    \n      4\n      30275\n      POINT (-57.70404 -16.72336)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4) 각각의 새들은 어디에서 여행을 시작하고 끝냅니까? (파트 2)\n위의 문제에서 사용한 GeoDataFrame인 path_gdf, start_gdf, end_gdf를 사용하여 모든 새의 경로를 한 지도에 시각화해보세요. 이 때 americas GeoDataFrame도 사용할 수 있습니다.\n\nax = americas.plot(figsize=(10, 10), color='white', linestyle=':', edgecolor='gray')\n\nstart_gdf.plot(ax=ax, color='red',  markersize=30)\npath_gdf.plot(ax=ax, cmap='tab20b', linestyle='-', linewidth=1, zorder=1)\nend_gdf.plot(ax=ax, color='black', markersize=30)\n\n<Axes: >\n\n\n\n\n\n\n\n5) 남미의 보호 지역은 어디입니까? (파트 1)\n새들이 모두 남아메리카 어딘가로 이동하는 것으로 보입니다. 하지만 그들이 보호구역으로 가는 걸까요?\n다음 코드 셀에서는 남아메리카의 모든 보호구역 위치가 포함 된 GeoDataFrame protected_areas를 만듭니다. 해당 형상 파일은 protected_filepath 경로에 위치합니다.\n\n# Path of the shapefile to load\nprotected_filepath = \"/content/drive/MyDrive/2023 데이터마이닝/dataset/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp\"\n\n# Your code here\nprotected_areas = gpd.read_file(protected_filepath)\n\n\n\n6) 남미의 보호 지역은 어디입니까? (파트 2)\n다음 코드 셀을 사용하여 protected_areas GeoDataFrame을 사용하여 남아메리카의 보호 지역 위치를 보여주는 플롯을 만드세요. (일부 보호 지역이 육지에 있고 다른 일부는 해양 수역에 있음을 알 수 있습니다.)\n\n# Country boundaries in South America\nsouth_america = americas.loc[americas['continent']=='South America']\n\n# Your code here: plot protected areas in South America\nax = south_america.plot(figsize=(10,10), color='white', edgecolor='gray')\nprotected_areas.plot(ax=ax, alpha=0.4)\n\n<Axes: >\n\n\n\n\n\n\n\n7) 몇 퍼센트가 남아메리카의 보호지역입니까?\n남아메리카 중 얼마나 많은 지역이 보호 구역인지를 결정하려고 합니다. 이를 위해 먼저, 남아메리카 내 보호 구역의 총 면적(해양 지역은 포함하지 않음)을 계산하고자 합니다. 이를 위해 “REP_AREA” 및 “REP_M_AREA” 열을 사용합니다. 이 열들은 각각 사각 킬로미터 단위로 측정된 총 면적 및 총 해양 면적을 나타냅니다.\n아래 코드 셀을 그대로 실행하세요.\n\nP_Area = sum(protected_areas['REP_AREA']-protected_areas['REP_M_AREA'])\nprint(\"South America has {} square kilometers of protected areas.\".format(P_Area))\n\nSouth America has 5396761.9116883585 square kilometers of protected areas.\n\n\n그런 다음 south_america GeoDataFrame을 사용하여 계산을 완료합니다.\n\nsouth_america.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      9\n      44938712.0\n      South America\n      Argentina\n      ARG\n      445445\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.25000...\n    \n    \n      10\n      18952038.0\n      South America\n      Chile\n      CHL\n      282318\n      MULTIPOLYGON (((-68.63401 -52.63637, -68.63335...\n    \n    \n      20\n      3398.0\n      South America\n      Falkland Is.\n      FLK\n      282\n      POLYGON ((-61.20000 -51.85000, -60.00000 -51.2...\n    \n    \n      28\n      3461734.0\n      South America\n      Uruguay\n      URY\n      56045\n      POLYGON ((-57.62513 -30.21629, -56.97603 -30.1...\n    \n    \n      29\n      211049527.0\n      South America\n      Brazil\n      BRA\n      1839758\n      POLYGON ((-53.37366 -33.76838, -53.65054 -33.2...\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음 단계를 따라 남아메리카 전체의 면적을 계산합니다:\n\n각 국가의 면적을 폴리곤의 area 속성을 사용하여 계산하고 결과를 합산합니다(EPSG 3035를 CRS로 사용). 계산된 면적은 제곱 미터 단위입니다.\n\n답변을 제곱 킬로미터 단위로 변환합니다.\n\n\n# Your code here: Calculate the total area of South America (in square kilometers)\ntotalArea = sum(south_america.geometry.to_crs(epsg=3035).area) / 10**6\n\n다음 코드 셀을 실행하여 남미 중 보호 구역이 차지하는 비율을 계산하세요.\n\n# What percentage of South America is protected?\npercentage_protected = P_Area/totalArea\nprint('Approximately {}% of South America is protected.'.format(round(percentage_protected*100, 2)))\n\nApproximately 30.39% of South America is protected.\n\n\n\n\n8) 남아메리카에는 새들이 어디에 있는가?\n그래서 새들이 보호 지역에 있나요?\n모든 새들의 발견 위치를 보여주는 플롯을 만드세요. 또한 남아메리카의 모든 보호 지역의 위치도 플롯에 나타내세요.\n육지 구성요소가 없는 순수한 해양 지역은 제외하려면 “MARINE” 열을 사용하여 (protected_areas[protected_areas['MARINE']!='2']의 각 행 대신 protected_areas GeoDataFrame의 모든 행을 플롯) 하면 됩니다.\n\n# Your code here\nax = south_america.plot(figsize=(10,10), color='white', edgecolor='gray')\nprotected_areas[protected_areas['MARINE']!='2'].plot(ax=ax, alpha=0.4, zorder=1)\nbirds[birds.geometry.y < 0].plot(ax=ax, color='red', alpha=0.6, markersize=10, zorder=2)\n\n<Axes: >"
  },
  {
    "objectID": "posts/bigleader-00/index.html",
    "href": "posts/bigleader-00/index.html",
    "title": "빅리더 프로젝트 목표",
    "section": "",
    "text": "체크리스트로 작성 (☐, ✔)\n\n☐ 개인프로젝트 하나 끝내기  - 서버 + 크롤링 이용해서 웹 크롤링 한달동안 굴리기 ( 블로그에 페이지 생성 ) - DL 이용해서 파이게임 정복하기 ☐ 선형대수 이론  ☐ 확률통계 이론  ☐ ML/DL 이론  ☐ 코딩테스트 lv3 까지 풀기 \n문제 풀어보기  https://www.datamanim.com/dataset/99_pandas/pandasMain.html"
  },
  {
    "objectID": "posts/new-post-04/exercise_your_first_map.html",
    "href": "posts/new-post-04/exercise_your_first_map.html",
    "title": "Kaggle_exercise_01",
    "section": "",
    "text": "kaggle 문제풀이 01 https://www.kaggle.com/code/lsiina/exercise-your-first-map/edit\n이 페이지는 위의 코드를 chat-gpt를 이용하여 한국어로 번역하고 문제풀이를 했다. 원문이 궁금하다면 위의 링크를 이용하길 바란다.\n\nIntroduction\nKiva.org는 전 세계의 가난한 사람들에게 금융 서비스를 제공하는 온라인 크라우드펀딩 플랫폼입니다. Kiva 대출자들은 2백만 명 이상에게 10억 달러 이상의 대출을 제공했습니다.\n\n\n\nKiva는 “필드 파트너”라는 글로벌 네트워크를 통해 세계에서 가장 외지에 위치한 지역에까지 도달합니다. 이들 파트너는 대출자를 검증하고 서비스를 제공하며 대출을 관리하는 지역 단체입니다. 이번 연습에서는 필리핀에서의 Kiva 대출을 조사할 것입니다. Kiva의 현재 네트워크 범위를 벗어나는 지역을 파악하여 새로운 필드 파트너 모집 기회를 식별할 수 있는지 확인해보세요. 시작하려면 아래 코드 셀을 실행하여 피드백 시스템을 설정하세요.\n\n#!pip install geopandas  # 코랩은 이거만 해도 댐\n\n\nimport geopandas as gpd\n\n\n1) 데이터 가져오기.\n다음 셀을 사용하여 loans_filepath에서 로드된 shapefile을 사용하여 GeoDataFrame world_loans를 생성하세요.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nloans_filepath = \"/content/drive/MyDrive/2023 데이터마이닝/dataset/kiva_loans/kiva_loans.shp\"\n\n# Your code here: Load the data\nworld_loans = gpd.read_file(loans_filepath)\n\n# Uncomment to view the first five rows of the data\nworld_loans.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      0\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      450\n      POINT (102.89751 13.66726)\n    \n    \n      1\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      20275\n      POINT (102.98962 13.02870)\n    \n    \n      2\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      9150\n      POINT (102.98962 13.02870)\n    \n    \n      3\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      604950\n      POINT (105.31312 12.09829)\n    \n    \n      4\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Sanitation\n      Cambodia\n      275\n      POINT (105.31312 12.09829)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n2) 데이터 시각화하기.\n다음 코드 셀을 수정하지 않고 실행하여 국가 경계를 포함하는 world GeoDataFrame을 로드하세요.\n\n# This dataset is provided in GeoPandas\nworld_filepath = gpd.datasets.get_path('naturalearth_lowres')\nworld = gpd.read_file(world_filepath)\n\nworld.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      pop_est\n      continent\n      name\n      iso_a3\n      gdp_md_est\n      geometry\n    \n  \n  \n    \n      0\n      889953.0\n      Oceania\n      Fiji\n      FJI\n      5496\n      MULTIPOLYGON (((180.00000 -16.06713, 180.00000...\n    \n    \n      1\n      58005463.0\n      Africa\n      Tanzania\n      TZA\n      63177\n      POLYGON ((33.90371 -0.95000, 34.07262 -1.05982...\n    \n    \n      2\n      603253.0\n      Africa\n      W. Sahara\n      ESH\n      907\n      POLYGON ((-8.66559 27.65643, -8.66512 27.58948...\n    \n    \n      3\n      37589262.0\n      North America\n      Canada\n      CAN\n      1736425\n      MULTIPOLYGON (((-122.84000 49.00000, -122.9742...\n    \n    \n      4\n      328239523.0\n      North America\n      United States of America\n      USA\n      21433226\n      MULTIPOLYGON (((-122.84000 49.00000, -120.0000...\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nworld와 world_loans GeoDataFrame을 사용하여 전 세계 Kiva 대출 위치를 시각화하세요.\n\n# Define a base map with county boundaries\nax = world.plot(figsize=(20,20), color='whitesmoke', linestyle=':', edgecolor='black')\n\n# Add wild lands, campsites, and foot trails to the base map\nworld_loans.plot(color='blue', ax=ax, markersize=2)\n\n<Axes: >\n\n\n\n\n\n\n\n3) 필리핀을 기반으로 하는 대출 선택하기.\n이제 필리핀에 기반을 둔 대출에 초점을 맞출 것입니다. 다음 코드 셀을 사용하여 필리핀에 기반을 둔 대출이 포함된 world_loans의 모든 행을 포함하는 GeoDataFrame인 PHL_loans을 만드세요.\n\nworld_loans\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      0\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      450\n      POINT (102.89751 13.66726)\n    \n    \n      1\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      20275\n      POINT (102.98962 13.02870)\n    \n    \n      2\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Higher Education\n      Cambodia\n      9150\n      POINT (102.98962 13.02870)\n    \n    \n      3\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Vulnerable Populations\n      Cambodia\n      604950\n      POINT (105.31312 12.09829)\n    \n    \n      4\n      9\n      KREDIT Microfinance Institution\n      General Financial Inclusion\n      Sanitation\n      Cambodia\n      275\n      POINT (105.31312 12.09829)\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      13657\n      539\n      N/A, direct to Sevamob\n      DSE Direct\n      Health\n      South Africa\n      50000\n      POINT (29.60355 -25.94599)\n    \n    \n      13658\n      540\n      N/A, direct to Sanergy\n      DSE Direct\n      Water and Sanitation\n      Kenya\n      50000\n      POINT (36.82195 -1.29207)\n    \n    \n      13659\n      542\n      N/A direct to BioLite Inc.\n      DSE Direct\n      Clean Cookstove\n      Uganda\n      50000\n      POINT (32.58252 0.34760)\n    \n    \n      13660\n      543\n      N/A direct to LegWorks Inc.\n      DSE Direct\n      Health\n      Canada\n      50000\n      POINT (-79.38318 43.65323)\n    \n    \n      13661\n      545\n      N/A, direct to Solar Home\n      DSE Direct\n      Solar Home Systems\n      Myanmar (Burma)\n      50000\n      POINT (96.19513 16.86607)\n    \n  \n\n13662 rows × 7 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# Your code here\nPHL_loans = world_loans.loc[world_loans.country=='Philippines'].copy()\nPHL_loans.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Partner ID\n      Field Part\n      sector\n      Loan Theme\n      country\n      amount\n      geometry\n    \n  \n  \n    \n      2859\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.73961 17.64228)\n    \n    \n      2860\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.74169 17.63235)\n    \n    \n      2861\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      400\n      POINT (121.46667 16.60000)\n    \n    \n      2862\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      6050\n      POINT (121.73333 17.83333)\n    \n    \n      2863\n      123\n      Alalay sa Kaunlaran (ASKI)\n      General Financial Inclusion\n      General\n      Philippines\n      625\n      POINT (121.51800 16.72368)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n4) 필리핀의 대출 이해하기.\n필리핀의 대출 이해하기. 다음 코드 셀을 수정하지 않고 실행하여 필리핀의 모든 섬의 경계를 포함하는 PHL GeoDataFrame을 로드하세요.\n\n# Load a KML file containing island boundaries\ngpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw'\nPHL = gpd.read_file(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/Philippines_AL258.kml\", driver='KML')\nPHL.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Name\n      Description\n      geometry\n    \n  \n  \n    \n      0\n      Autonomous Region in Muslim Mindanao\n      \n      MULTIPOLYGON (((119.46690 4.58718, 119.46653 4...\n    \n    \n      1\n      Bicol Region\n      \n      MULTIPOLYGON (((124.04577 11.57862, 124.04594 ...\n    \n    \n      2\n      Cagayan Valley\n      \n      MULTIPOLYGON (((122.51581 17.04436, 122.51568 ...\n    \n    \n      3\n      Calabarzon\n      \n      MULTIPOLYGON (((120.49202 14.05403, 120.49201 ...\n    \n    \n      4\n      Caraga\n      \n      MULTIPOLYGON (((126.45401 8.24400, 126.45407 8...\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\nPHL과 PHL_loans GeoDataFrame을 사용하여 필리핀의 대출을 시각화하세요.\n\n# Your code here\nax1 = PHL.plot(figsize=(10,10), color='whitesmoke', linestyle=':', edgecolor='black')\nPHL_loans.plot(color='blue', ax=ax1, markersize=2)\n\n<Axes: >\n\n\n\n\n\n어떤 섬에서 새로운 필드 파트너를 모집하는 것이 유용할지 식별할 수 있나요?\nKiva의 도달 범위를 벗어나 보이는 섬이 있나요?\n이 질문에 대한 답변을 찾는 데 이 맵이 유용할 수 있습니다.\n사각화 된 데이터를 보고 위의 문제에 대한 답을 하자면 내 생각은 다음과 같다.\n대출 받은 사람을 관리하는 파트너를 모집하는 것이라면 당연히 점이 많이 찍힌 섬들로 가야겠지만. (아마도 이 문제에선 이걸 원하는 것은 아닐것이다.)\n아직 대출을 받지 않은 사람들 중에서 대출을 받고자하는 사람들을 찾는 관리자를 원한다면 점이 찍히지 않은 빈 섬으로 가야겠지…\n다만 빈 섬들중에서 어떤 섬이 인구가 많은가(가능하면 대출을 원하는 인구가 많아야함) 를 추가적으로 탐색하는게 좋아보이는데 이는 인구수도 같이 찍어보면 좋지 않을까..?\n추가적으로 해당 지역의 평균 소득도 데이터가 있다면 같이 살펴보면 좋을듯하다.\n이를 한줄로 정리하자면\n필리핀 중앙의 mindoro 섬이나 남쪽의 큰 섬을 공략해보는게 좋을듯 하다.\n다음에 진행할 연습문제 coordinate reference systems.\n\nHave questions or comments? Visit the course discussion forum to chat with other learners."
  },
  {
    "objectID": "posts/bigleader-01-10/index.html",
    "href": "posts/bigleader-01-10/index.html",
    "title": "빅리더 시각화 체크리스트",
    "section": "",
    "text": "내가 멋있다고 생각하고 follow하는 개발자 분이 정리한 내용을 보고 영감을 얻어 이런 페이지를 한번쯤은 작성하고 있었는데 이번에 기회가 생겨 작성하게 되었다.\n시각화 할 일이 생기게 된다면 이 체크리스트를 펴두고 진행해보자.\n\n시각화 참고 노트\n\n시각화 할때는 색상도 중요함. 발표 대상(기업) 이라던가 주제의 메인 컬러를 잘 이용해야함. 근데 이부분은 디자인적인 영역이 아닌가?\n파이차트 보다는 도넛차트가 좋다. 도넛차트는 가운데 빈 공간도 활용할 수 있기에 정보를 많이 담을 수 있다.\n그래프나 글자의 배치도 신경써야한다. (선거 현수막? 이미지. 후보들 얼굴이 왼 오 왼 오 순으로 있어서 강조해주던 이미지)"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html",
    "href": "posts/new-post-03/week_1b_pandas_exam.html",
    "title": "새로 추가한 포스팅3333",
    "section": "",
    "text": "ipynb 파일 추가 테스트"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#series-만들기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#series-만들기",
    "title": "새로 추가한 포스팅3333",
    "section": "Series 만들기",
    "text": "Series 만들기\n첫 번째 Series 객체를 만들어 보죠!\n\nimport numpy as np\nnp.array([2,-1,3,5])\n\narray([ 2, -1,  3,  5])\n\n\n\ns = pd.Series([2,-1,3,5])\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#d-ndarray와-비슷합니다",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#d-ndarray와-비슷합니다",
    "title": "새로 추가한 포스팅3333",
    "section": "1D ndarray와 비슷합니다",
    "text": "1D ndarray와 비슷합니다\nSeries 객체는 넘파이 ndarray와 비슷하게 동작합니다. 넘파이 함수에 매개변수로 종종 전달할 수 있습니다:\n\nimport numpy as np\nnp.exp(s)\n\nNameError: ignored\n\n\nSeries 객체에 대한 산술 연산도 가능합니다. ndarray와 비슷하게 원소별로 적용됩니다:\n\ns + [1000,2000,3000,4000]\n\n0    1002\n1    1999\n2    3003\n3    4005\ndtype: int64\n\n\n넘파이와 비슷하게 Series에 하나의 숫자를 더하면 Series에 있는 모든 원소에 더해집니다. 이를 브로드캐스팅(broadcasting)이라고 합니다:\n\ns\n\n0    2\n1   -1\n2    3\n3    5\ndtype: int64\n\n\n\ns + 1000\n\n0    1002\n1     999\n2    1003\n3    1005\ndtype: int64\n\n\n*나 / 같은 모든 이항 연산과 심지어 조건 연산에서도 마찬가지입니다:\n\ns < 0\n\n0    False\n1     True\n2    False\n3    False\ndtype: bool"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#인덱스-레이블",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#인덱스-레이블",
    "title": "새로 추가한 포스팅3333",
    "section": "인덱스 레이블",
    "text": "인덱스 레이블\nSeries 객체에 있는 각 원소는 인덱스 레이블(index label)이라 불리는 고유한 식별자를 가지고 있습니다. 기본적으로 Series에 있는 원소의 순서입니다(0에서 시작합니다). 하지만 수동으로 인덱스 레이블을 지정할 수도 있습니다:\n\ns2 = pd.Series([68, 83, 112, 68], index=[\"alice\", \"bob\", \"charles\", \"darwin\"])\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n그다음 dict처럼 Series를 사용할 수 있습니다:\n\ns2[\"bob\"]\n\n83\n\n\n일반 배열처럼 정수 인덱스를 사용하여 계속 원소에 접근할 수 있습니다:\n\ns2[1]\n\n83\n\n\n레이블이나 정수를 사용해 접근할 때 명확하게 하기 위해 레이블은 loc 속성을 사용하고 정수는 iloc 속성을 사용하는 것이 좋습니다:\n\ns2.loc[\"bob\"]\n\n83\n\n\n\ns2.iloc[1]\n\n83\n\n\nSeries는 인덱스 레이블을 슬라이싱할 수도 있습니다:\n\ns2.iloc[1:3]\n\nbob         83\ncharles    112\ndtype: int64\n\n\n기본 정수 레이블을 사용할 때 예상 외의 결과를 만들 수 있기 때문에 주의해야 합니다:\n\nsurprise = pd.Series([1000, 1001, 1002, 1003])\nsurprise\n\n0    1000\n1    1001\n2    1002\n3    1003\ndtype: int64\n\n\n\nsurprise_slice = surprise[2:]\nsurprise_slice\n\n2    1002\n3    1003\ndtype: int64\n\n\n보세요. 첫 번째 원소의 인덱스 레이블이 2입니다. 따라서 슬라이싱 결과에서 인덱스 레이블 0인 원소는 없습니다:\n\ntry:\n    surprise_slice[0]\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 0\n\n\n하지만 iloc 속성을 사용해 정수 인덱스로 원소에 접근할 수 있습니다. Series 객체를 사용할 때 loc와 iloc를 사용하는 것이 좋은 이유입니다:\n\nsurprise_slice.iloc[0]\n\n1002\n\n\n\nsurprise_slice.loc[2]\n\n1002"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#dict에서-초기화",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#dict에서-초기화",
    "title": "새로 추가한 포스팅3333",
    "section": "dict에서 초기화",
    "text": "dict에서 초기화\ndict에서 Series 객체를 만들 수 있습니다. 키는 인덱스 레이블로 사용됩니다:\n\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\ns3 = pd.Series(weights)\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\nSeries에 포함할 원소를 제어하고 index를 지정하여 명시적으로 순서를 결정할 수 있습니다:\n\ns4 = pd.Series(weights, index = [\"colin\", \"alice\"])\ns4\n\ncolin    86\nalice    68\ndtype: int64"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#자동-정렬",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#자동-정렬",
    "title": "새로 추가한 포스팅3333",
    "section": "자동 정렬",
    "text": "자동 정렬\n여러 개의 Series 객체를 다룰 때 pandas는 자동으로 인덱스 레이블에 따라 원소를 정렬합니다.\n\ns2\n\nalice       68\nbob         83\ncharles    112\ndarwin      68\ndtype: int64\n\n\n\ns3\n\nalice     68\nbob       83\ncolin     86\ndarwin    68\ndtype: int64\n\n\n\nprint(s2.keys())\nprint(s3.keys())\n\ns2 + s3\n\nIndex(['alice', 'bob', 'charles', 'darwin'], dtype='object')\nIndex(['alice', 'bob', 'colin', 'darwin'], dtype='object')\n\n\nalice      136.0\nbob        166.0\ncharles      NaN\ncolin        NaN\ndarwin     136.0\ndtype: float64\n\n\n만들어진 Series는 s2와 s3의 인덱스 레이블의 합집합을 담고 있습니다. s2에 \"colin\"이 없고 s3에 \"charles\"가 없기 때문에 이 원소는 NaN 값을 가집니다(Not-a-Number는 누락이란 의미입니다).\n자동 정렬은 구조가 다르고 누락된 값이 있는 여러 데이터를 다룰 때 매우 편리합니다. 하지만 올바른 인덱스 레이블을 지정하는 것을 잊는다면 원치않는 결과를 얻을 수 있습니다:\n\ns5 = pd.Series([1000,1000,1000,1000])\nprint(\"s2 =\", s2.values)\nprint(\"s5 =\", s5.values)\n\ns2 + s5\n\ns2 = [ 68  83 112  68]\ns5 = [1000 1000 1000 1000]\n\n\nalice     NaN\nbob       NaN\ncharles   NaN\ndarwin    NaN\n0         NaN\n1         NaN\n2         NaN\n3         NaN\ndtype: float64\n\n\n레이블이 하나도 맞지 않기 때문에 판다스가 이 Series를 정렬할 수 없습니다. 따라서 모두 NaN이 되었습니다."
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#스칼라로-초기화",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#스칼라로-초기화",
    "title": "새로 추가한 포스팅3333",
    "section": "스칼라로 초기화",
    "text": "스칼라로 초기화\n스칼라와 인덱스 레이블의 리스트로 Series 객체를 초기화할 수도 있습니다: 모든 원소가 이 스칼라 값으로 설정됩니다.\n\nmeaning = pd.Series(42, [\"life\", \"universe\", \"everything\"])\nmeaning\n\nlife          42\nuniverse      42\neverything    42\ndtype: int64"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#series-이름",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#series-이름",
    "title": "새로 추가한 포스팅3333",
    "section": "Series 이름",
    "text": "Series 이름\nSeries는 name을 가질 수 있습니다:\n\ns6 = pd.Series([83, 68], index=[\"bob\", \"alice\"], name=\"weights\")\ns6\n\nbob      83\nalice    68\nName: weights, dtype: int64\n\n\n\ns6.name\n\n'weights'"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#series-그래프-출력",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#series-그래프-출력",
    "title": "새로 추가한 포스팅3333",
    "section": "Series 그래프 출력",
    "text": "Series 그래프 출력\n맷플롯립을 사용해 Series 데이터를 쉽게 그래프로 출력할 수 있습니다(맷플롯립에 대한 자세한 설명은 맷플롯립 튜토리얼을 참고하세요). 맷플롯립을 임포트하고 plot() 메서드를 호출하면 끝입니다:\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ntemperatures = [4.4,5.1,6.1,6.2,6.1,6.1,5.7,5.2,4.7,4.1,3.9,3.5]\ns7 = pd.Series(temperatures, name=\"Temperature\")\ns7.plot()\nplt.show()\n\n\n\n\n데이터를 그래프로 출력하는데 많은 옵션이 있습니다. 여기에서 모두 나열할 필요는 없습니다. 특정 종류의 그래프(히스토그램, 파이 차트 등)가 필요하면 판다스 문서의 시각화 섹션에서 예제 코드를 참고하세요."
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#시간-범위",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#시간-범위",
    "title": "새로 추가한 포스팅3333",
    "section": "시간 범위",
    "text": "시간 범위\n먼저 pd.date_range()를 사용해 시계열을 만들어 보죠. 이 함수는 2016년 10월 29일 5:30pm에서 시작하여 12시간마다 하나의 datetime을 담고 있는 DatetimeIndex를 반환합니다.\n\ndates = pd.date_range('2016/10/29 5:30pm', periods=12, freq='H')\ndates\n\nDatetimeIndex(['2016-10-29 17:30:00', '2016-10-29 18:30:00',\n               '2016-10-29 19:30:00', '2016-10-29 20:30:00',\n               '2016-10-29 21:30:00', '2016-10-29 22:30:00',\n               '2016-10-29 23:30:00', '2016-10-30 00:30:00',\n               '2016-10-30 01:30:00', '2016-10-30 02:30:00',\n               '2016-10-30 03:30:00', '2016-10-30 04:30:00'],\n              dtype='datetime64[ns]', freq='H')\n\n\n\npd.date_range('2020-10-07', '2020-10-20', freq='3D')\n\nDatetimeIndex(['2020-10-07', '2020-10-10', '2020-10-13', '2020-10-16',\n               '2020-10-19'],\n              dtype='datetime64[ns]', freq='3D')\n\n\n이 DatetimeIndex를 Series의 인덱스로 사용할수 있습니다:\n\ntemp_series = pd.Series(temperatures, dates)\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n이 시리즈를 그래프로 출력해 보죠:\n\ntemp_series.plot(kind=\"bar\")\n\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#리샘플링",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#리샘플링",
    "title": "새로 추가한 포스팅3333",
    "section": "리샘플링",
    "text": "리샘플링\n판다스는 매우 간단하게 시계열을 리샘플링할 수 있습니다. resample() 메서드를 호출하고 새로운 주기를 지정하면 됩니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\")\ntemp_series_freq_2H\n\n<pandas.core.resample.DatetimeIndexResampler object at 0x000001957FB36490>\n\n\n리샘플링 연산은 사실 지연된 연산입니다. (https://ko.wikipedia.org/wiki/%EB%8A%90%EA%B8%8B%ED%95%9C_%EA%B3%84%EC%82%B0%EB%B2%95) 그래서 Series 객체 대신 DatetimeIndexResampler 객체가 반환됩니다. 실제 리샘플링 연산을 수행하려면 mean() 같은 메서드를 호출할 수 있습니다. 이 메서드는 연속적인 시간 쌍에 대해 평균을 계산합니다:\n\ntemp_series_freq_2H = temp_series_freq_2H.mean()\n\n\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n결과를 그래프로 출력해 보죠:\n\ntemp_series_freq_2H.plot(kind=\"bar\")\nplt.show()\n\n\n\n\n2시간 간격으로 어떻게 값이 수집되었는지 확인해 보세요. 예를 들어 6-8pm 간격을 보면 6:30pm에서 5.1이고 7:30pm에서 6.1입니다. 리샘플링 후에 5.1과 6.1의 평균인 5.6 하나를 얻었습니다. 평균말고 어떤 집계 함수(aggregation function)도 사용할 수 있습니다. 예를 들어 각 기간에서 최솟값을 찾을 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").mean()\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.40\n2016-10-29 18:00:00    5.60\n2016-10-29 20:00:00    6.15\n2016-10-29 22:00:00    5.90\n2016-10-30 00:00:00    4.95\n2016-10-30 02:00:00    4.00\n2016-10-30 04:00:00    3.50\nFreq: 2H, dtype: float64\n\n\n또는 동일한 효과를 내는 apply() 메서드를 사용할 수 있습니다:\n\ntemp_series_freq_2H = temp_series.resample(\"2H\").apply(np.min)\ntemp_series_freq_2H\n\n2016-10-29 16:00:00    4.4\n2016-10-29 18:00:00    5.1\n2016-10-29 20:00:00    6.1\n2016-10-29 22:00:00    5.7\n2016-10-30 00:00:00    4.7\n2016-10-30 02:00:00    3.9\n2016-10-30 04:00:00    3.5\nFreq: 2H, dtype: float64"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#업샘플링과-보간",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#업샘플링과-보간",
    "title": "새로 추가한 포스팅3333",
    "section": "업샘플링과 보간",
    "text": "업샘플링과 보간\n다운샘플링의 예를 보았습니다. 하지만 업샘플링(즉, 빈도를 높입니다)도 할 수 있습니다. 하지만 데이터에 구멍을 만듭니다:\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").mean()\ntemp_series_freq_15min.head(n=10) # `head`는 상위 n 개의 값만 출력합니다\n\n2016-10-29 17:30:00    4.4\n2016-10-29 17:45:00    NaN\n2016-10-29 18:00:00    NaN\n2016-10-29 18:15:00    NaN\n2016-10-29 18:30:00    5.1\n2016-10-29 18:45:00    NaN\n2016-10-29 19:00:00    NaN\n2016-10-29 19:15:00    NaN\n2016-10-29 19:30:00    6.1\n2016-10-29 19:45:00    NaN\nFreq: 15T, dtype: float64\n\n\n한가지 방법은 보간으로 사이를 채우는 것입니다. 이렇게 하려면 interpolate() 메서드를 호출합니다. 기본값은 선형 보간이지만 3차 보간(cubic interpolation) 같은 다른 방법을 선택할 수 있습니다: https://bskyvision.com/789\n\ntemp_series_freq_15min = temp_series.resample(\"15Min\").interpolate(method=\"cubic\")\ntemp_series_freq_15min.head(n=10)\n\n2016-10-29 17:30:00    4.400000\n2016-10-29 17:45:00    4.452911\n2016-10-29 18:00:00    4.605113\n2016-10-29 18:15:00    4.829758\n2016-10-29 18:30:00    5.100000\n2016-10-29 18:45:00    5.388992\n2016-10-29 19:00:00    5.669887\n2016-10-29 19:15:00    5.915839\n2016-10-29 19:30:00    6.100000\n2016-10-29 19:45:00    6.203621\nFreq: 15T, dtype: float64\n\n\n\ntemp_series.plot(label=\"Period: 1 hour\")\ntemp_series_freq_15min.plot(label=\"Period: 15 minutes\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#시간대",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#시간대",
    "title": "새로 추가한 포스팅3333",
    "section": "시간대",
    "text": "시간대\n기본적으로 datetime은 단순합니다. 시간대(timezone)을 고려하지 않죠. 따라서 2016-10-30 02:30는 파리나 뉴욕이나 2016년 10월 30일 2:30pm입니다. tz_localize() 메서드로 시간대를 고려한 datetime을 만들 수 있습니다: https://www.timeanddate.com/time/map/\n\ntemp_series\n\n2016-10-29 17:30:00    4.4\n2016-10-29 18:30:00    5.1\n2016-10-29 19:30:00    6.1\n2016-10-29 20:30:00    6.2\n2016-10-29 21:30:00    6.1\n2016-10-29 22:30:00    6.1\n2016-10-29 23:30:00    5.7\n2016-10-30 00:30:00    5.2\n2016-10-30 01:30:00    4.7\n2016-10-30 02:30:00    4.1\n2016-10-30 03:30:00    3.9\n2016-10-30 04:30:00    3.5\nFreq: H, dtype: float64\n\n\n\ntemp_series_ny = temp_series.tz_localize(\"America/New_York\")\ntemp_series_ny\n\n2016-10-29 17:30:00-04:00    4.4\n2016-10-29 18:30:00-04:00    5.1\n2016-10-29 19:30:00-04:00    6.1\n2016-10-29 20:30:00-04:00    6.2\n2016-10-29 21:30:00-04:00    6.1\n2016-10-29 22:30:00-04:00    6.1\n2016-10-29 23:30:00-04:00    5.7\n2016-10-30 00:30:00-04:00    5.2\n2016-10-30 01:30:00-04:00    4.7\n2016-10-30 02:30:00-04:00    4.1\n2016-10-30 03:30:00-04:00    3.9\n2016-10-30 04:30:00-04:00    3.5\ndtype: float64\n\n\n모든 datetime에 -04:00이 추가됩니다. 즉 모든 시간은 UTC - 4시간을 의미합니다.\n다음처럼 파리 시간대로 바꿀 수 있습니다:\n\ntemp_series_paris = temp_series_ny.tz_convert(\"Europe/Paris\")\ntemp_series_paris\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64\n\n\nUTC와의 차이가 +02:00에서 +01:00으로 바뀐 것을 알 수 있습니다. 이는 프랑스가 10월 30일 3am에 겨울 시간으로 바꾸기 때문입니다(2am으로 바뀝니다). 따라서 2:30am이 두 번 등장합니다! 시간대가 없는 표현으로 돌아가 보죠(시간대가 없이 지역 시간으로 매시간 로그를 기록하는 경우 이와 비슷할 것입니다):\n\ntemp_series_paris_naive = temp_series_paris.tz_localize(None)\ntemp_series_paris_naive\n\n2016-10-29 23:30:00    4.4\n2016-10-30 00:30:00    5.1\n2016-10-30 01:30:00    6.1\n2016-10-30 02:30:00    6.2\n2016-10-30 02:30:00    6.1\n2016-10-30 03:30:00    6.1\n2016-10-30 04:30:00    5.7\n2016-10-30 05:30:00    5.2\n2016-10-30 06:30:00    4.7\n2016-10-30 07:30:00    4.1\n2016-10-30 08:30:00    3.9\n2016-10-30 09:30:00    3.5\ndtype: float64\n\n\n이렇게 되면 02:30이 정말 애매합니다. 시간대가 없는 datetime을 파리 시간대로 바꿀 때 에러가 발생합니다:\n\ntry:\n    temp_series_paris_naive.tz_localize(\"Europe/Paris\")\nexcept Exception as e:\n    print(type(e))\n    print(e)\n\n<class 'pytz.exceptions.AmbiguousTimeError'>\nCannot infer dst time from 2016-10-30 02:30:00, try using the 'ambiguous' argument\n\n\n다행히 ambiguous 매개변수를 사용하면 판다스가 타임스탬프의 순서를 기반으로 적절한 DST(일광 절약 시간제)를 추측합니다:\nhttps://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=tori-tours&logNo=221221361831\n\ntemp_series_paris_naive.tz_localize(\"Europe/Paris\", ambiguous=\"infer\")\n\n2016-10-29 23:30:00+02:00    4.4\n2016-10-30 00:30:00+02:00    5.1\n2016-10-30 01:30:00+02:00    6.1\n2016-10-30 02:30:00+02:00    6.2\n2016-10-30 02:30:00+01:00    6.1\n2016-10-30 03:30:00+01:00    6.1\n2016-10-30 04:30:00+01:00    5.7\n2016-10-30 05:30:00+01:00    5.2\n2016-10-30 06:30:00+01:00    4.7\n2016-10-30 07:30:00+01:00    4.1\n2016-10-30 08:30:00+01:00    3.9\n2016-10-30 09:30:00+01:00    3.5\ndtype: float64"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#기간",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#기간",
    "title": "새로 추가한 포스팅3333",
    "section": "기간",
    "text": "기간\npd.period_range() 함수는 DatetimeIndex가 아니라 PeriodIndex를 반환합니다. 예를 들어 2016과 2017년의 전체 분기를 가져와 보죠:\n\nquarters = pd.period_range('2016Q1', periods=8, freq='Q')\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\nPeriodIndex에 숫자 N을 추가하면 PeriodIndex 빈도의 N 배만큼 이동시킵니다:\n\nquarters + 3\n\nPeriodIndex(['2016Q4', '2017Q1', '2017Q2', '2017Q3', '2017Q4', '2018Q1',\n             '2018Q2', '2018Q3'],\n            dtype='period[Q-DEC]')\n\n\nasfreq() 메서드를 사용하면 PeriodIndex의 빈도를 바꿀 수 있습니다. 모든 기간이 늘어나거나 줄어듭니다. 예를 들어 분기 기간을 모두 월별 기간으로 바꾸어 보죠:\n\nquarters.asfreq(\"M\")\n\nPeriodIndex(['2016-03', '2016-06', '2016-09', '2016-12', '2017-03', '2017-06',\n             '2017-09', '2017-12'],\n            dtype='period[M]')\n\n\n\nquarters\n\nPeriodIndex(['2016Q1', '2016Q2', '2016Q3', '2016Q4', '2017Q1', '2017Q2',\n             '2017Q3', '2017Q4'],\n            dtype='period[Q-DEC]')\n\n\n기본적으로 asfreq는 각 기간의 끝에 맞춥니다. 기간의 시작에 맞추도록 변경할 수 있습니다:\n\nquarters.asfreq(\"M\", how=\"start\")\n\nPeriodIndex(['2016-01', '2016-04', '2016-07', '2016-10', '2017-01', '2017-04',\n             '2017-07', '2017-10'],\n            dtype='period[M]')\n\n\n간격을 늘릴 수도 있습니다: pandas 공식 메뉴얼 참조: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html\n\nquarters.asfreq(\"A\")\n\nPeriodIndex(['2016', '2016', '2016', '2016', '2017', '2017', '2017', '2017'], dtype='period[A-DEC]')\n\n\n물론 PeriodIndex로 Series를 만들 수 있습니다:\n\nquarterly_revenue = pd.Series([300, 320, 290, 390, 320, 360, 310, 410], index = quarters)\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nquarterly_revenue.plot(kind=\"line\")\nplt.show()\n\n\n\n\nto_timestamp를 호출해서 기간을 타임스탬프로 변경할 수 있습니다. 기본적으로 기간의 첫 번째 날을 반환합니다. 하지만 how와 freq를 지정해서 기간의 마지막 시간을 얻을 수 있습니다:\n\nquarterly_revenue\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n\nlast_hours = quarterly_revenue.to_timestamp(how=\"end\", freq=\"H\")\nlast_hours\n\n2016-03-31 23:59:59.999999999    300\n2016-06-30 23:59:59.999999999    320\n2016-09-30 23:59:59.999999999    290\n2016-12-31 23:59:59.999999999    390\n2017-03-31 23:59:59.999999999    320\n2017-06-30 23:59:59.999999999    360\n2017-09-30 23:59:59.999999999    310\n2017-12-31 23:59:59.999999999    410\ndtype: int64\n\n\nto_peroid를 호출하면 다시 기간으로 돌아갑니다:\n\nlast_hours.to_period()\n\n2016Q1    300\n2016Q2    320\n2016Q3    290\n2016Q4    390\n2017Q1    320\n2017Q2    360\n2017Q3    310\n2017Q4    410\nFreq: Q-DEC, dtype: int64\n\n\n판다스는 여러 가지 시간 관련 함수를 많이 제공합니다. 온라인 문서를 확인해 보세요. 예를 하나 들면 2016년 매월 마지막 업무일의 9시를 얻는 방법은 다음과 같습니다:\n\nmonths_2022 = pd.period_range(\"2022\", periods=12, freq=\"M\")\none_day_after_last_days = months_2022.asfreq(\"D\") + 1\nlast_bdays = one_day_after_last_days.to_timestamp() - pd.tseries.offsets.BDay(n=1)\nlast_bdays.to_period(\"H\") + 9\n\nPeriodIndex(['2022-01-31 09:00', '2022-02-28 09:00', '2022-03-31 09:00',\n             '2022-04-29 09:00', '2022-05-31 09:00', '2022-06-30 09:00',\n             '2022-07-29 09:00', '2022-08-31 09:00', '2022-09-30 09:00',\n             '2022-10-31 09:00', '2022-11-30 09:00', '2022-12-30 09:00'],\n            dtype='period[H]')"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-만들기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-만들기",
    "title": "새로 추가한 포스팅3333",
    "section": "DataFrame 만들기",
    "text": "DataFrame 만들기\nSeries 객체의 딕셔너리를 전달하여 데이터프레임을 만들 수 있습니다:\n\npeople_dict = {\n    \"weight\": pd.Series([68, 83, 112], index=[\"alice\", \"bob\", \"charles\"]),\n    \"birthyear\": pd.Series([1984, 1985, 1992], index=[\"bob\", \"alice\", \"charles\"], name=\"year\"),\n    \"children\": pd.Series([0, 3], index=[\"charles\", \"bob\"]),\n    \"hobby\": pd.Series([\"Biking\", \"Dancing\"], index=[\"alice\", \"bob\"]),\n}\npeople = pd.DataFrame(people_dict)\npeople\n\n\n\n\n\n  \n    \n      \n      weight\n      birthyear\n      children\n      hobby\n    \n  \n  \n    \n      alice\n      68\n      1985\n      NaN\n      Biking\n    \n    \n      bob\n      83\n      1984\n      3.0\n      Dancing\n    \n    \n      charles\n      112\n      1992\n      0.0\n      NaN\n    \n  \n\n\n\n\n몇가지 알아 두어야 할 것은 다음과 같습니다:\n\nSeries는 인덱스를 기반으로 자동으로 정렬됩니다.\n누란된 값은 NaN으로 표현됩니다.\nSeries 이름은 무시됩니다(\"year\"란 이름은 삭제됩니다).\nDataFrame은 주피터 노트북에서 멋지게 출력됩니다!\n\n예상하는 방식으로 열을 참조할 수 있고 Series 객체가 반환됩니다:\n\npeople[\"birthyear\"]\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n동시에 여러 개의 열을 선택할 수 있습니다:\n\npeople[[\"birthyear\", \"hobby\"]]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n열 리스트나 행 인덱스 레이블을 DataFrame 생성자에 전달하면 해당 열과 행으로 채워진 데이터프레임이 반환됩니다. 예를 들면:\n\npeople_dict\n\n{'weight': alice       68\n bob         83\n charles    112\n dtype: int64,\n 'birthyear': bob        1984\n alice      1985\n charles    1992\n Name: year, dtype: int64,\n 'children': charles    0\n bob        3\n dtype: int64,\n 'hobby': alice     Biking\n bob      Dancing\n dtype: object}\n\n\n\nd2 = pd.DataFrame(\n        people_dict,\n        columns=[\"birthyear\", \"weight\", \"height\"],\n        index=[\"bob\", \"alice\", \"eugene\"]\n     )\n\n\nd2\n\n\n\n\n\n  \n    \n      \n      birthyear\n      weight\n      height\n    \n  \n  \n    \n      bob\n      1984.0\n      83.0\n      NaN\n    \n    \n      alice\n      1985.0\n      68.0\n      NaN\n    \n    \n      eugene\n      NaN\n      NaN\n      NaN\n    \n  \n\n\n\n\nDataFrame을 만드는 또 다른 편리한 방법은 ndarray나 리스트의 리스트로 모든 값을 생성자에게 전달하고 열 이름과 행 인덱스 레이블을 각기 지정하는 것입니다:\n\nvalues = [\n            [1985, np.nan, \"Biking\",   68],\n            [1984, 3,      \"Dancing\",  83],\n            [1992, 0,      np.nan,    112]\n         ]\nd3 = pd.DataFrame(\n        values,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3.0\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0.0\n      NaN\n      112\n    \n  \n\n\n\n\n누락된 값을 지정하려면 np.nan이나 넘파이 마스크 배열을 사용합니다:\ndtype = object는 문자열 데이터를 의미\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array\n\nmasked_array(\n  data=[[1985, nan, 'Biking', 68],\n        [1984, 3, 'Dancing', 83],\n        [1992, 0, nan, 112]],\n  mask=False,\n  fill_value='?',\n  dtype=object)\n\n\n\nmasked_array = np.ma.asarray(values, dtype=object)\nmasked_array[(0, 2), (1, 2)] = np.ma.masked\nd3 = pd.DataFrame(\n        masked_array,\n        columns=[\"birthyear\", \"children\", \"hobby\", \"weight\"],\n        index=[\"alice\", \"bob\", \"charles\"]\n     )\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\nndarray 대신에 DataFrame 객체를 전달할 수도 있습니다:\n\nd3\n\n\n\n\n\n  \n    \n      \n      birthyear\n      children\n      hobby\n      weight\n    \n  \n  \n    \n      alice\n      1985\n      NaN\n      Biking\n      68\n    \n    \n      bob\n      1984\n      3\n      Dancing\n      83\n    \n    \n      charles\n      1992\n      0\n      NaN\n      112\n    \n  \n\n\n\n\n\nd4 = pd.DataFrame(\n         d3,\n         columns=[\"hobby\", \"children\"],\n         index=[\"alice\", \"bob\"]\n     )\nd4\n\n\n\n\n\n  \n    \n      \n      hobby\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      NaN\n    \n    \n      bob\n      Dancing\n      3\n    \n  \n\n\n\n\n딕셔너리의 딕셔너리(또는 리스트의 리스트)로 DataFrame을 만들 수 있습니다:\n\npeople = pd.DataFrame({\n    \"birthyear\": {\"alice\":1985, \"bob\": 1984, \"charles\": 1992},\n    \"hobby\": {\"alice\":\"Biking\", \"bob\": \"Dancing\"},\n    \"weight\": {\"alice\":68, \"bob\": 83, \"charles\": 112},\n    \"children\": {\"bob\": 3, \"charles\": 0}\n})\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#멀티-인덱싱",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#멀티-인덱싱",
    "title": "새로 추가한 포스팅3333",
    "section": "멀티 인덱싱",
    "text": "멀티 인덱싱\n모든 열이 같은 크기의 튜플이면 멀티 인덱스로 인식합니다. 열 인덱스 레이블에도 같은 방식이 적용됩니다. 예를 들면:\n\nd5 = pd.DataFrame(\n  {\n    (\"public\", \"birthyear\"):\n        {(\"Paris\",\"alice\"):1985, (\"Paris\",\"bob\"): 1984, (\"London\",\"charles\"): 1992},\n    (\"public\", \"hobby\"):\n        {(\"Paris\",\"alice\"):\"Biking\", (\"Paris\",\"bob\"): \"Dancing\"},\n    (\"private\", \"weight\"):\n        {(\"Paris\",\"alice\"):68, (\"Paris\",\"bob\"): 83, (\"London\",\"charles\"): 112},\n    (\"private\", \"children\"):\n        {(\"Paris\", \"alice\"):np.nan, (\"Paris\",\"bob\"): 3, (\"London\",\"charles\"): 0}\n  }\n)\nd5\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n이제 \"public\" 열을 모두 담은 DataFrame을 손쉽게 만들 수 있습니다:\n\nd5[\"public\"]\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n    \n    \n      bob\n      1984\n      Dancing\n    \n    \n      London\n      charles\n      1992\n      NaN\n    \n  \n\n\n\n\n\nd5[\"public\", \"hobby\"]  # d5[\"public\"][\"hobby\"]와 같습니다.\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: (public, hobby), dtype: object\n\n\n\nd5[\"public\"]['hobby']\n\nParis   alice       Biking\n        bob        Dancing\nLondon  charles        NaN\nName: hobby, dtype: object"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#레벨-낮추기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#레벨-낮추기",
    "title": "새로 추가한 포스팅3333",
    "section": "레벨 낮추기",
    "text": "레벨 낮추기\nd5를 다시 확인해 보죠:\n\nd5\n\n\n\n\n\n  \n    \n      \n      \n      public\n      private\n    \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n열의 레벨(level)이 2개이고 인덱스 레벨이 2개입니다. droplevel()을 사용해 열 레벨을 낮출 수 있습니다(인덱스도 마찬가지입니다):\n\nd5.columns = d5.columns.droplevel(level = 0)\nd5\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd6 = d5.copy()\nd6.index = d6.index.droplevel(level = 0)\nd6\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#전치",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#전치",
    "title": "새로 추가한 포스팅3333",
    "section": "전치",
    "text": "전치\nT 속성을 사용해 열과 인덱스를 바꿀 수 있습니다:\n\nd5\n\n\n\n\n\n  \n    \n      \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      Paris\n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      London\n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\nd6 = d5.T\nd6\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#레벨-스택과-언스택",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#레벨-스택과-언스택",
    "title": "새로 추가한 포스팅3333",
    "section": "레벨 스택과 언스택",
    "text": "레벨 스택과 언스택\nstack() 메서드는 가장 낮은 열 레벨을 가장 낮은 인덱스 뒤에 추가합니다:\n\nd6\n\n\n\n\n\n  \n    \n      \n      Paris\n      London\n    \n    \n      \n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      1985\n      1984\n      1992\n    \n    \n      hobby\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      68\n      83\n      112\n    \n    \n      children\n      NaN\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nd7 = d6.stack()\nd7\n\n\n\n\n\n  \n    \n      \n      \n      London\n      Paris\n    \n  \n  \n    \n      birthyear\n      alice\n      NaN\n      1985\n    \n    \n      bob\n      NaN\n      1984\n    \n    \n      charles\n      1992\n      NaN\n    \n    \n      hobby\n      alice\n      NaN\n      Biking\n    \n    \n      bob\n      NaN\n      Dancing\n    \n    \n      weight\n      alice\n      NaN\n      68\n    \n    \n      bob\n      NaN\n      83\n    \n    \n      charles\n      112\n      NaN\n    \n    \n      children\n      bob\n      NaN\n      3.0\n    \n    \n      charles\n      0.0\n      NaN\n    \n  \n\n\n\n\nNaN 값이 생겼습니다. 이전에 없던 조합이 생겼기 때문입니다(예를 들어 London에 bob이 없었습니다).\nunstack()을 호출하면 반대가 됩니다. 여기에서도 많은 NaN 값이 생성됩니다.\n\nd8 = d7.unstack()\nd8\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN\n    \n  \n\n\n\n\nunstack을 다시 호출하면 Series 객체가 만들어 집니다:\n\nd9 = d8.unstack()\nd9\n\nLondon  alice    birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        bob      birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\n        charles  birthyear       1992\n                 children         0.0\n                 hobby            NaN\n                 weight           112\nParis   alice    birthyear       1985\n                 children         NaN\n                 hobby         Biking\n                 weight            68\n        bob      birthyear       1984\n                 children         3.0\n                 hobby        Dancing\n                 weight            83\n        charles  birthyear        NaN\n                 children         NaN\n                 hobby            NaN\n                 weight           NaN\ndtype: object\n\n\nstack()과 unstack() 메서드를 사용할 때 스택/언스택할 level을 선택할 수 있습니다. 심지어 한 번에 여러 개의 레벨을 스택/언스택할 수도 있습니다:\n\nd10 = d9.unstack(level = (0,1))\nd10\n\n\n\n\n\n  \n    \n      \n      London\n      Paris\n    \n    \n      \n      alice\n      bob\n      charles\n      alice\n      bob\n      charles\n    \n  \n  \n    \n      birthyear\n      NaN\n      NaN\n      1992\n      1985\n      1984\n      NaN\n    \n    \n      children\n      NaN\n      NaN\n      0.0\n      NaN\n      3.0\n      NaN\n    \n    \n      hobby\n      NaN\n      NaN\n      NaN\n      Biking\n      Dancing\n      NaN\n    \n    \n      weight\n      NaN\n      NaN\n      112\n      68\n      83\n      NaN"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#대부분의-메서드는-수정된-복사본을-반환합니다",
    "title": "새로 추가한 포스팅3333",
    "section": "대부분의 메서드는 수정된 복사본을 반환합니다",
    "text": "대부분의 메서드는 수정된 복사본을 반환합니다\n눈치챘겠지만 stack()과 unstack() 메서드는 객체를 수정하지 않습니다. 대신 복사본을 만들어 반환합니다. 판다스에 있는 대부분의 메서드들이 이렇게 동작합니다.\nStack & Unstack + Pivot에 대한 설명 참고 https://pandas.pydata.org/docs/user_guide/reshaping.html\nData Reshaping!\n\nPivot\n\nimport pandas._testing as tm\n\ndef unpivot(frame):\n    N, K = frame.shape\n    data = {\n        \"value\": frame.to_numpy().ravel(\"F\"),\n        \"variable\": np.asarray(frame.columns).repeat(N),\n        \"date\": np.tile(np.asarray(frame.index), K),\n    }\n    return pd.DataFrame(data, columns=[\"date\", \"variable\", \"value\"])\n\ndf = unpivot(tm.makeTimeDataFrame(3))\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -0.106608\n    \n    \n      1\n      2000-01-04\n      A\n      -0.152363\n    \n    \n      2\n      2000-01-05\n      A\n      -0.590536\n    \n    \n      3\n      2000-01-03\n      B\n      -0.504281\n    \n    \n      4\n      2000-01-04\n      B\n      1.571623\n    \n    \n      5\n      2000-01-05\n      B\n      -0.516232\n    \n    \n      6\n      2000-01-03\n      C\n      -1.973264\n    \n    \n      7\n      2000-01-04\n      C\n      -0.453793\n    \n    \n      8\n      2000-01-05\n      C\n      -0.240602\n    \n    \n      9\n      2000-01-03\n      D\n      -0.394477\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224817\n    \n    \n      11\n      2000-01-05\n      D\n      0.130486\n    \n  \n\n\n\n\nTo select out everything for variable A we could do:\n\nfiltered = df[df[\"variable\"] == \"A\"]\nfiltered\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -0.106608\n    \n    \n      1\n      2000-01-04\n      A\n      -0.152363\n    \n    \n      2\n      2000-01-05\n      A\n      -0.590536\n    \n  \n\n\n\n\nBut suppose we wish to do time series operations with the variables. A better representation would be where the columns are the unique variables and an index of dates identifies individual observations. To reshape the data into this form, we use the DataFrame.pivot() method (also implemented as a top level function pivot()):\n\npivoted = df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\n\npivoted\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -0.106608\n      -0.504281\n      -1.973264\n      -0.394477\n    \n    \n      2000-01-04\n      -0.152363\n      1.571623\n      -0.453793\n      -0.224817\n    \n    \n      2000-01-05\n      -0.590536\n      -0.516232\n      -0.240602\n      0.130486\n    \n  \n\n\n\n\n\npivoted.columns\n\nIndex(['A', 'B', 'C', 'D'], dtype='object', name='variable')\n\n\n\npivoted.index\n\nDatetimeIndex(['2000-01-03', '2000-01-04', '2000-01-05'], dtype='datetime64[ns]', name='date', freq=None)\n\n\nIf the values argument is omitted, and the input DataFrame has more than one column of values which are not used as column or index inputs to pivot(), then the resulting “pivoted” DataFrame will have hierarchical columns whose topmost level indicates the respective value column:\n\ndf[\"value2\"] = df[\"value\"] * 2\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      date\n      variable\n      value\n      value2\n    \n  \n  \n    \n      0\n      2000-01-03\n      A\n      -0.106608\n      -0.213216\n    \n    \n      1\n      2000-01-04\n      A\n      -0.152363\n      -0.304727\n    \n    \n      2\n      2000-01-05\n      A\n      -0.590536\n      -1.181073\n    \n    \n      3\n      2000-01-03\n      B\n      -0.504281\n      -1.008562\n    \n    \n      4\n      2000-01-04\n      B\n      1.571623\n      3.143247\n    \n    \n      5\n      2000-01-05\n      B\n      -0.516232\n      -1.032464\n    \n    \n      6\n      2000-01-03\n      C\n      -1.973264\n      -3.946528\n    \n    \n      7\n      2000-01-04\n      C\n      -0.453793\n      -0.907586\n    \n    \n      8\n      2000-01-05\n      C\n      -0.240602\n      -0.481204\n    \n    \n      9\n      2000-01-03\n      D\n      -0.394477\n      -0.788953\n    \n    \n      10\n      2000-01-04\n      D\n      -0.224817\n      -0.449633\n    \n    \n      11\n      2000-01-05\n      D\n      0.130486\n      0.260972\n    \n  \n\n\n\n\n\npivoted = df.pivot(index=\"date\", columns=\"variable\")\n\npivoted\n\n\n\n\n\n  \n    \n      \n      value\n      value2\n    \n    \n      variable\n      A\n      B\n      C\n      D\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -0.106608\n      -0.504281\n      -1.973264\n      -0.394477\n      -0.213216\n      -1.008562\n      -3.946528\n      -0.788953\n    \n    \n      2000-01-04\n      -0.152363\n      1.571623\n      -0.453793\n      -0.224817\n      -0.304727\n      3.143247\n      -0.907586\n      -0.449633\n    \n    \n      2000-01-05\n      -0.590536\n      -0.516232\n      -0.240602\n      0.130486\n      -1.181073\n      -1.032464\n      -0.481204\n      0.260972\n    \n  \n\n\n\n\n\npivoted['value']\n\n\n\n\n\n  \n    \n      variable\n      A\n      B\n      C\n      D\n    \n    \n      date\n      \n      \n      \n      \n    \n  \n  \n    \n      2000-01-03\n      -0.106608\n      -0.504281\n      -1.973264\n      -0.394477\n    \n    \n      2000-01-04\n      -0.152363\n      1.571623\n      -0.453793\n      -0.224817\n    \n    \n      2000-01-05\n      -0.590536\n      -0.516232\n      -0.240602\n      0.130486"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#행-참조하기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#행-참조하기",
    "title": "새로 추가한 포스팅3333",
    "section": "행 참조하기",
    "text": "행 참조하기\npeople DataFrame으로 돌아가 보죠:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\nloc 속성으로 열 대신 행을 참조할 수 있습니다. DataFrame의 열 이름이 행 인덱스 레이블로 매핑된 Series 객체가 반환됩니다:\n\npeople['birthyear']\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople.loc[\"charles\"]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\niloc 속성을 사용해 정수 인덱스로 행을 참조할 수 있습니다:\n\npeople.iloc[2]\n\nbirthyear    1992\nhobby         NaN\nweight        112\nchildren      0.0\nName: charles, dtype: object\n\n\n행을 슬라이싱할 수 있으며 DataFrame 객체가 반환됩니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\npeople.iloc[1:3]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n마자믹으로 불리언 배열을 전달하여 해당하는 행을 가져올 수 있습니다:\n\npeople[np.array([True, False, True])]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n불리언 표현식을 사용할 때 아주 유용합니다:\n\npeople[\"birthyear\"] < 1990\n\nalice       True\nbob         True\ncharles    False\nName: birthyear, dtype: bool\n\n\n\npeople[people[\"birthyear\"] < 1990]\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#열-추가-삭제",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#열-추가-삭제",
    "title": "새로 추가한 포스팅3333",
    "section": "열 추가, 삭제",
    "text": "열 추가, 삭제\nDataFrame을 Series의 딕셔너리처럼 다룰 수 있습니다. 따라서 다음 같이 쓸 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n    \n  \n\n\n\n\n\npeople[\"age\"] = 2022 - people[\"birthyear\"]  # \"age\" 열을 추가합니다\npeople[\"over 30\"] = people[\"age\"] > 30      # \"over 30\" 열을 추가합니다\n\npeople\n\n\n\n\n\n  \n    \n      \n      birthyear\n      hobby\n      weight\n      children\n      age\n      over 30\n    \n  \n  \n    \n      alice\n      1985\n      Biking\n      68\n      NaN\n      37\n      True\n    \n    \n      bob\n      1984\n      Dancing\n      83\n      3.0\n      38\n      True\n    \n    \n      charles\n      1992\n      NaN\n      112\n      0.0\n      30\n      False\n    \n  \n\n\n\n\n\nbirthyears = people.pop(\"birthyear\")\ndel people[\"children\"]\n\n\nbirthyears\n\nalice      1985\nbob        1984\ncharles    1992\nName: birthyear, dtype: int64\n\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\n# 딕셔너리도 유사함\nweights = {\"alice\": 68, \"bob\": 83, \"colin\": 86, \"darwin\": 68}\n\n\nweights.pop(\"alice\")\n\n68\n\n\n\nweights\n\n{'bob': 83, 'colin': 86, 'darwin': 68}\n\n\n\ndel weights[\"bob\"]\n\n\nweights\n\n{'colin': 86, 'darwin': 68}\n\n\n새로운 열을 추가할 때 행의 개수는 같아야 합니다. 누락된 행은 NaN으로 채워지고 추가적인 행은 무시됩니다:\n\npeople.index\n\nIndex(['alice', 'bob', 'charles'], dtype='object')\n\n\n\npeople[\"pets\"] = pd.Series({\"bob\": 0, \"charles\": 5, \"eugene\":1})  # alice 누락됨, eugene은 무시됨\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      83\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      112\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n새로운 열을 추가할 때 기본적으로 (오른쪽) 끝에 추가됩니다. insert() 메서드를 사용해 다른 곳에 열을 추가할 수 있습니다:\n\npeople.insert(1, \"height\", [172, 181, 185])\npeople\n\nValueError: cannot insert height, already exists"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#새로운-열-할당하기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#새로운-열-할당하기",
    "title": "새로 추가한 포스팅3333",
    "section": "새로운 열 할당하기",
    "text": "새로운 열 할당하기\nassign() 메서드를 호출하여 새로운 열을 만들 수도 있습니다. 이는 새로운 DataFrame 객체를 반환하며 원본 객체는 변경되지 않습니다:\n\npeople.assign(\n    body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n    has_pets = people[\"pets\"] > 0\n)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      has_pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      False\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n    \n  \n\n\n\n\n\ndel people[\"body_mass_index\"]\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n    \n  \n\n\n\n\n할당문 안에서 만든 열은 접근할 수 없습니다:\n\ntry:\n    people.assign(\n        body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2,\n        overweight = people[\"body_mass_index\"] > 25\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n해결책은 두 개의 연속된 할당문으로 나누는 것입니다:\n\nd6 = people.assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\nd6.assign(overweight = d6[\"body_mass_index\"] > 25)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n임시 변수 d6를 만들면 불편합니다. assign() 메서드를 연결하고 싶겠지만 people 객체가 첫 번째 할당문에서 실제로 수정되지 않기 때문에 작동하지 않습니다:\n\ntry:\n    (people\n         .assign(body_mass_index = people[\"weight\"] / (people[\"height\"] / 100) ** 2)\n         .assign(overweight = people[\"body_mass_index\"] > 25)\n    )\nexcept KeyError as e:\n    print(\"키 에러:\", e)\n\n키 에러: 'body_mass_index'\n\n\n하지만 걱정하지 마세요. 간단한 방법이 있습니다. assign() 메서드에 함수(전형적으로 lambda 함수)를 전달하면 DataFrame을 매개변수로 이 함수를 호출할 것입니다:\n\n(people\n     .assign(body_mass_index = lambda df: df[\"weight\"] / (df[\"height\"] / 100) ** 2)\n     .assign(overweight = lambda df: df[\"body_mass_index\"] > 25)\n)\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n문제가 해결되었군요!\n\npeople[\"body_mass_index\"] = people[\"weight\"] / (people[\"height\"] / 100) ** 2\npeople[\"overweight\"] = people[\"body_mass_index\"]>25\n\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#표현식-평가",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#표현식-평가",
    "title": "새로 추가한 포스팅3333",
    "section": "표현식 평가",
    "text": "표현식 평가\n판다스가 제공하는 뛰어난 기능 하나는 표현식 평가입니다. 이는 numexpr 라이브러리에 의존하기 때문에 설치가 되어 있어야 합니다.\n\npeople\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      height\n      age\n      over 30\n      pets\n      body_mass_index\n      overweight\n    \n  \n  \n    \n      alice\n      Biking\n      68\n      172\n      37\n      True\n      NaN\n      22.985398\n      False\n    \n    \n      bob\n      Dancing\n      83\n      181\n      38\n      True\n      0.0\n      25.335002\n      True\n    \n    \n      charles\n      NaN\n      112\n      185\n      30\n      False\n      5.0\n      32.724617\n      True\n    \n  \n\n\n\n\n\n\"weight / (height/100) ** 2 > 25\"\n\n'weight / (height/100) ** 2 > 25'\n\n\n\npeople.eval(\"weight / (height/100) ** 2 > 25\")\n\nalice      False\nbob         True\ncharles     True\ndtype: bool\n\n\n할당 표현식도 지원됩니다. inplace=True로 지정하면 수정된 복사본을 만들지 않고 바로 DataFrame을 변경합니다:\n\npeople.eval(\"body_mass_index = weight / (height/100) ** 2\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n'@'를 접두어로 사용하여 지역 변수나 전역 변수를 참조할 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\noverweight_threshold = 30\npeople.eval(\"overweight = body_mass_index > @overweight_threshold\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-쿼리하기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-쿼리하기",
    "title": "새로 추가한 포스팅3333",
    "section": "DataFrame 쿼리하기",
    "text": "DataFrame 쿼리하기\nquery() 메서드를 사용하면 쿼리 표현식에 기반하여 DataFrame을 필터링할 수 있습니다:\n\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\npeople.query(\"age > 30 and pets == 0\")\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\npeople[(people[\"age\"]>30) & (people[\"pets\"] == 0)]\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n\nmask = (people[\"age\"]>30) & (people[\"pets\"] == 0)\n\n\npeople[mask]\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-정렬",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-정렬",
    "title": "새로 추가한 포스팅3333",
    "section": "DataFrame 정렬",
    "text": "DataFrame 정렬\nsort_index 메서드를 호출하여 DataFrame을 정렬할 수 있습니다. 기본적으로 인덱스 레이블을 기준으로 오름차순으로 행을 정렬합니다. 여기에서는 내림차순으로 정렬해 보죠:\n\npeople.sort_index(ascending=False)\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n  \n\n\n\n\nsort_index는 DataFrame의 정렬된 복사본을 반환합니다. people을 직접 수정하려면 inplace 매개변수를 True로 지정합니다. 또한 axis=1로 지정하여 열 대신 행을 정렬할 수 있습니다:\n\npeople.sort_index(axis=1, inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83\n    \n  \n\n\n\n\n레이블이 아니라 값을 기준으로 DataFrame을 정렬하려면 sort_values에 정렬하려는 열을 지정합니다:\n\npeople.sort_values(by=\"age\", inplace=True)\npeople\n\n\n\n\n\n  \n    \n      \n      age\n      body_mass_index\n      height\n      hobby\n      over 30\n      overweight\n      pets\n      weight\n    \n  \n  \n    \n      charles\n      30\n      32.724617\n      185\n      NaN\n      False\n      True\n      5.0\n      112\n    \n    \n      alice\n      37\n      22.985398\n      172\n      Biking\n      True\n      False\n      NaN\n      68\n    \n    \n      bob\n      38\n      25.335002\n      181\n      Dancing\n      True\n      False\n      0.0\n      83"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-그래프-그리기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-그래프-그리기",
    "title": "새로 추가한 포스팅3333",
    "section": "DataFrame 그래프 그리기",
    "text": "DataFrame 그래프 그리기\nSeries와 마찬가지로 판다스는 DataFrame 기반으로 멋진 그래프를 손쉽게 그릴 수 있습니다.\n예를 들어 plot 메서드를 호출하여 DataFrame의 데이터에서 선 그래프를 쉽게 그릴 수 있습니다:\n\npeople.plot(kind = \"line\", x = \"body_mass_index\", y = [\"height\", \"weight\"])\nplt.show()\n\n\n\n\n맷플롯립의 함수가 지원하는 다른 매개변수를 사용할 수 있습니다. 예를 들어, 산점도를 그릴 때 맷플롯립의 scatter() 함수의 s 매개변수를 사용해 크기를 지정할 수 있습니다:\n\npeople.plot(kind = \"scatter\", x = \"height\", y = \"weight\", s=[40, 120, 200])\nplt.show()\n\n\n\n\n선택할 수 있는 옵션이 많습니다. 판다스 문서의 시각화 페이지에서 마음에 드는 그래프를 찾아 예제 코드를 살펴 보세요.\n\nHistogram\n\n\ndf4 = pd.DataFrame(\n    {\n        \"a\": np.random.randn(1000) + 1,\n        \"b\": np.random.randn(1000),\n        \"c\": np.random.randn(1000) - 1,\n    },\n    columns=[\"a\", \"b\", \"c\"],\n)\n\nplt.figure();\n\ndf4.plot.hist(alpha=0.5);\n\n<Figure size 432x288 with 0 Axes>\n\n\n\n\n\n\ndf4\n\n\n\n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1.024702\n      0.205680\n      -1.151098\n    \n    \n      1\n      0.932532\n      -0.406178\n      -0.108419\n    \n    \n      2\n      -0.616295\n      -1.700059\n      -1.133808\n    \n    \n      3\n      1.481105\n      -0.520759\n      0.685474\n    \n    \n      4\n      0.736898\n      -0.533195\n      -1.705509\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      995\n      -0.220861\n      -0.182931\n      -1.275840\n    \n    \n      996\n      1.069163\n      -1.368469\n      -0.772002\n    \n    \n      997\n      1.378971\n      -0.554198\n      -2.278343\n    \n    \n      998\n      0.661284\n      -0.835081\n      -0.700224\n    \n    \n      999\n      0.664459\n      -0.083600\n      0.502208\n    \n  \n\n1000 rows × 3 columns\n\n\n\n\ndf4.plot(kind=\"hist\",alpha=0.5, x=\"a\")\nplt.show()\n\n\n\n\n\ndf4['a'].plot.hist()\nplt.show()\n\n\n\n\n\nBoxplot\n\n\ndf\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n    \n  \n  \n    \n      0\n      0.887491\n      0.028126\n      0.003576\n      0.013055\n      0.509898\n    \n    \n      1\n      0.653231\n      0.003621\n      0.969910\n      0.173393\n      0.409486\n    \n    \n      2\n      0.803033\n      0.382834\n      0.195527\n      0.616920\n      0.581911\n    \n    \n      3\n      0.524122\n      0.926863\n      0.170608\n      0.300242\n      0.930059\n    \n    \n      4\n      0.968483\n      0.187320\n      0.839602\n      0.149723\n      0.650208\n    \n    \n      5\n      0.110352\n      0.393050\n      0.719806\n      0.859684\n      0.501955\n    \n    \n      6\n      0.866960\n      0.221862\n      0.892753\n      0.990645\n      0.736521\n    \n    \n      7\n      0.801126\n      0.614989\n      0.057752\n      0.183695\n      0.569820\n    \n    \n      8\n      0.013725\n      0.439573\n      0.021304\n      0.192832\n      0.270145\n    \n    \n      9\n      0.696394\n      0.974029\n      0.351002\n      0.409430\n      0.581877\n    \n  \n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 5), columns=[\"A\", \"B\", \"C\", \"D\", \"E\"])\n\ndf.plot.box();\n\n\n\n\n\ndf = pd.DataFrame(np.random.rand(10, 2), columns=[\"Col1\", \"Col2\"])\n\ndf[\"X\"] = pd.Series([\"A\", \"A\", \"A\", \"A\", \"A\", \"B\", \"B\", \"B\", \"B\", \"B\"])\n\ndf\n\n\n\n\n\n  \n    \n      \n      Col1\n      Col2\n      X\n    \n  \n  \n    \n      0\n      0.078610\n      0.759337\n      A\n    \n    \n      1\n      0.443074\n      0.020230\n      A\n    \n    \n      2\n      0.858927\n      0.881363\n      A\n    \n    \n      3\n      0.912618\n      0.354576\n      A\n    \n    \n      4\n      0.784800\n      0.178885\n      A\n    \n    \n      5\n      0.552493\n      0.020503\n      B\n    \n    \n      6\n      0.895306\n      0.838616\n      B\n    \n    \n      7\n      0.901667\n      0.012848\n      B\n    \n    \n      8\n      0.814965\n      0.330909\n      B\n    \n    \n      9\n      0.211971\n      0.653164\n      B\n    \n  \n\n\n\n\n\nplt.figure();\n\nbp = df.boxplot(by=\"X\")\n\n<Figure size 432x288 with 0 Axes>"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-연산",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#dataframe-연산",
    "title": "새로 추가한 포스팅3333",
    "section": "DataFrame 연산",
    "text": "DataFrame 연산\nDataFrame이 넘파이 배열을 흉내내려는 것은 아니지만 몇 가지 비슷한 점이 있습니다. 예제 DataFrame을 만들어 보죠:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\nDataFrame에 넘파이 수학 함수를 적용하면 모든 값에 이 함수가 적용됩니다:\n\nnp.sqrt(grades)\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      2.828427\n      2.828427\n      3.000000\n    \n    \n      bob\n      3.162278\n      3.000000\n      3.000000\n    \n    \n      charles\n      2.000000\n      2.828427\n      1.414214\n    \n    \n      darwin\n      3.000000\n      3.162278\n      3.162278\n    \n  \n\n\n\n\n비슷하게 DataFrame에 하나의 값을 더하면 DataFrame의 모든 원소에 이 값이 더해집니다. 이를 브로드캐스팅이라고 합니다:\n\ngrades + 1\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      9\n      9\n      10\n    \n    \n      bob\n      11\n      10\n      10\n    \n    \n      charles\n      5\n      9\n      3\n    \n    \n      darwin\n      10\n      11\n      11\n    \n  \n\n\n\n\n물론 산술 연산(*,/,**…)과 조건 연산(>, ==…)을 포함해 모든 이항 연산에도 마찬가지 입니다:\n\ngrades >= 5\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      True\n      True\n      True\n    \n    \n      bob\n      True\n      True\n      True\n    \n    \n      charles\n      False\n      True\n      False\n    \n    \n      darwin\n      True\n      True\n      True\n    \n  \n\n\n\n\nDataFrame의 max, sum, mean 같은 집계 연산은 각 열에 적용되어 Series 객체가 반환됩니다:\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\nall 메서드도 집계 연산입니다: 모든 값이 True인지 아닌지 확인합니다. 모든 학생의 점수가 5 이상인 월을 찾아 보죠:\n\n(grades > 5).all()\n\nsep    False\noct     True\nnov    False\ndtype: bool\n\n\nMost of these functions take an optional axis parameter which lets you specify along which axis of the DataFrame you want the operation executed. The default is axis=0, meaning that the operation is executed vertically (on each column). You can set axis=1 to execute the operation horizontally (on each row). For example, let’s find out which students had all grades greater than 5:\n\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\n(grades > 5).all(axis = 1)\n\nalice       True\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nany 메서드는 하나라도 참이면 True를 반환합니다. 한 번이라도 10점을 받은 사람을 찾아 보죠:\n\n(grades == 10).any(axis = 1)\n\nalice      False\nbob         True\ncharles    False\ndarwin      True\ndtype: bool\n\n\nDataFrame에 Series 객체를 더하면 (또는 다른 이항 연산을 수행하면) 판다스는 DataFrame에 있는 모든 행에 이 연산을 브로드캐스팅합니다. 이는 Series 객체가 DataFrame의 행의 개수와 크기가 같을 때만 동작합니다. 예를 들어 DataFrame의 mean(Series 객체)을 빼보죠:\n\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\ngrades.mean()\n\nsep    7.75\noct    8.75\nnov    7.50\ndtype: float64\n\n\n\ngrades - grades.mean()  # grades - [7.75, 8.75, 7.50] 와 동일\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.25\n      -0.75\n      1.5\n    \n    \n      bob\n      2.25\n      0.25\n      1.5\n    \n    \n      charles\n      -3.75\n      -0.75\n      -5.5\n    \n    \n      darwin\n      1.25\n      1.25\n      2.5\n    \n  \n\n\n\n\n모든 9월 성적에서 7.75를 빼고, 10월 성적에서 8.75를 빼고, 11월 성적에서 7.50을 뺍니다. 이는 다음 DataFrame을 빼는 것과 같습니다:\n\npd.DataFrame([[7.75, 8.75, 7.50]]*4, index=grades.index, columns=grades.columns)\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      7.75\n      8.75\n      7.5\n    \n    \n      bob\n      7.75\n      8.75\n      7.5\n    \n    \n      charles\n      7.75\n      8.75\n      7.5\n    \n    \n      darwin\n      7.75\n      8.75\n      7.5\n    \n  \n\n\n\n\n모든 성적의 전체 평균을 빼고 싶다면 다음과 같은 방법을 사용합니다:\n\n grades.values.mean()\n\n8.0\n\n\n\ngrades - grades.values.mean() # 모든 점수에서 전체 평균(8.00)을 뺍니다\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      1.0\n    \n    \n      bob\n      2.0\n      1.0\n      1.0\n    \n    \n      charles\n      -4.0\n      0.0\n      -6.0\n    \n    \n      darwin\n      1.0\n      2.0\n      2.0"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#자동-정렬-1",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#자동-정렬-1",
    "title": "새로 추가한 포스팅3333",
    "section": "자동 정렬",
    "text": "자동 정렬\nSeries와 비슷하게 여러 개의 DataFrame에 대한 연산을 수행하면 판다스는 자동으로 행 인덱스 레이블로 정렬하지만 열 이름으로도 정렬할 수 있습니다. 10월부터 12월까지 보너스 포인트를 담은 DataFrame을 만들어 보겠습니다:\n\ngrades_array = np.array([[8,8,9],[10,9,9],[4, 8, 2], [9, 10, 10]])\ngrades = pd.DataFrame(grades_array, columns=[\"sep\", \"oct\", \"nov\"], index=[\"alice\",\"bob\",\"charles\",\"darwin\"])\ngrades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8\n      8\n      9\n    \n    \n      bob\n      10\n      9\n      9\n    \n    \n      charles\n      4\n      8\n      2\n    \n    \n      darwin\n      9\n      10\n      10\n    \n  \n\n\n\n\n\nbonus_array = np.array([[0,np.nan,2],[np.nan,1,0],[0, 1, 0], [3, 3, 0]])\nbonus_points = pd.DataFrame(bonus_array, columns=[\"oct\", \"nov\", \"dec\"], index=[\"bob\",\"colin\", \"darwin\", \"charles\"])\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\ngrades + bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      bob\n      NaN\n      NaN\n      9.0\n      NaN\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n덧셈 연산이 수행되었지만 너무 많은 원소가 NaN이 되었습니다. DataFrame을 정렬할 때 일부 열과 행이 한 쪽에만 있기 때문입니다. 다른 쪽에는 누란되었다고 간주합니다(NaN). NaN에 어떤 수를 더하면 NaN이 됩니다."
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#누락된-데이터-다루기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#누락된-데이터-다루기",
    "title": "새로 추가한 포스팅3333",
    "section": "누락된 데이터 다루기",
    "text": "누락된 데이터 다루기\n실제 데이터에서 누락된 데이터를 다루는 경우는 자주 발생합니다. 판다스는 누락된 데이터를 다룰 수 있는 몇 가지 방법을 제공합니다.\n위 데이터에 있는 문제를 해결해 보죠. 예를 들어, 누락된 데이터는 NaN이 아니라 0이 되어야 한다고 결정할 수 있습니다. fillna() 메서드를 사용해 모든 NaN 값을 어떤 값으로 바꿀 수 있습니다:\n\n(grades + bonus_points).fillna(0)\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      bob\n      0.0\n      0.0\n      9.0\n      0.0\n    \n    \n      charles\n      0.0\n      5.0\n      11.0\n      0.0\n    \n    \n      colin\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      darwin\n      0.0\n      11.0\n      10.0\n      0.0\n    \n  \n\n\n\n\n9월의 점수를 0으로 만드는 것은 공정하지 않습니다. 누락된 점수는 그대로 두고, 누락된 보너스 포인트는 0으로 바꿀 수 있습니다:\n\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\n\nfixed_bonus_points = bonus_points.fillna(0) # NA 값 0으로 바꾸기\nfixed_bonus_points.insert(loc=0, column=\"sep\", value=0) # 누락된 컬럼 만들기\nfixed_bonus_points.loc[\"alice\"] = 0 # 누락된 행 만들기\nfixed_bonus_points\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0\n      0.0\n      0.0\n      2.0\n    \n    \n      colin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      darwin\n      0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n\ngrades + fixed_bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      9.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n훨씬 낫네요: 일부 데이터를 꾸며냈지만 덜 불공정합니다.\n누락된 값을 다루는 또 다른 방법은 보간입니다. bonus_points DataFrame을 다시 보죠:\n\nbonus_points\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      NaN\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\ninterpolate 메서드를 사용해 보죠. 기본적으로 수직 방향(axis=0)으로 보간합니다. 따라서 수평으로(axis=1)으로 보간하도록 지정합니다.\n\nbonus_points.interpolate(axis=1)\n\n\n\n\n\n  \n    \n      \n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      NaN\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      3.0\n      3.0\n      0.0\n    \n  \n\n\n\n\nbob의 보너스 포인트는 10월에 0이고 12월에 2입니다. 11월을 보간하면 평균 보너스 포인트 1을 얻습니다. colin의 보너스 포인트는 11월에 1이지만 9월에 포인트는 얼마인지 모릅니다. 따라서 보간할 수 없고 10월의 포인트는 그대로 누락된 값으로 남아 있습니다. 이를 해결하려면 보간하기 전에 9월의 보너스 포인트를 0으로 설정해야 합니다.\n\nbetter_bonus_points = bonus_points.copy()\nbetter_bonus_points.insert(0, \"sep\", 0)\nbetter_bonus_points.loc[\"alice\"] = 0\nbetter_bonus_points = better_bonus_points.interpolate(axis=1)\nbetter_bonus_points\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      bob\n      0.0\n      0.0\n      1.0\n      2.0\n    \n    \n      colin\n      0.0\n      0.5\n      1.0\n      0.0\n    \n    \n      darwin\n      0.0\n      0.0\n      1.0\n      0.0\n    \n    \n      charles\n      0.0\n      3.0\n      3.0\n      0.0\n    \n    \n      alice\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n\n\n좋습니다. 이제 모든 보너스 포인트가 합리적으로 보간되었습니다. 최종 점수를 확인해 보죠:\n\ngrades + better_bonus_points\n\n\n\n\n\n  \n    \n      \n      dec\n      nov\n      oct\n      sep\n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n      8.0\n      8.0\n    \n    \n      bob\n      NaN\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      NaN\n      5.0\n      11.0\n      4.0\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      NaN\n      11.0\n      10.0\n      9.0\n    \n  \n\n\n\n\n9월 열이 오른쪽에 추가되었는데 좀 이상합니다. 이는 더하려는 DataFrame이 정확히 같은 열을 가지고 있지 않기 때문입니다(grade DataFrame에는 \"dec\" 열이 없습니다). 따라서 판다스는 알파벳 순서로 최종 열을 정렬합니다. 이를 해결하려면 덧셈을 하기 전에 누락된 열을 추가하면 됩니다:\n\ngrades[\"dec\"] = np.nan\nfinal_grades = grades + better_bonus_points\nfinal_grades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n12월과 colin에 대해 할 수 있는 것이 많지 않습니다. 보너스 포인트를 만드는 것이 나쁘지만 점수를 합리적으로 올릴 수는 없습니다(어떤 선생님들은 그럴 수 있지만). dropna() 메서드를 사용해 모두 NaN인 행을 삭제합니다:\n\nfinal_grades_clean = final_grades.dropna(how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n    \n  \n\n\n\n\n그다음 axis 매개변수를 1로 지정하여 모두 NaN인 열을 삭제합니다:\n\nfinal_grades_clean = final_grades_clean.dropna(axis=1, how=\"all\")\nfinal_grades_clean\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#groupby로-집계하기",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#groupby로-집계하기",
    "title": "새로 추가한 포스팅3333",
    "section": "groupby로 집계하기",
    "text": "groupby로 집계하기\nSQL과 비슷하게 판다스는 데이터를 그룹핑하고 각 그룹에 대해 연산을 수행할 수 있습니다.\n먼저 그루핑을 위해 각 사람의 데이터를 추가로 만들겠습니다. NaN 값을 어떻게 다루는지 보기 위해 final_grades DataFrame을 다시 사용하겠습니다:\n\nfinal_grades[\"hobby\"] = [\"Biking\", \"Dancing\", np.nan, \"Dancing\", \"Biking\"]\nfinal_grades\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n      hobby\n    \n  \n  \n    \n      alice\n      8.0\n      8.0\n      9.0\n      NaN\n      Biking\n    \n    \n      bob\n      10.0\n      9.0\n      10.0\n      NaN\n      Dancing\n    \n    \n      charles\n      4.0\n      11.0\n      5.0\n      NaN\n      NaN\n    \n    \n      colin\n      NaN\n      NaN\n      NaN\n      NaN\n      Dancing\n    \n    \n      darwin\n      9.0\n      10.0\n      11.0\n      NaN\n      Biking\n    \n  \n\n\n\n\nhobby로 이 DataFrame을 그룹핑해 보죠:\n\ngrouped_grades = final_grades.groupby(\"hobby\")\ngrouped_grades\n\n<pandas.core.groupby.generic.DataFrameGroupBy object at 0x00000273EBA146D0>\n\n\n이제 hobby마다 평균 점수를 계산할 수 있습니다:\n\ngrouped_grades.mean()\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n\nfinal_grades.groupby(\"hobby\").mean()\n\n\n\n\n\n  \n    \n      \n      sep\n      oct\n      nov\n      dec\n    \n    \n      hobby\n      \n      \n      \n      \n    \n  \n  \n    \n      Biking\n      8.5\n      9.0\n      10.0\n      NaN\n    \n    \n      Dancing\n      10.0\n      9.0\n      10.0\n      NaN\n    \n  \n\n\n\n\n아주 쉽네요! 평균을 계산할 때 NaN 값은 그냥 무시됩니다."
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#피봇-테이블",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#피봇-테이블",
    "title": "새로 추가한 포스팅3333",
    "section": "피봇 테이블",
    "text": "피봇 테이블\n판다스는 스프레드시트와 비슷하 피봇 테이블을 지원하여 데이터를 빠르게 요약할 수 있습니다. 어떻게 동작하는 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\nbonus_points.stack().reset_index()\n\n\n\n\n\n  \n    \n      \n      level_0\n      level_1\n      0\n    \n  \n  \n    \n      0\n      bob\n      oct\n      0.0\n    \n    \n      1\n      bob\n      dec\n      2.0\n    \n    \n      2\n      colin\n      nov\n      1.0\n    \n    \n      3\n      colin\n      dec\n      0.0\n    \n    \n      4\n      darwin\n      oct\n      0.0\n    \n    \n      5\n      darwin\n      nov\n      1.0\n    \n    \n      6\n      darwin\n      dec\n      0.0\n    \n    \n      7\n      charles\n      oct\n      3.0\n    \n    \n      8\n      charles\n      nov\n      3.0\n    \n    \n      9\n      charles\n      dec\n      0.0\n    \n  \n\n\n\n\n\nmore_grades = final_grades_clean.stack().reset_index()\nmore_grades.columns = [\"name\", \"month\", \"grade\"]\nmore_grades[\"bonus\"] = [np.nan, np.nan, np.nan, 0, np.nan, 2, 3, 3, 0, 0, 1, 0]\nmore_grades\n\n\n\n\n\n  \n    \n      \n      name\n      month\n      grade\n      bonus\n    \n  \n  \n    \n      0\n      alice\n      sep\n      8.0\n      NaN\n    \n    \n      1\n      alice\n      oct\n      8.0\n      NaN\n    \n    \n      2\n      alice\n      nov\n      9.0\n      NaN\n    \n    \n      3\n      bob\n      sep\n      10.0\n      0.0\n    \n    \n      4\n      bob\n      oct\n      9.0\n      NaN\n    \n    \n      5\n      bob\n      nov\n      10.0\n      2.0\n    \n    \n      6\n      charles\n      sep\n      4.0\n      3.0\n    \n    \n      7\n      charles\n      oct\n      11.0\n      3.0\n    \n    \n      8\n      charles\n      nov\n      5.0\n      0.0\n    \n    \n      9\n      darwin\n      sep\n      9.0\n      0.0\n    \n    \n      10\n      darwin\n      oct\n      10.0\n      1.0\n    \n    \n      11\n      darwin\n      nov\n      11.0\n      0.0\n    \n  \n\n\n\n\n이제 이 DataFrame에 대해 pd.pivot_table() 함수를 호출하고 name 열로 그룹핑합니다. 기본적으로 pivot_table()은 수치 열의 평균을 계산합니다:\n\npd.pivot_table(more_grades, index=\"name\")\n\n\n\n\n\n  \n    \n      \n      bonus\n      grade\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      alice\n      NaN\n      8.333333\n    \n    \n      bob\n      1.000000\n      9.666667\n    \n    \n      charles\n      2.000000\n      6.666667\n    \n    \n      darwin\n      0.333333\n      10.000000\n    \n  \n\n\n\n\n집계 함수를 aggfunc 매개변수로 바꿀 수 있습니다. 또한 집계 대상의 열을 리스트로 지정할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=[\"grade\",\"bonus\"], aggfunc=np.max)\n\n\n\n\n\n  \n    \n      \n      bonus\n      grade\n    \n    \n      name\n      \n      \n    \n  \n  \n    \n      alice\n      NaN\n      9.0\n    \n    \n      bob\n      2.0\n      10.0\n    \n    \n      charles\n      3.0\n      11.0\n    \n    \n      darwin\n      1.0\n      11.0\n    \n  \n\n\n\n\ncolumns 매개변수를 지정하여 수평으로 집계할 수 있고 margins=True로 설정해 각 행과 열에 대해 전체 합을 계산할 수 있습니다:\n\npd.pivot_table(more_grades, index=\"name\", values=\"grade\", columns=\"month\", margins=True)\n\n\n\n\n\n  \n    \n      month\n      nov\n      oct\n      sep\n      All\n    \n    \n      name\n      \n      \n      \n      \n    \n  \n  \n    \n      alice\n      9.00\n      8.0\n      8.00\n      8.333333\n    \n    \n      bob\n      10.00\n      9.0\n      10.00\n      9.666667\n    \n    \n      charles\n      5.00\n      11.0\n      4.00\n      6.666667\n    \n    \n      darwin\n      11.00\n      10.0\n      9.00\n      10.000000\n    \n    \n      All\n      8.75\n      9.5\n      7.75\n      8.666667\n    \n  \n\n\n\n\n마지막으로 여러 개의 인덱스나 열 이름을 지정하면 판다스가 다중 레벨 인덱스를 만듭니다:\n\npd.pivot_table(more_grades, index=(\"name\", \"month\"), margins=True)\n\n\n\n\n\n  \n    \n      \n      \n      bonus\n      grade\n    \n    \n      name\n      month\n      \n      \n    \n  \n  \n    \n      alice\n      nov\n      NaN\n      9.00\n    \n    \n      oct\n      NaN\n      8.00\n    \n    \n      sep\n      NaN\n      8.00\n    \n    \n      bob\n      nov\n      2.000\n      10.00\n    \n    \n      oct\n      NaN\n      9.00\n    \n    \n      sep\n      0.000\n      10.00\n    \n    \n      charles\n      nov\n      0.000\n      5.00\n    \n    \n      oct\n      3.000\n      11.00\n    \n    \n      sep\n      3.000\n      4.00\n    \n    \n      darwin\n      nov\n      0.000\n      11.00\n    \n    \n      oct\n      1.000\n      10.00\n    \n    \n      sep\n      0.000\n      9.00\n    \n    \n      All\n      \n      1.125\n      8.75"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#함수",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#함수",
    "title": "새로 추가한 포스팅3333",
    "section": "함수",
    "text": "함수\n큰 DataFrame을 다룰 때 내용을 간단히 요약하는 것이 도움이 됩니다. 판다스는 이를 위한 몇 가지 함수를 제공합니다. 먼저 수치 값, 누락된 값, 텍스트 값이 섞인 큰 DataFrame을 만들어 보죠. 주피터 노트북은 이 DataFrame의 일부만 보여줍니다:\n\nmuch_data = np.fromfunction(lambda x,y: (x+y*y)%17*11, (10000, 26))\nlarge_df = pd.DataFrame(much_data, columns=list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nlarge_df[large_df % 16 == 0] = np.nan\nlarge_df.insert(3,\"some_text\", \"Blabla\")\nlarge_df\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      9995\n      NaN\n      NaN\n      33.0\n      Blabla\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n      ...\n      NaN\n      NaN\n      NaN\n      33.0\n      88.0\n      165.0\n      77.0\n      11.0\n      154.0\n      132.0\n    \n    \n      9996\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      9997\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n10000 rows × 27 columns\n\n\n\nhead() 메서드는 처음 5개 행을 반환합니다:\n\nlarge_df.head(n=10)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      0\n      NaN\n      11.0\n      44.0\n      Blabla\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n      ...\n      11.0\n      NaN\n      11.0\n      44.0\n      99.0\n      NaN\n      88.0\n      22.0\n      165.0\n      143.0\n    \n    \n      1\n      11.0\n      22.0\n      55.0\n      Blabla\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n      ...\n      22.0\n      11.0\n      22.0\n      55.0\n      110.0\n      NaN\n      99.0\n      33.0\n      NaN\n      154.0\n    \n    \n      2\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      3\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n    \n      4\n      44.0\n      55.0\n      88.0\n      Blabla\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n      ...\n      55.0\n      44.0\n      55.0\n      88.0\n      143.0\n      33.0\n      132.0\n      66.0\n      22.0\n      NaN\n    \n    \n      5\n      55.0\n      66.0\n      99.0\n      Blabla\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n      ...\n      66.0\n      55.0\n      66.0\n      99.0\n      154.0\n      44.0\n      143.0\n      77.0\n      33.0\n      11.0\n    \n    \n      6\n      66.0\n      77.0\n      110.0\n      Blabla\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n      ...\n      77.0\n      66.0\n      77.0\n      110.0\n      165.0\n      55.0\n      154.0\n      88.0\n      44.0\n      22.0\n    \n    \n      7\n      77.0\n      88.0\n      121.0\n      Blabla\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n      ...\n      88.0\n      77.0\n      88.0\n      121.0\n      NaN\n      66.0\n      165.0\n      99.0\n      55.0\n      33.0\n    \n    \n      8\n      88.0\n      99.0\n      132.0\n      Blabla\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n      ...\n      99.0\n      88.0\n      99.0\n      132.0\n      NaN\n      77.0\n      NaN\n      110.0\n      66.0\n      44.0\n    \n    \n      9\n      99.0\n      110.0\n      143.0\n      Blabla\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n      ...\n      110.0\n      99.0\n      110.0\n      143.0\n      11.0\n      88.0\n      NaN\n      121.0\n      77.0\n      55.0\n    \n  \n\n10 rows × 27 columns\n\n\n\n마지막 5개 행을 반환하는 tail() 함수도 있습니다. 원하는 행 개수를 전달할 수도 있습니다:\n\nlarge_df.tail(n=2)\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      some_text\n      D\n      E\n      F\n      G\n      H\n      I\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      9998\n      22.0\n      33.0\n      66.0\n      Blabla\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n      ...\n      33.0\n      22.0\n      33.0\n      66.0\n      121.0\n      11.0\n      110.0\n      44.0\n      NaN\n      165.0\n    \n    \n      9999\n      33.0\n      44.0\n      77.0\n      Blabla\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n      ...\n      44.0\n      33.0\n      44.0\n      77.0\n      132.0\n      22.0\n      121.0\n      55.0\n      11.0\n      NaN\n    \n  \n\n2 rows × 27 columns\n\n\n\ninfo() 메서드는 각 열의 내용을 요약하여 출력합니다:\n\nlarge_df.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 10000 entries, 0 to 9999\nData columns (total 27 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   A          8823 non-null   float64\n 1   B          8824 non-null   float64\n 2   C          8824 non-null   float64\n 3   some_text  10000 non-null  object \n 4   D          8824 non-null   float64\n 5   E          8822 non-null   float64\n 6   F          8824 non-null   float64\n 7   G          8824 non-null   float64\n 8   H          8822 non-null   float64\n 9   I          8823 non-null   float64\n 10  J          8823 non-null   float64\n 11  K          8822 non-null   float64\n 12  L          8824 non-null   float64\n 13  M          8824 non-null   float64\n 14  N          8822 non-null   float64\n 15  O          8824 non-null   float64\n 16  P          8824 non-null   float64\n 17  Q          8824 non-null   float64\n 18  R          8823 non-null   float64\n 19  S          8824 non-null   float64\n 20  T          8824 non-null   float64\n 21  U          8824 non-null   float64\n 22  V          8822 non-null   float64\n 23  W          8824 non-null   float64\n 24  X          8824 non-null   float64\n 25  Y          8822 non-null   float64\n 26  Z          8823 non-null   float64\ndtypes: float64(26), object(1)\nmemory usage: 2.1+ MB\n\n\n마지막으로 describe() 메서드는 각 열에 대한 주요 집계 연산을 수행한 결과를 보여줍니다:\nFinally, the describe() method gives a nice overview of the main aggregated values over each column: * count: null(NaN)이 아닌 값의 개수 * mean: null이 아닌 값의 평균 * std: null이 아닌 값의 표준 편차 * min: null이 아닌 값의 최솟값 * 25%, 50%, 75%: null이 아닌 값의 25번째, 50번째, 75번째 백분위수 * max: null이 아닌 값의 최댓값\n\nlarge_df.describe()\n\n\n\n\n\n  \n    \n      \n      A\n      B\n      C\n      D\n      E\n      F\n      G\n      H\n      I\n      J\n      ...\n      Q\n      R\n      S\n      T\n      U\n      V\n      W\n      X\n      Y\n      Z\n    \n  \n  \n    \n      count\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n      8823.000000\n      ...\n      8824.000000\n      8823.000000\n      8824.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8824.000000\n      8824.000000\n      8822.000000\n      8823.000000\n    \n    \n      mean\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n      88.022441\n      ...\n      87.972575\n      87.977559\n      87.972575\n      87.987534\n      88.012466\n      87.983791\n      88.007480\n      87.977561\n      88.000000\n      88.022441\n    \n    \n      std\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n      47.535911\n      ...\n      47.535523\n      47.535911\n      47.535523\n      47.521679\n      47.521679\n      47.535001\n      47.519371\n      47.529755\n      47.536879\n      47.535911\n    \n    \n      min\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      ...\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n      11.000000\n    \n    \n      25%\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      ...\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n      44.000000\n    \n    \n      50%\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      ...\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n      88.000000\n    \n    \n      75%\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      ...\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n      132.000000\n    \n    \n      max\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      ...\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n      165.000000\n    \n  \n\n8 rows × 26 columns"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#저장",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#저장",
    "title": "새로 추가한 포스팅3333",
    "section": "저장",
    "text": "저장\nCSV, HTML, JSON로 저장해 보죠:\n\nmy_df.to_csv(\"my_df.csv\")\nmy_df.to_html(\"my_df.html\")\nmy_df.to_json(\"my_df.json\")\n\n저장된 내용을 확인해 보죠:\n\nfor filename in (\"my_df.csv\", \"my_df.html\", \"my_df.json\"):\n    print(\"#\", filename)\n    with open(filename, \"rt\") as f:\n        print(f.read())\n        print()\n\n# my_df.csv\n,hobby,weight,birthyear,children\nalice,Biking,68.5,1985,\nbob,Dancing,83.1,1984,3.0\n\n\n# my_df.html\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>hobby</th>\n      <th>weight</th>\n      <th>birthyear</th>\n      <th>children</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>alice</th>\n      <td>Biking</td>\n      <td>68.5</td>\n      <td>1985</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>bob</th>\n      <td>Dancing</td>\n      <td>83.1</td>\n      <td>1984</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n\n# my_df.json\n{\"hobby\":{\"alice\":\"Biking\",\"bob\":\"Dancing\"},\"weight\":{\"alice\":68.5,\"bob\":83.1},\"birthyear\":{\"alice\":1985,\"bob\":1984},\"children\":{\"alice\":null,\"bob\":3.0}}\n\n\n\n인덱스는 (이름 없이) CSV 파일의 첫 번째 열에 저장되었습니다. HTML에서는 <th> 태그와 JSON에서는 키로 저장되었습니다.\n다른 포맷으로 저장하는 것도 비슷합니다. 하지만 일부 포맷은 추가적인 라이브러리 설치가 필요합니다. 예를 들어, 엑셀로 저장하려면 openpyxl 라이브러리가 필요합니다:\n\ntry:\n    my_df.to_excel(\"my_df.xlsx\", sheet_name='People')\nexcept ImportError as e:\n    print(e)\n\nNo module named 'openpyxl'"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#로딩",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#로딩",
    "title": "새로 추가한 포스팅3333",
    "section": "로딩",
    "text": "로딩\nCSV 파일을 DataFrame으로 로드해 보죠:\n\nmy_df_loaded = pd.read_csv(\"my_df.csv\", index_col=0)\nmy_df_loaded\n\n\n\n\n\n  \n    \n      \n      hobby\n      weight\n      birthyear\n      children\n    \n  \n  \n    \n      alice\n      Biking\n      68.5\n      1985\n      NaN\n    \n    \n      bob\n      Dancing\n      83.1\n      1984\n      3.0\n    \n  \n\n\n\n\n예상할 수 있듯이 read_json, read_html, read_excel 함수도 있습니다. 인터넷에서 데이터를 바로 읽을 수도 있습니다. 예를 들어 깃허브에서 1,000개의 U.S. 도시를 로드해 보죠:\n\nus_cities = None\ntry:\n    csv_url = \"https://raw.githubusercontent.com/plotly/datasets/master/us-cities-top-1k.csv\"\n    us_cities = pd.read_csv(csv_url, index_col=0)\n    us_cities = us_cities.head()\nexcept IOError as e:\n    print(e)\nus_cities\n\n\n\n\n\n  \n    \n      \n      State\n      Population\n      lat\n      lon\n    \n    \n      City\n      \n      \n      \n      \n    \n  \n  \n    \n      Marysville\n      Washington\n      63269\n      48.051764\n      -122.177082\n    \n    \n      Perris\n      California\n      72326\n      33.782519\n      -117.228648\n    \n    \n      Cleveland\n      Ohio\n      390113\n      41.499320\n      -81.694361\n    \n    \n      Worcester\n      Massachusetts\n      182544\n      42.262593\n      -71.802293\n    \n    \n      Columbia\n      South Carolina\n      133358\n      34.000710\n      -81.034814\n    \n  \n\n\n\n\n이외에도 많은 옵션이 있습니다. 특히 datetime 포맷에 관련된 옵션이 많습니다. 더 자세한 내용은 온라인 문서를 참고하세요."
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#sql-조인",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#sql-조인",
    "title": "새로 추가한 포스팅3333",
    "section": "SQL 조인",
    "text": "SQL 조인\n판다스의 강력한 기능 중 하나는 DataFrame에 대해 SQL 같은 조인(join)을 수행할 수 있는 것입니다. 여러 종류의 조인이 지원됩니다. 이너 조인(inner join), 레프트/라이트 아우터 조인(left/right outer join), 풀 조인(full join)입니다. 이에 대해 알아 보기 위해 간단한 DataFrame을 만들어 보죠:\n\ncity_loc = pd.DataFrame(\n    [\n        [\"CA\", \"San Francisco\", 37.781334, -122.416728],\n        [\"NY\", \"New York\", 40.705649, -74.008344],\n        [\"FL\", \"Miami\", 25.791100, -80.320733],\n        [\"OH\", \"Cleveland\", 41.473508, -81.739791],\n        [\"UT\", \"Salt Lake City\", 40.755851, -111.896657]\n    ], columns=[\"state\", \"city\", \"lat\", \"lng\"])\ncity_loc\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\ncity_pop = pd.DataFrame(\n    [\n        [808976, \"San Francisco\", \"California\"],\n        [8363710, \"New York\", \"New-York\"],\n        [413201, \"Miami\", \"Florida\"],\n        [2242193, \"Houston\", \"Texas\"]\n    ], index=[3,4,5,6], columns=[\"population\", \"city\", \"state\"])\ncity_pop\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n이제 merge() 함수를 사용해 이 DataFrame을 조인해 보죠:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n  \n\n\n\n\n두 DataFrame은 state란 이름의 열을 가지고 있으므로 state_x와 state_y로 이름이 바뀌었습니다.\n또한 Cleveland, Salt Lake City, Houston은 두 DataFrame에 모두 존재하지 않기 때문에 삭제되었습니다. SQL의 INNER JOIN과 동일합니다. 도시를 삭제하지 않고 NaN으로 채우는 FULL OUTER JOIN을 원하면 how=\"outer\"로 지정합니다:\n\nall_cities = pd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"outer\")\nall_cities\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      5\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\n물론 LEFT OUTER JOIN은 how=\"left\"로 지정할 수 있습니다. 왼쪽의 DataFrame에 있는 도시만 남습니다. 비슷하게 how=\"right\"는 오른쪽 DataFrame에 있는 도시만 결과에 남습니다. 예를 들면:\n\npd.merge(left=city_loc, right=city_pop, on=\"city\", how=\"right\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Florida\n    \n    \n      3\n      NaN\n      Houston\n      NaN\n      NaN\n      2242193\n      Texas\n    \n  \n\n\n\n\n조인할 키가 DataFrame 인덱스라면 left_index=True나 right_index=True로 지정해야 합니다. 키 열의 이름이 다르면 left_on과 right_on을 사용합니다. 예를 들어:\n\ncity_pop2 = city_pop.copy()\ncity_pop2.columns = [\"population\", \"name\", \"state\"]\ncity_pop2\n\n\n\n\n\n  \n    \n      \n      population\n      name\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\npd.merge(left=city_loc, right=city_pop2, left_on=\"city\", right_on=\"name\")\n\n\n\n\n\n  \n    \n      \n      state_x\n      city\n      lat\n      lng\n      population\n      name\n      state_y\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      808976\n      San Francisco\n      California\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      8363710\n      New York\n      New-York\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      413201\n      Miami\n      Florida"
  },
  {
    "objectID": "posts/new-post-03/week_1b_pandas_exam.html#연결",
    "href": "posts/new-post-03/week_1b_pandas_exam.html#연결",
    "title": "새로 추가한 포스팅3333",
    "section": "연결",
    "text": "연결\nDataFrame을 조인하는 대신 그냥 연결할 수도 있습니다. concat() 함수가 하는 일입니다:\n\ncity_loc\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n    \n  \n\n\n\n\n\ncity_pop\n\n\n\n\n\n  \n    \n      \n      population\n      city\n      state\n    \n  \n  \n    \n      3\n      808976\n      San Francisco\n      California\n    \n    \n      4\n      8363710\n      New York\n      New-York\n    \n    \n      5\n      413201\n      Miami\n      Florida\n    \n    \n      6\n      2242193\n      Houston\n      Texas\n    \n  \n\n\n\n\n\nresult_concat = pd.concat([city_loc, city_pop])\nresult_concat\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n이 연산은 (행을 따라) 수직적으로 데이터를 연결하고 (열을 따라) 수평으로 연결하지 않습니다. 이 예에서 동일한 인덱스를 가진 행이 있습니다(예를 들면 3). 판다스는 이를 우아하게 처리합니다:\n\nresult_concat.loc[3]\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n  \n\n\n\n\n또는 인덱스를 무시하도록 설정할 수 있습니다:\n\npd.concat([city_loc, city_pop], ignore_index=True)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      5\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      6\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      7\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      8\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n한 DataFrame에 열이 없을 때 NaN이 채워져 있는 것처럼 동작합니다. join=\"inner\"로 설정하면 양쪽의 DataFrame에 존재하는 열만 반환됩니다:\n\npd.concat([city_loc, city_pop], join=\"inner\")\n\n\n\n\n\n  \n    \n      \n      state\n      city\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n    \n    \n      1\n      NY\n      New York\n    \n    \n      2\n      FL\n      Miami\n    \n    \n      3\n      OH\n      Cleveland\n    \n    \n      4\n      UT\n      Salt Lake City\n    \n    \n      3\n      California\n      San Francisco\n    \n    \n      4\n      New-York\n      New York\n    \n    \n      5\n      Florida\n      Miami\n    \n    \n      6\n      Texas\n      Houston\n    \n  \n\n\n\n\naxis=1로 설정하면 DataFrame을 수직이 아니라 수평으로 연결할 수 있습니다:\n\npd.concat([city_loc, city_pop], axis=1)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n      city\n      state\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n      NaN\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n      NaN\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n      NaN\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      808976.0\n      San Francisco\n      California\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      8363710.0\n      New York\n      New-York\n    \n    \n      5\n      NaN\n      NaN\n      NaN\n      NaN\n      413201.0\n      Miami\n      Florida\n    \n    \n      6\n      NaN\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Houston\n      Texas\n    \n  \n\n\n\n\n이 경우 인덱스가 잘 정렬되지 않기 때문에 의미가 없습니다(예를 들어 Cleveland와 San Francisco의 인덱스 레이블이 3이기 때문에 동일한 행에 놓여 있습니다). 이 DataFrame을 연결하기 전에 도시로 인덱스를 재설정해 보죠:\n\npd.concat([city_loc.set_index(\"city\"), city_pop.set_index(\"city\")], axis=1)\n\n\n\n\n\n  \n    \n      \n      state\n      lat\n      lng\n      population\n      state\n    \n    \n      city\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      San Francisco\n      CA\n      37.781334\n      -122.416728\n      808976.0\n      California\n    \n    \n      New York\n      NY\n      40.705649\n      -74.008344\n      8363710.0\n      New-York\n    \n    \n      Miami\n      FL\n      25.791100\n      -80.320733\n      413201.0\n      Florida\n    \n    \n      Cleveland\n      OH\n      41.473508\n      -81.739791\n      NaN\n      NaN\n    \n    \n      Salt Lake City\n      UT\n      40.755851\n      -111.896657\n      NaN\n      NaN\n    \n    \n      Houston\n      NaN\n      NaN\n      NaN\n      2242193.0\n      Texas\n    \n  \n\n\n\n\nFULL OUTER JOIN을 수행한 것과 비슷합니다. 하지만 state 열이 state_x와 state_y로 바뀌지 않았고 city 열이 인덱스가 되었습니다.\nappend() 메서드는 DataFrame을 수직으로 연결하는 단축 메서드입니다:\n\ncity_loc.append(city_pop)\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n\npd.concat([city_loc,city_pop])\n\n\n\n\n\n  \n    \n      \n      state\n      city\n      lat\n      lng\n      population\n    \n  \n  \n    \n      0\n      CA\n      San Francisco\n      37.781334\n      -122.416728\n      NaN\n    \n    \n      1\n      NY\n      New York\n      40.705649\n      -74.008344\n      NaN\n    \n    \n      2\n      FL\n      Miami\n      25.791100\n      -80.320733\n      NaN\n    \n    \n      3\n      OH\n      Cleveland\n      41.473508\n      -81.739791\n      NaN\n    \n    \n      4\n      UT\n      Salt Lake City\n      40.755851\n      -111.896657\n      NaN\n    \n    \n      3\n      California\n      San Francisco\n      NaN\n      NaN\n      808976.0\n    \n    \n      4\n      New-York\n      New York\n      NaN\n      NaN\n      8363710.0\n    \n    \n      5\n      Florida\n      Miami\n      NaN\n      NaN\n      413201.0\n    \n    \n      6\n      Texas\n      Houston\n      NaN\n      NaN\n      2242193.0\n    \n  \n\n\n\n\n판다스의 다른 메서드와 마찬가지로 append() 메서드는 실제 city_loc을 수정하지 않습니다. 복사본을 만들어 수정한 다음 반환합니다."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "마크다운 문법 연습",
    "section": "",
    "text": "여기엔 먼가 귀여운 움짤 하나. 사이즈는 포스팅 공간 여백에 딱 맞춰서! 아래의 코드를 적당히 바꿔주면 될 것 같은데…\n여기는 본문을 적는 공간\n마크다운 문법을 따로 익혀야하는게 귀찮아서 미루고 있었는데 이번 블로그 만들기 & 포스팅이 좋은 기회가 될 듯 하다.\n여기서 부터는 마크다운 문법 연습 문법 링크 : https://quarto.org/docs/authoring/markdown-basics.html"
  },
  {
    "objectID": "posts/welcome/index.html#h1---h6",
    "href": "posts/welcome/index.html#h1---h6",
    "title": "마크다운 문법 연습",
    "section": "H1 - H6",
    "text": "H1 - H6\n제목이나 머릿글 작성할 때 사용. H1-H6까지 있음. # 이후 H1 과 같이 사용할 수 있음. 각 타입마다 사용하는 영역이 있었는데… 나중에 정리할 것."
  },
  {
    "objectID": "posts/welcome/index.html#블럭-인용-문자",
    "href": "posts/welcome/index.html#블럭-인용-문자",
    "title": "마크다운 문법 연습",
    "section": "블럭 인용 문자",
    "text": "블럭 인용 문자\n대화, 인용, 특별 어구 따위를 나타낼 때 사용. > 에러 발생 : 이상하다, 왜 안되지? > 에러 없음 : 이상하다, 왜 되지? 왜 진짜 안댐?"
  },
  {
    "objectID": "posts/welcome/index.html#코드",
    "href": "posts/welcome/index.html#코드",
    "title": "마크다운 문법 연습",
    "section": "코드",
    "text": "코드\n코드를 작성할 때 사용. '’’ 언어이름 과 같은 형식으로 사용 가능. 출력 결과는 따로 안보여주는 듯.\nprint('1 + 1 = ', 1 + 1)"
  },
  {
    "objectID": "posts/welcome/index.html#비디오",
    "href": "posts/welcome/index.html#비디오",
    "title": "마크다운 문법 연습",
    "section": "비디오",
    "text": "비디오\n유튜브 링크를 가져올 수 있음. 다른 링크도 가능한가? -> 링크 가져올때 공유하기 이후 짧은 링크로 가져와야 동작함."
  },
  {
    "objectID": "posts/welcome/index.html#경고-주의문",
    "href": "posts/welcome/index.html#경고-주의문",
    "title": "마크다운 문법 연습",
    "section": "경고? 주의문",
    "text": "경고? 주의문\n\n\n\n\n\n\nʲ�고\n\n\n\n이 강을 건너지 마시오.: 강에, 악어, 살고있음, and 배고픔.\n\n\n\np태그나 span태그 등 html 태그도 사용 가능\n\ncss나 자바스크립트도 적용 가능 할 듯 한데…"
  },
  {
    "objectID": "posts/basic-statistics-01/index.html",
    "href": "posts/basic-statistics-01/index.html",
    "title": "기초 통계 정리",
    "section": "",
    "text": "장철원 교수님의 특강을 들으며 통계 정리에 대한 필요성을 느껴서 페이지를 만들었다.\n머신러닝을 할때 통계적인 지식이 없어도 가능은 하지만 통계지식이 있다면 보다 명확한 근거를 이야기할 수 있어서 보다 설득력이 증가하기 때문에 가져다 붙이고 싶은데 그때마다 자료를 찾으러 다니기 귀찮아서 내 방식대로 정리했다.\n정리하다보니 느낀거지만 정말 통계공부를 안했다..\n우선 교수님의 ‘선형대수와 통계학으로 배우는 머신러닝 with 파이썬’ 책을 통해 내용을 간단하게 정리하고 추가적으로 자료등을 정리해서 이후 프로젝트 진행할때 사용할 비법노트를 만들고자한다.\n편의를 위해 목차를 통해 해당 페이지로 이동 할 수 있게끔 구성하였다.\n\n목차?"
  },
  {
    "objectID": "posts/bigleader-01-09/index.html",
    "href": "posts/bigleader-01-09/index.html",
    "title": "류성한 프로님 특강",
    "section": "",
    "text": "메모보고 작성중…"
  },
  {
    "objectID": "posts/bigleader-01-07/index.html",
    "href": "posts/bigleader-01-07/index.html",
    "title": "빅리더 시각화 PowerBI & Tableau",
    "section": "",
    "text": "사실 시각화 수업은 어제 끝났는데 글은 오늘 작성함…\n\n시각화 수업 내용 정리\n대략 4일간에 걸친 시각화 수업이 모두 끝났다. 수업 내용에서 툴 사용법이나 자료등을 정리하는 건 너무 비효율적인 것 같아서 좋았다고 생각했던 내용들과 기억에 남는 내용을 위주로 작성하고자한다. 요컨데 시각화 메모장이다.\n우선 각 수업에 대해 정리해보았다…\n\n\nPowerBI\n\nData Literacy (데이터를 읽고 해석해서 활용 할 수 있는 능력) 배양\n코딩 과 통계적/수학적 지식이 없어도 할 수 있는 데이터 분석!\n\n이라는 목표로 수업이 진행되었다. 어색한 분위기를 깰겸 말씀해주신 내용들이 재미있었는데 그 중에 기억에 남는 내용은 넷플릭스에서 스트리밍된 ‘돈룩업(2021)’ 이야기이다.\n해당 시리즈는 지구로 날아오는 혜성, 이를 경고하는 과학자들, 하늘을 올려다보지 말라는 정치인들, 복잡하다며 무시하는 사람들? 에 대해 이야기 하는 내용이라고 한다. 과학자들이 자신들만의 언어로 사람들에게 위험성을 설명하지만 제대로 전달되지 못하고 오히려 무시당하는 내용이라고 들었다.\n이런 이야기를 해주시면서 ‘설명력/설득력에 따라 사람들은 과학의 결과를 받아들이던지 어렵다고 배척하던지… 하는 경향이 있다’고 한다. 솔직히 나 역시도 아무리 관심있는 분야라 할 지라도 알아듣기 힘들다면 커뮤니케이션이 제대로 이루어 지지 않는다면 금방 흥미를 잃곤한다. 당장 내가 다니던 학교에도 내가 정말 좋아하는 교수님이지만 수업만큼은 정말 듣기 힘들었던… 교수님도 계시고…\n반면 관심이 없던 분야라도 설명을 너무 잘하고 재미있게 이야기 하셔서 집중해서 듣게되는 강사님도 계셨는데 지금은 잘 계시려나..? 아무튼 본론으로 돌아와서 내가 아무리 많은 정보를 가지고 있다고 하더라도 상대에게 이걸 전달하려면 상대에 맞춰서 이야기를 자료를 정리하는 과정을 꼭 거쳐야한다는 것이다.\n비유가 적당할진 모르겠지만 내가 병원에 가서 검진을 받았는데 의사가 전문용어만 사용해서 내 몸상태를 설명해준다면 내가 이해할까?\n같은 내용 예를 들어 ’우리는 왜 법을 지켜야 하는가?’ 라는 내용에 대해 설명할 때도 상대가 유치원생 이라면 동화같은 이야기를 해야할것이고 중~고등학생 이라면 도덕적인 이유와 현실적인 이유 등을 이야기할지도 모르겠고 대상이 성인이라면 철학적인 이야기를 해볼지도 모르겠다(난 철학에 대해선 하나도 모르지만).\n시각화 이야기를 하다가 왜 이런 이야기가 나왔는지 잘 모르겠지만 다시 시각화 이야기로 돌아가서 시각화에서 가장 중요한건 ‘왜 이걸 보여주는지 설득력을 가져야한다’ 라는 것이다. 데이터 과학자에도 직군이 많이 있지만(DE, DS, DA …) 상대적으로 커뮤니케이션의 중요도가 다를 순 있겠지만 데이터 과학자의 일터는 비즈니스 필드 라는 것이다.\n특히 데이터 애널리스트의 경우 데이터 분석 결과를 비즈니스 맨 대상으로 설명/설득 해야 내 성과가 나온다고 강조하셨다. 또한 데이터 과학자가 커뮤니케이션 하기 가장 좋은 방법은 시각화 라고 말씀해 주시면서 데이터를 통한 커뮤니케이션과 인사이트 도출에 대해 중요성을 엄청나게 강조하셨다.\n이후엔 툴 사용법에 대해 설명해주시면서 몇가지 시각화를 따라서 만들어 보는 식으로 진행되었는데 솔직히 수업 내용중 툴 사용법에 대해서 기억나는건 없었고 PowerBI라는 툴이 정말 편한 툴이구나 라는 생각이 들었다. 그동안 파이썬 코드를 작성해서 전처리 / 시각화 작업을 진행했었는데 해당 툴을 이용하면 드래그 드롭과 클릭 몇번만으로 대쉬보드를 쉽게(그것도 필터링 기능까지 넣어서) 작성 할 수 있었다. 시각화에 대해 이론적인 내용등은 책을 추천해주셔서 욕심이 생긴다면 책을 구매해볼까 싶기도 하다. 그외엔 팁등을 많이 주셨는데 간단히 정리하자면 아래와 같다.\n그 외 팁들\nChatGPT 이용법 ( 이건 기존에도 하고 있던거긴 한데 혹시나 싶어서 적어둔다. ) - 깃헙 코드 하나 복붙해서 설명해달라하기 - 설명을 요청할 때 한줄 단위로 알려달라하기 - 에러 발생시 코드랑 같이 물어보기\nChatGPT를 활용한 면접 준비? - TALK-TO-ChatGPT (크롬확장자) - 프롬프트 지니 (자동번역) - 모의 면접 : 내 목표 회사는 OO이야. 예상 면접 질문을 만들어줘. 답도 알려줘. - 외국어(한국어 포함) 나의 발음 정확도 체크 가능\n교재 추천 - 파이썬 시각화 Scientific Visualization - → ebook 도 있음\n메모엔 RNN 이 특히 중요함 이라고 적어놨는데 무슨 내용이었더라…\n유용한 사이트 - 데이터 수집 문화 빅데이터 플랫폼 : https://www.bigdata-culture.kr/bigdata/user/main.do\n온라인 공공데이터포털 서울열린데이터광장 행정안전부 통계청 통계마이크로데이터 한국관광공사 sk빅데이터허브\n오프라인 빅데이터캠퍼스 건강보험심사평가원\n구글 : fake data 실제 데이터는 아니지만 연습용으로 사용가능한 데이터셋\n정리하다보니 생각보다 길어지는데… 솔직히 수업 내용은 Tableau 강사님께 조금 더 마음에 들었다. 그러니 적어야겠지.\nTableau 역시 강력한 툴이고 현업에서 많이 요구하기도 하고 정말 많은 자료와 강의를 온라인을 통해서 제공하고계셨다. 강사님 YouTube : https://www.youtube.com/@data-viz\n\n\nTableau\n글을 작성하려고 메모해둔 내용을 참고하면서 엄청난 사실을 깨달았다.\n분명 Tableau 강의가 더 마음에 들었고 열심히 참여했고 배운 내용도 많은데!!!\n놀랍게도 메모해둔 내용이 없다!\n시각화 하는 방법을 배우고 세네개의 시각화 프로젝트를 진행하고 대쉬보드도 여러개 만들었는데 메모해둔 내용은 하나도 없었다. 그나마 여기에 적을만한 내용을 찾는다면 기업에 들어가서 발표 자료를 시각화 할때는 해당 기업의 메인 컬러를 지켜야하고 비즈니스적인 시각화가 아니더라도 해당 내용을 잘 설명할 수 있는 주제에 대한 컬러도 고민해봐야하고 카테고리에 색상을 입힐때나 어떤 시각화가 좋은것이라던가 … 이런 내용들은 시각화 참고노트 라는 글을 하나 포스팅해서 작성하려 했기에 해당 링크 하나를 남기고 이번 포스팅은 마치려고 한다.\n시각화 체크리스트 https://noveled.github.io/minstar_blog/posts/bigleader-01-08/"
  },
  {
    "objectID": "posts/bigleader-01-01/bigleader_01_week01_01.html",
    "href": "posts/bigleader-01-01/bigleader_01_week01_01.html",
    "title": "빅리더 1주차 02일 Pandas",
    "section": "",
    "text": "Pandas 기본 조작방법 익히기 코드 보기\n\n데이터프레임 만들고 조작하기\n만들기, 조회하기, 값 추가하기\n\nimport pandas as pd # pd 라는 이름으로 pandas 패키지 불러오기\n\n\nlst = ['a', 'b', 'c']\ns1 = pd.Series(lst)\ns1\n\n0    a\n1    b\n2    c\ndtype: object\n\n\n\ntype(s1)\n\npandas.core.series.Series\n\n\n\ndic = {1:'a', 2:'b', 3:'c'}\ns2 = pd.Series(dic)\ns2\n\n1    a\n2    b\n3    c\ndtype: object\n\n\n\ndict = {'a':[1,2,3], 'b':[2,3,4], 'c':[3,4,5]}\ndf = pd.DataFrame(dict) # 데이터 프래임 만들기\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      a\n      b\n      c\n    \n  \n  \n    \n      0\n      1\n      2\n      3\n    \n    \n      1\n      2\n      3\n      4\n    \n    \n      2\n      3\n      4\n      5\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nlst1 = ['홍길동', 20, '남']\nlst2 = ['이순신', 21, '남']\nlst3 = ['김개동', 22, '남']\n\ndf2 = pd.DataFrame([lst1, lst2, lst3], columns=['name', 'age', 'gender'])\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      age\n      gender\n    \n  \n  \n    \n      0\n      홍길동\n      20\n      남\n    \n    \n      1\n      이순신\n      21\n      남\n    \n    \n      2\n      김개동\n      22\n      남\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.rename(columns={'name':'이름','age':'나이','gender':'성별'}, inplace=True)\n\n\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      나이\n      성별\n    \n  \n  \n    \n      0\n      홍길동\n      20\n      남\n    \n    \n      1\n      이순신\n      21\n      남\n    \n    \n      2\n      김개동\n      22\n      남\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.drop(0)\n# df2.drop(0, inplace=True)\n# df2 = df2.drop(0)\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      나이\n      성별\n    \n  \n  \n    \n      1\n      이순신\n      21\n      남\n    \n    \n      2\n      김개동\n      22\n      남\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      나이\n      성별\n    \n  \n  \n    \n      0\n      홍길동\n      20\n      남\n    \n    \n      1\n      이순신\n      21\n      남\n    \n    \n      2\n      김개동\n      22\n      남\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf3 = df2.drop(0)\ndf3.drop('성별', axis=1, inplace=True)\ndf3\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      나이\n    \n  \n  \n    \n      1\n      이순신\n      21\n    \n    \n      2\n      김개동\n      22\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc[[0, 2]]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      나이\n      성별\n    \n  \n  \n    \n      0\n      홍길동\n      20\n      남\n    \n    \n      2\n      김개동\n      22\n      남\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc[0:3]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      나이\n      성별\n    \n  \n  \n    \n      0\n      홍길동\n      20\n      남\n    \n    \n      1\n      이순신\n      21\n      남\n    \n    \n      2\n      김개동\n      22\n      남\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2['이름']\n\n0    홍길동\n1    이순신\n2    김개동\nName: 이름, dtype: object\n\n\n\ndf2.이름\n\n0    홍길동\n1    이순신\n2    김개동\nName: 이름, dtype: object\n\n\n\nexam_data = {'이름' : ['서준', '우현', '인아'],\n             '수학' : [90, 80, 70],\n             '영어' : [98, 89, 95],\n             '음악' : [85, 95, 100],\n             '체육' : [100, 90, 90]}\ndf = pd.DataFrame(exam_data)\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      수학\n      영어\n      음악\n      체육\n    \n  \n  \n    \n      0\n      서준\n      90\n      98\n      85\n      100\n    \n    \n      1\n      우현\n      80\n      89\n      95\n      90\n    \n    \n      2\n      인아\n      70\n      95\n      100\n      90\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.set_index('이름', inplace=True)\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n    \n    \n      이름\n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n    \n    \n      우현\n      80\n      89\n      95\n      90\n    \n    \n      인아\n      70\n      95\n      100\n      90\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf['총점'] = df['수학'] + df['영어'] + df['음악'] + df['체육']\n\n\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n      총점\n    \n    \n      이름\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n      373\n    \n    \n      우현\n      80\n      89\n      95\n      90\n      354\n    \n    \n      인아\n      70\n      95\n      100\n      90\n      355\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf['평균'] = df['총점'] / 4\n\n\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n      총점\n      평균\n    \n    \n      이름\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n      373\n      93.25\n    \n    \n      우현\n      80\n      89\n      95\n      90\n      354\n      88.50\n    \n    \n      인아\n      70\n      95\n      100\n      90\n      355\n      88.75\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2 = pd.DataFrame(exam_data)\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      이름\n      수학\n      영어\n      음악\n      체육\n    \n  \n  \n    \n      0\n      서준\n      90\n      98\n      85\n      100\n    \n    \n      1\n      우현\n      80\n      89\n      95\n      90\n    \n    \n      2\n      인아\n      70\n      95\n      100\n      90\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.set_index('이름', inplace=True) # 컬럼중 하나를 지정해서 index로 사용\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n    \n    \n      이름\n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n    \n    \n      우현\n      80\n      89\n      95\n      90\n    \n    \n      인아\n      70\n      95\n      100\n      90\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc['길동'] = [100, 100, 100, 100] # 리스트 형식으로 추가\n\n\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n    \n    \n      이름\n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n    \n    \n      우현\n      80\n      89\n      95\n      90\n    \n    \n      인아\n      70\n      95\n      100\n      90\n    \n    \n      길동\n      100\n      100\n      100\n      100\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.loc['황진'] = {'수학': 12, '영어' : 30, '음악' : 20, '체육' : 50} # dict 형식으로 추가\n\n\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n    \n    \n      이름\n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n    \n    \n      우현\n      80\n      89\n      95\n      90\n    \n    \n      인아\n      70\n      95\n      100\n      90\n    \n    \n      길동\n      100\n      100\n      100\n      100\n    \n    \n      황진\n      12\n      30\n      20\n      50\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2.T # 행열 바꾸기 (전치)\n\n\n\n  \n    \n      \n\n\n  \n    \n      이름\n      서준\n      우현\n      인아\n      길동\n      황진\n    \n  \n  \n    \n      수학\n      90\n      80\n      70\n      100\n      12\n    \n    \n      영어\n      98\n      89\n      95\n      100\n      30\n    \n    \n      음악\n      85\n      95\n      100\n      100\n      20\n    \n    \n      체육\n      100\n      90\n      90\n      100\n      50\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 데이터프레임 df를 전치하기 (메소드 활용) df = df.transpose()\n'''\n메소드는 클래스가 가지는 함수\n냉장고 예시에서 문열기(), 음식넣기(), 문닫기()\n'''\n# 데이터프레임 df를 다시 전치하기 (클래스 속성 활용) df = df.T\n'''\n클래스 속성은 클래스에서 있던 변수\n냉장고가 열려있는지 or 닫혀있는지\n'''\n\n\n\n데이터 프레임가지고 계산하기\n\ndf2 # 학점 계산을 위해서 데이터 만들기\ndf2['총점'] = df2['수학'] + df2['영어'] + df2['음악'] + df2['체육']\ndf2['평균'] = df2['총점'] / 4\n\n\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n      총점\n      평균\n    \n    \n      이름\n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n      373\n      93.25\n    \n    \n      우현\n      80\n      89\n      95\n      90\n      354\n      88.50\n    \n    \n      인아\n      70\n      95\n      100\n      90\n      355\n      88.75\n    \n    \n      길동\n      100\n      100\n      100\n      100\n      400\n      100.00\n    \n    \n      황진\n      12\n      30\n      20\n      50\n      112\n      28.00\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ngrade = [] # 학점을 한번에 입력하기 위해 리스트 이용\n\nfor avg in df2['평균']:\n    if avg >= 90:\n        grade.append('A')\n    elif avg >= 80:\n        grade.append('B')\n    else:\n        grade.append('C')\n\nprint(\"위에서 부터 학생들 성적 : \", grade)\n\n위에서 부터 학생들 성적 :  ['A', 'B', 'B', 'A', 'C']\n\n\n\ndf2['성적'] = grade\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n      총점\n      평균\n      성적\n    \n    \n      이름\n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n      373\n      93.25\n      A\n    \n    \n      우현\n      80\n      89\n      95\n      90\n      354\n      88.50\n      B\n    \n    \n      인아\n      70\n      95\n      100\n      90\n      355\n      88.75\n      B\n    \n    \n      길동\n      100\n      100\n      100\n      100\n      400\n      100.00\n      A\n    \n    \n      황진\n      12\n      30\n      20\n      50\n      112\n      28.00\n      C\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 성적이 A 이면 장학생으로 선정해주는 함수\ndef scholarship(row):\n    if row == 'A':\n        return '장학생'\n    else:\n        return '비장학생'\n\n\ndf2['장학여부'] = df2['성적'].apply(scholarship) # apply는 함수를 바로 적용해준다고 생각하면 좋을듯\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      수학\n      영어\n      음악\n      체육\n      총점\n      평균\n      성적\n      장학여부\n    \n    \n      이름\n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      서준\n      90\n      98\n      85\n      100\n      373\n      93.25\n      A\n      장학생\n    \n    \n      우현\n      80\n      89\n      95\n      90\n      354\n      88.50\n      B\n      비장학생\n    \n    \n      인아\n      70\n      95\n      100\n      90\n      355\n      88.75\n      B\n      비장학생\n    \n    \n      길동\n      100\n      100\n      100\n      100\n      400\n      100.00\n      A\n      장학생\n    \n    \n      황진\n      12\n      30\n      20\n      50\n      112\n      28.00\n      C\n      비장학생\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n외부 데이터 파일 불러오기\n\n# 이 부분은 구글 코랩 사용중이라 따로 불러온 코드.\n# 다른 환경에서는 실행 필요 x\n\n#from google.colab import drive\n#drive.mount('/content/drive')\n\n\ndf = pd.read_csv('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/csv_sample.csv') # csv 파일 불러오기\n\n\ndf.to_csv('0627.csv') # csv 파일 저장하기\n\n\n# 분석할 파일 불러오기\ndf = pd.read_excel('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/남북한발전전력량.xlsx')\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      전력량 (억㎾h)\n      발전 전력별\n      1990\n      1991\n      1992\n      1993\n      1994\n      1995\n      1996\n      1997\n      ...\n      2007\n      2008\n      2009\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n    \n  \n  \n    \n      0\n      남한\n      합계\n      1077\n      1186\n      1310\n      1444\n      1650\n      1847\n      2055\n      2244\n      ...\n      4031\n      4224\n      4336\n      4747\n      4969\n      5096\n      5171\n      5220\n      5281\n      5404\n    \n    \n      1\n      NaN\n      수력\n      64\n      51\n      49\n      60\n      41\n      55\n      52\n      54\n      ...\n      50\n      56\n      56\n      65\n      78\n      77\n      84\n      78\n      58\n      66\n    \n    \n      2\n      NaN\n      화력\n      484\n      573\n      696\n      803\n      1022\n      1122\n      1264\n      1420\n      ...\n      2551\n      2658\n      2802\n      3196\n      3343\n      3430\n      3581\n      3427\n      3402\n      3523\n    \n    \n      3\n      NaN\n      원자력\n      529\n      563\n      565\n      581\n      587\n      670\n      739\n      771\n      ...\n      1429\n      1510\n      1478\n      1486\n      1547\n      1503\n      1388\n      1564\n      1648\n      1620\n    \n    \n      4\n      NaN\n      신재생\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      ...\n      -\n      -\n      -\n      -\n      -\n      86\n      118\n      151\n      173\n      195\n    \n    \n      5\n      북한\n      합계\n      277\n      263\n      247\n      221\n      231\n      230\n      213\n      193\n      ...\n      236\n      255\n      235\n      237\n      211\n      215\n      221\n      216\n      190\n      239\n    \n    \n      6\n      NaN\n      수력\n      156\n      150\n      142\n      133\n      138\n      142\n      125\n      107\n      ...\n      133\n      141\n      125\n      134\n      132\n      135\n      139\n      130\n      100\n      128\n    \n    \n      7\n      NaN\n      화력\n      121\n      113\n      105\n      88\n      93\n      88\n      88\n      86\n      ...\n      103\n      114\n      110\n      103\n      79\n      80\n      82\n      86\n      90\n      111\n    \n    \n      8\n      NaN\n      원자력\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      ...\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n      -\n    \n  \n\n9 rows × 29 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf = pd.read_csv('/content/drive/MyDrive/2023 빅리더/파이썬/data/pythondata/auto-mpg.csv', header=None)\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504.0\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165.0\n      3693.0\n      11.5\n      70\n      1\n      buick skylark 320\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436.0\n      11.0\n      70\n      1\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433.0\n      12.0\n      70\n      1\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449.0\n      10.5\n      70\n      1\n      ford torino\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      393\n      27.0\n      4\n      140.0\n      86.00\n      2790.0\n      15.6\n      82\n      1\n      ford mustang gl\n    \n    \n      394\n      44.0\n      4\n      97.0\n      52.00\n      2130.0\n      24.6\n      82\n      2\n      vw pickup\n    \n    \n      395\n      32.0\n      4\n      135.0\n      84.00\n      2295.0\n      11.6\n      82\n      1\n      dodge rampage\n    \n    \n      396\n      28.0\n      4\n      120.0\n      79.00\n      2625.0\n      18.6\n      82\n      1\n      ford ranger\n    \n    \n      397\n      31.0\n      4\n      119.0\n      82.00\n      2720.0\n      19.4\n      82\n      1\n      chevy s-10\n    \n  \n\n398 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.columns = ['mpg','cylinders','displacement',\n              'horsepower','weight', 'acceleration',\n             'model year','origin','name']\n\n\ndf\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      model year\n      origin\n      name\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504.0\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165.0\n      3693.0\n      11.5\n      70\n      1\n      buick skylark 320\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436.0\n      11.0\n      70\n      1\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433.0\n      12.0\n      70\n      1\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449.0\n      10.5\n      70\n      1\n      ford torino\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      393\n      27.0\n      4\n      140.0\n      86.00\n      2790.0\n      15.6\n      82\n      1\n      ford mustang gl\n    \n    \n      394\n      44.0\n      4\n      97.0\n      52.00\n      2130.0\n      24.6\n      82\n      2\n      vw pickup\n    \n    \n      395\n      32.0\n      4\n      135.0\n      84.00\n      2295.0\n      11.6\n      82\n      1\n      dodge rampage\n    \n    \n      396\n      28.0\n      4\n      120.0\n      79.00\n      2625.0\n      18.6\n      82\n      1\n      ford ranger\n    \n    \n      397\n      31.0\n      4\n      119.0\n      82.00\n      2720.0\n      19.4\n      82\n      1\n      chevy s-10\n    \n  \n\n398 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      model year\n      origin\n      name\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504.0\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165.0\n      3693.0\n      11.5\n      70\n      1\n      buick skylark 320\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436.0\n      11.0\n      70\n      1\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433.0\n      12.0\n      70\n      1\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449.0\n      10.5\n      70\n      1\n      ford torino\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.tail()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      model year\n      origin\n      name\n    \n  \n  \n    \n      393\n      27.0\n      4\n      140.0\n      86.00\n      2790.0\n      15.6\n      82\n      1\n      ford mustang gl\n    \n    \n      394\n      44.0\n      4\n      97.0\n      52.00\n      2130.0\n      24.6\n      82\n      2\n      vw pickup\n    \n    \n      395\n      32.0\n      4\n      135.0\n      84.00\n      2295.0\n      11.6\n      82\n      1\n      dodge rampage\n    \n    \n      396\n      28.0\n      4\n      120.0\n      79.00\n      2625.0\n      18.6\n      82\n      1\n      ford ranger\n    \n    \n      397\n      31.0\n      4\n      119.0\n      82.00\n      2720.0\n      19.4\n      82\n      1\n      chevy s-10\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.shape\n\n(398, 9)\n\n\n\ndf.info()\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 398 entries, 0 to 397\nData columns (total 9 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   mpg           398 non-null    float64\n 1   cylinders     398 non-null    int64  \n 2   displacement  398 non-null    float64\n 3   horsepower    398 non-null    object \n 4   weight        398 non-null    float64\n 5   acceleration  398 non-null    float64\n 6   model year    398 non-null    int64  \n 7   origin        398 non-null    int64  \n 8   name          398 non-null    object \ndtypes: float64(4), int64(3), object(2)\nmemory usage: 28.1+ KB\n\n\n\ndf.describe()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      weight\n      acceleration\n      model year\n      origin\n    \n  \n  \n    \n      count\n      398.000000\n      398.000000\n      398.000000\n      398.000000\n      398.000000\n      398.000000\n      398.000000\n    \n    \n      mean\n      23.514573\n      5.454774\n      193.425879\n      2970.424623\n      15.568090\n      76.010050\n      1.572864\n    \n    \n      std\n      7.815984\n      1.701004\n      104.269838\n      846.841774\n      2.757689\n      3.697627\n      0.802055\n    \n    \n      min\n      9.000000\n      3.000000\n      68.000000\n      1613.000000\n      8.000000\n      70.000000\n      1.000000\n    \n    \n      25%\n      17.500000\n      4.000000\n      104.250000\n      2223.750000\n      13.825000\n      73.000000\n      1.000000\n    \n    \n      50%\n      23.000000\n      4.000000\n      148.500000\n      2803.500000\n      15.500000\n      76.000000\n      1.000000\n    \n    \n      75%\n      29.000000\n      8.000000\n      262.000000\n      3608.000000\n      17.175000\n      79.000000\n      2.000000\n    \n    \n      max\n      46.600000\n      8.000000\n      455.000000\n      5140.000000\n      24.800000\n      82.000000\n      3.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf['mpg'].max()\n\n46.6\n\n\n\n# 데이터 프레임 각 요소별로 갯수 확인\nunique_values = df['origin'].value_counts()\nunique_values\n\n1    249\n3     79\n2     70\nName: origin, dtype: int64\n\n\n\ndf.corr() # 상관계수\n\nFutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  df.corr() # 상관계수\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      weight\n      acceleration\n      model year\n      origin\n    \n  \n  \n    \n      mpg\n      1.000000\n      -0.775396\n      -0.804203\n      -0.831741\n      0.420289\n      0.579267\n      0.563450\n    \n    \n      cylinders\n      -0.775396\n      1.000000\n      0.950721\n      0.896017\n      -0.505419\n      -0.348746\n      -0.562543\n    \n    \n      displacement\n      -0.804203\n      0.950721\n      1.000000\n      0.932824\n      -0.543684\n      -0.370164\n      -0.609409\n    \n    \n      weight\n      -0.831741\n      0.896017\n      0.932824\n      1.000000\n      -0.417457\n      -0.306564\n      -0.581024\n    \n    \n      acceleration\n      0.420289\n      -0.505419\n      -0.543684\n      -0.417457\n      1.000000\n      0.288137\n      0.205873\n    \n    \n      model year\n      0.579267\n      -0.348746\n      -0.370164\n      -0.306564\n      0.288137\n      1.000000\n      0.180662\n    \n    \n      origin\n      0.563450\n      -0.562543\n      -0.609409\n      -0.581024\n      0.205873\n      0.180662\n      1.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf[['mpg','weight']].corr()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      weight\n    \n  \n  \n    \n      mpg\n      1.000000\n      -0.831741\n    \n    \n      weight\n      -0.831741\n      1.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      model year\n      origin\n      name\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504.0\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n    \n    \n      1\n      15.0\n      8\n      350.0\n      165.0\n      3693.0\n      11.5\n      70\n      1\n      buick skylark 320\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436.0\n      11.0\n      70\n      1\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433.0\n      12.0\n      70\n      1\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449.0\n      10.5\n      70\n      1\n      ford torino\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf[df['mpg'] > 15]\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      mpg\n      cylinders\n      displacement\n      horsepower\n      weight\n      acceleration\n      model year\n      origin\n      name\n    \n  \n  \n    \n      0\n      18.0\n      8\n      307.0\n      130.0\n      3504.0\n      12.0\n      70\n      1\n      chevrolet chevelle malibu\n    \n    \n      2\n      18.0\n      8\n      318.0\n      150.0\n      3436.0\n      11.0\n      70\n      1\n      plymouth satellite\n    \n    \n      3\n      16.0\n      8\n      304.0\n      150.0\n      3433.0\n      12.0\n      70\n      1\n      amc rebel sst\n    \n    \n      4\n      17.0\n      8\n      302.0\n      140.0\n      3449.0\n      10.5\n      70\n      1\n      ford torino\n    \n    \n      14\n      24.0\n      4\n      113.0\n      95.00\n      2372.0\n      15.0\n      70\n      3\n      toyota corona mark ii\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      393\n      27.0\n      4\n      140.0\n      86.00\n      2790.0\n      15.6\n      82\n      1\n      ford mustang gl\n    \n    \n      394\n      44.0\n      4\n      97.0\n      52.00\n      2130.0\n      24.6\n      82\n      2\n      vw pickup\n    \n    \n      395\n      32.0\n      4\n      135.0\n      84.00\n      2295.0\n      11.6\n      82\n      1\n      dodge rampage\n    \n    \n      396\n      28.0\n      4\n      120.0\n      79.00\n      2625.0\n      18.6\n      82\n      1\n      ford ranger\n    \n    \n      397\n      31.0\n      4\n      119.0\n      82.00\n      2720.0\n      19.4\n      82\n      1\n      chevy s-10\n    \n  \n\n329 rows × 9 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf_groupby = df.groupby(by=['model year'])\ndf_groupby.groups\n\n{70: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28], 71: [29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56], 72: [57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84], 73: [85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124], 74: [125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151], 75: [152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181], 76: [182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215], 77: [216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243], 78: [244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279], 79: [280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308], 80: [309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337], 81: [338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366], 82: [367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397]}\n\n\n\nl1 = [{'name': 'John', 'job': \"teacher\"},\n        {'name': 'Nate', 'job': \"student\"},\n        {'name': 'Fred', 'job': \"developer\"}]\nl2 = [{'name': 'Ed', 'job': \"dentist\"},\n        {'name': 'Jack', 'job': \"farmer\"},\n        {'name': 'Ted', 'job': \"designer\"}]\ndf1 = pd.DataFrame(l1, columns = ['name', 'job'])\ndf2 = pd.DataFrame(l2, columns = ['name', 'job'])\n\n\ndf1\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n    \n  \n  \n    \n      0\n      John\n      teacher\n    \n    \n      1\n      Nate\n      student\n    \n    \n      2\n      Fred\n      developer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf2\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n    \n  \n  \n    \n      0\n      Ed\n      dentist\n    \n    \n      1\n      Jack\n      farmer\n    \n    \n      2\n      Ted\n      designer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf1.append(df2) # df1 기준으로 df2 붙이기\n\nFutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df1.append(df2)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n    \n  \n  \n    \n      0\n      John\n      teacher\n    \n    \n      1\n      Nate\n      student\n    \n    \n      2\n      Fred\n      developer\n    \n    \n      0\n      Ed\n      dentist\n    \n    \n      1\n      Jack\n      farmer\n    \n    \n      2\n      Ted\n      designer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf1.append(df2, ignore_index=True) # df1 기준으로 df2 붙이면서 인덱스도 초기화\n\nFutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n  df1.append(df2, ignore_index=True)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n    \n  \n  \n    \n      0\n      John\n      teacher\n    \n    \n      1\n      Nate\n      student\n    \n    \n      2\n      Fred\n      developer\n    \n    \n      3\n      Ed\n      dentist\n    \n    \n      4\n      Jack\n      farmer\n    \n    \n      5\n      Ted\n      designer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nl1 = [{'name': 'John', 'job': \"teacher\"},\n        {'name': 'Nate', 'job': \"student\"},\n        {'name': 'Fred', 'job': \"developer\"}]\n\nl2 = [{'name': 'Ed', 'work': \"dentist\"},\n        {'name': 'Nate', 'work': \"farmer\"},\n        {'name': 'Ted', 'work': \"designer\"}]\n\ndf3 = pd.DataFrame(l1, columns = ['name', 'job'])\ndf4 = pd.DataFrame(l2, columns = ['name', 'work'])\n\n\ndf3\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n    \n  \n  \n    \n      0\n      John\n      teacher\n    \n    \n      1\n      Nate\n      student\n    \n    \n      2\n      Fred\n      developer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf4\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      work\n    \n  \n  \n    \n      0\n      Ed\n      dentist\n    \n    \n      1\n      Nate\n      farmer\n    \n    \n      2\n      Ted\n      designer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 교집합\nmerge_inner = pd.merge(df3, df4)\nmerge_inner\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n      work\n    \n  \n  \n    \n      0\n      Nate\n      student\n      farmer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n# 합집합\nmerge_outter = pd.merge(df3, df4, on = \"name\", how = 'outer')\nmerge_outter\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n      work\n    \n  \n  \n    \n      0\n      John\n      teacher\n      NaN\n    \n    \n      1\n      Nate\n      student\n      farmer\n    \n    \n      2\n      Fred\n      developer\n      NaN\n    \n    \n      3\n      Ed\n      NaN\n      dentist\n    \n    \n      4\n      Ted\n      NaN\n      designer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf_left = pd.merge(df3, df4, how='left', on='name')\ndf_left\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n      work\n    \n  \n  \n    \n      0\n      John\n      teacher\n      NaN\n    \n    \n      1\n      Nate\n      student\n      farmer\n    \n    \n      2\n      Fred\n      developer\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\ndf_left = pd.merge(df3, df4, how='right', on='name')\ndf_left\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      name\n      job\n      work\n    \n  \n  \n    \n      0\n      Ed\n      NaN\n      dentist\n    \n    \n      1\n      Nate\n      student\n      farmer\n    \n    \n      2\n      Ted\n      NaN\n      designer\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Name\n      Sex\n      Age\n    \n    \n      PassengerId\n      \n      \n      \n    \n  \n  \n    \n      1\n      Braund, Mr. Owen Harris\n      male\n      22.0\n    \n    \n      2\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n    \n    \n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n    \n    \n      4\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n    \n    \n      5\n      Allen, Mr. William Henry\n      male\n      35.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      PassengerId\n      Name\n      Sex\n      Age\n    \n  \n  \n    \n      0\n      1\n      Braund, Mr. Owen Harris\n      male\n      22.0\n    \n    \n      1\n      2\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.0\n    \n    \n      2\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.0\n    \n    \n      3\n      4\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.0\n    \n    \n      4\n      5\n      Allen, Mr. William Henry\n      male\n      35.0\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\nFutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  df.corr()\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      Survived\n      1.000000\n      -0.338481\n      -0.077221\n      -0.035322\n      0.081629\n      0.257307\n    \n    \n      Pclass\n      -0.338481\n      1.000000\n      -0.369226\n      0.083081\n      0.018443\n      -0.549500\n    \n    \n      Age\n      -0.077221\n      -0.369226\n      1.000000\n      -0.308247\n      -0.189119\n      0.096067\n    \n    \n      SibSp\n      -0.035322\n      0.083081\n      -0.308247\n      1.000000\n      0.414838\n      0.159651\n    \n    \n      Parch\n      0.081629\n      0.018443\n      -0.189119\n      0.414838\n      1.000000\n      0.216225\n    \n    \n      Fare\n      0.257307\n      -0.549500\n      0.096067\n      0.159651\n      0.216225\n      1.000000\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\n\n\nSurvived      0\nPclass        0\nName          0\nSex           0\nAge         177\nSibSp         0\nParch         0\nTicket        0\nFare          0\nCabin       687\nEmbarked      2\ndtype: int64\n\n\n결측치를 무작정 제거하는건 좋은 생각이 아님\n탑승객들의 나이는 어떤식으로 결측치 처리 할 것인가\n\n# 그냥 평균으로 넣을 수도 있음.\n\n# df['Age'].fillna(df['Age'].mean())\n\n\n# 생존자 나이 평균\nmean1 = df[df['Survived'] == 1]['Age'].mean()\n\n# 사망자 나이 평균\nmean0 = df[df['Survived'] == 0]['Age'].mean()\n\nprint(mean1, mean0)\n\n28.343689655172415 30.62617924528302\n\n\n\ndf.loc[df['Survived'] == 1, 'Age']\n\nPassengerId\n2      38.0\n3      26.0\n4      35.0\n9      27.0\n10     14.0\n       ... \n876    15.0\n880    56.0\n881    25.0\n888    19.0\n890    26.0\nName: Age, Length: 342, dtype: float64\n\n\n\ndf[df['Survived'] == 1]['Age'] # 생존그룹에서 나이만 추출\n\nPassengerId\n2      38.0\n3      26.0\n4      35.0\n9      27.0\n10     14.0\n       ... \n876    15.0\n880    56.0\n881    25.0\n888    19.0\n890    26.0\nName: Age, Length: 342, dtype: float64\n\n\n\n# 앞에는 생존자 그룹을 분류 / 뒤에는\ndf.loc[df['Survived'] == 1, 'Age'] = df[df['Survived'] == 1]['Age'].fillna(mean1)\ndf.loc[df['Survived'] == 0, 'Age'] = df[df['Survived'] == 0]['Age'].fillna(mean0)\n\n\n\nPython 시각화\n\n%matplotlib inline\n\ns = [1,2,3,4,5,6,5,4,3,2,1]\n\ns1 = pd.Series(s)\ns1.plot()\n\n<Axes: >\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ns = [1,2,3,4,5,6,5,4,3,2,1]\n\nplt.plot(s)\nplt.show()\n\n\n\n\nchat gpt한태 물어보면 엄청 잘 알려줌\n데이터의 종류에 따라서 적절한 시각화 방법이 다른데 이걸 경험적으로 아는게 중요함\n\nimport matplotlib.pyplot as plt\n\n\np1 = ['a', 'b', 'c', 'd', 'e']\ns1 = [1, 2, 3, 4, 5]\ns2 = [2, 4, 3.5, 5, 2]\n\nplt.xlabel('Test')\nplt.ylabel('Value')\nplt.title('Chart')\nplt.plot(p1, s1, 'r')\nplt.bar(p1, s2) # 여기까지 실행하면 그래프가 안그려짐. 하지만 주피터 노트북 환경에선 그려지는데 인터프리터의 특성이 반영된 것임.\nplt.legend(['test1', 'test2']) # 범례\nplt.show()"
  },
  {
    "objectID": "posts/bigleader-01-06/index.html",
    "href": "posts/bigleader-01-06/index.html",
    "title": "빅리더 알고리즘 특강 1주차",
    "section": "",
    "text": "그냥 비상이다… 알고리즘은 답이 없다… - 코딩테스트 수업을 듣고 나서 김민식 -\n\n코딩테스트 준비는 나중에 해야지 하고 미루고 있었는데 더이상 피할수가 없어졌다. 중요하단건 알고 있었는데 내가 생각한것보다 더 중요하네…\n앞에 완전 쉬운 문제에서(별찍기) 바로바로 못풀고 고생할 줄은 꿈에도 몰랐다. 멘토님들이 선정해주신 심화 문제는 두 문제 였는데 한 문제는 손도 못댔다. 평소에도 로직을 짜는것에 약하다고 생각은 하고 있었는데 나 그냥 상상이상이네… 일단 돌아가기만 하면 되는거 아닌가? 라는 안일한 생각부터 고쳐야겠다. 그리고 이왕 주마다 연습하는거 좀 열심히 해야지.\n책 한권하고 온라인 강의도 참고하고 멘토님과 잘하는 친구들 좀 괴롭히면서 빨대좀 꽂아야겠다.\n이번 빅리더 목표에 ‘코딩테스트 좀 친다’ 정도까지 공부하는 것을 추가하고 백준 / 프로그래머스 문제좀 많이 풀어야겠다.\n급해져야지만 시작하는 나는 뭘까…"
  },
  {
    "objectID": "posts/bigleader-01-08/index.html",
    "href": "posts/bigleader-01-08/index.html",
    "title": "빅리더 시각화 체크리스트",
    "section": "",
    "text": "내가 멋있다고 생각하고 follow하는 개발자 분이 정리한 내용을 보고 영감을 얻어 이런 페이지를 한번쯤은 작성하고 있었는데 이번에 기회가 생겨 작성하게 되었다.\n시각화 할 일이 생기게 된다면 이 체크리스트를 펴두고 진행해보자.\n\n시각화 참고 노트\n\n시각화 할때는 색상도 중요함. 발표 대상(기업) 이라던가 주제의 메인 컬러를 잘 이용해야함. 근데 이부분은 디자인적인 영역이 아닌가?\n파이차트 보다는 도넛차트가 좋다. 도넛차트는 가운데 빈 공간도 활용할 수 있기에 정보를 많이 담을 수 있다.\n그래프나 글자의 배치도 신경써야한다. (선거 현수막? 이미지. 후보들 얼굴이 왼 오 왼 오 순으로 있어서 강조해주던 이미지)"
  },
  {
    "objectID": "posts/new-post-06/exercise_interactive_maps.html",
    "href": "posts/new-post-06/exercise_interactive_maps.html",
    "title": "Kaggle_exercise_03",
    "section": "",
    "text": "kaggle 문제풀이 03 https://www.kaggle.com/code/lsiina/exercise-interactive-maps/edit\n\n소개\n당신은 일본의 도시 안전 기획자이며, 일본의 어떤 지역이 지진 강화를 위해 추가적인 조치가 필요한지 분석하고 있습니다. 인구 밀도가 높으면서 지진이 발생하기 쉬운 지역은 어디인가요?\n\n\n\n시작하기 전에 아래의 코드 셀을 실행하여 모든 설정을 완료하세요.\n\n!pip install geopandas\n\n\nimport pandas as pd\nimport geopandas as gpd\n\nimport folium\nfrom folium import Choropleth\nfrom folium.plugins import HeatMap\n\n인터랙티브 지도를 표시하는 embed_map() 함수를 정의합니다. 이 함수는 두 개의 인수를 받습니다: 지도를 담고 있는 변수와 지도가 저장될 HTML 파일의 이름입니다.\n이 함수는 지도가 모든 웹 브라우저에서 표시될 수 있도록 합니다.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='1000px')\n\n\n\n연습문제\n\n1) 지진은 플레이트 경계와 일치하는가?\n아래의 코드 셀을 실행하여 전 세계 플레이트 경계를 보여주는 DataFrame plate_boundaries를 생성하세요. “coordinates” 열은 경계에 따라 (위도, 경도) 위치의 리스트입니다.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\nplate_boundaries = gpd.read_file(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/Plate_Boundaries/Plate_Boundaries.shp\")\nplate_boundaries['coordinates'] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis='columns')\nplate_boundaries.drop('geometry', axis=1, inplace=True)\n\nplate_boundaries.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      HAZ_PLATES\n      HAZ_PLAT_1\n      HAZ_PLAT_2\n      Shape_Leng\n      coordinates\n    \n  \n  \n    \n      0\n      TRENCH\n      SERAM TROUGH (ACTIVE)\n      6722\n      5.843467\n      [(-5.444200361999947, 133.6808931800001), (-5....\n    \n    \n      1\n      TRENCH\n      WETAR THRUST\n      6722\n      1.829013\n      [(-7.760600482999962, 125.47879802900002), (-7...\n    \n    \n      2\n      TRENCH\n      TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ...\n      6621\n      6.743604\n      [(19.817899819000047, 120.09999798800004), (19...\n    \n    \n      3\n      TRENCH\n      BONIN TRENCH\n      9821\n      8.329381\n      [(26.175899215000072, 143.20620700100005), (26...\n    \n    \n      4\n      TRENCH\n      NEW GUINEA TRENCH\n      8001\n      11.998145\n      [(0.41880004000006466, 132.8273013480001), (0....\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음으로, 아래의 코드 셀을 수정하지 말고 실행하여 역사적인 지진 데이터를 DataFrame earthquakes에 로드하세요.\n\n# Load the data and print the first 5 rows\nearthquakes = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/earthquakes1970-2014.csv\", parse_dates=[\"DateTime\"])\nearthquakes.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      DateTime\n      Latitude\n      Longitude\n      Depth\n      Magnitude\n      MagType\n      NbStations\n      Gap\n      Distance\n      RMS\n      Source\n      EventID\n    \n  \n  \n    \n      0\n      1970-01-04 17:00:40.200\n      24.139\n      102.503\n      31.0\n      7.5\n      Ms\n      90.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970010e+09\n    \n    \n      1\n      1970-01-06 05:35:51.800\n      -9.628\n      151.458\n      8.0\n      6.2\n      Ms\n      85.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      2\n      1970-01-08 17:12:39.100\n      -34.741\n      178.568\n      179.0\n      6.1\n      Mb\n      59.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      3\n      1970-01-10 12:07:08.600\n      6.825\n      126.737\n      73.0\n      6.1\n      Mb\n      91.0\n      NaN\n      NaN\n      0.0\n      NEI\n      1.970011e+09\n    \n    \n      4\n      1970-01-16 08:05:39.000\n      60.280\n      -152.660\n      85.0\n      6.0\n      ML\n      0.0\n      NaN\n      NaN\n      NaN\n      AK\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n아래의 코드 셀은 지도 상에 플레이트 경계를 시각화합니다. 플레이트 경계와 지진이 일치하는지 확인하기 위해 모든 지진 데이터를 사용하여 같은 지도에 히트맵을 추가하세요.\n\n# Create a base map with plate boundaries\nm_1 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_1)\n\n# Your code here: Add a heatmap to the map\nHeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=15).add_to(m_1)\n\n# Uncomment to see a hint\n#q_1.a.hint()\n\n# Show the map\nembed_map(m_1, 'q_1.html')\n\n\n        \n        \n\n\n위의 지도를 기반으로 지진이 플레이트 경계와 일치하는지 확인해보면 어떻게 되나요?\n\n\n2) 일본에서 지진 깊이와 판 경계에 근접하는 것 사이에 관계가 있습니까?\n일본에서 지진의 깊이와 판 경계와의 근접성 사이에 관련이 있는지 궁금합니다. 최근에 읽은 바에 따르면, 지진의 깊이는 지구의 구조에 대한 중요한 정보를 제공한다고 합니다. 전 세계적으로 흥미로운 패턴이 있는지 확인하고, 또한 일본에서 깊이가 어떻게 변하는지 이해하고 싶습니다.\n\n# Create a base map with plate boundaries\nm_2 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\nfor i in range(len(plate_boundaries)):\n    folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color='black').add_to(m_2)\n    \n# Your code here: Add a map to visualize earthquake depth\n# Custom function to assign a color to each circle\ndef color_producer(val):\n    if val < 50:\n        return 'forestgreen'\n    elif val < 100:\n        return 'darkorange'\n    else:\n        return 'darkred'\n\n# Add a map to visualize earthquake depth\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        radius=2000,\n        color=color_producer(earthquakes.iloc[i]['Depth'])).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# View the map\nembed_map(m_2, 'q_2.html')\n\n\n        \n        \n\n\n판 경계와 지진의 깊이 사이에 관계를 발견할 수 있나요? 이러한 패턴은 전 세계적으로 유지되나요? 일본에서도 마찬가지인가요?\n\n\n3) 인구 밀도가 높은 현은 어디입니까?\n다음 코드 셀을 실행하여 (변경하지 않고) 일본 지방의 지리적 경계를 포함하는 GeoDataFrame prefectures를 생성하세요.\n\n# GeoDataFrame with prefecture boundaries\nprefectures = gpd.read_file(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/japan-prefecture-boundaries/japan-prefecture-boundaries.shp\")\nprefectures.set_index('prefecture', inplace=True)\nprefectures.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      geometry\n    \n    \n      prefecture\n      \n    \n  \n  \n    \n      Aichi\n      MULTIPOLYGON (((137.09523 34.65330, 137.09546 ...\n    \n    \n      Akita\n      MULTIPOLYGON (((139.55725 39.20330, 139.55765 ...\n    \n    \n      Aomori\n      MULTIPOLYGON (((141.39860 40.92472, 141.39806 ...\n    \n    \n      Chiba\n      MULTIPOLYGON (((139.82488 34.98967, 139.82434 ...\n    \n    \n      Ehime\n      MULTIPOLYGON (((132.55859 32.91224, 132.55904 ...\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음 코드 셀은 일본 각 지방의 인구, 면적 (제곱 킬로미터), 인구 밀도 (제곱 킬로미터당)를 포함하는 DataFrame stats를 생성합니다. 코드 셀을 수정하지 않고 실행하세요.\n\n# DataFrame containing population of each prefecture\npopulation = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/japan-prefecture-population.csv\")\npopulation.set_index('prefecture', inplace=True)\n\n# Calculate area (in square kilometers) of each prefecture\narea_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name='area_sqkm')\nstats = population.join(area_sqkm)\n\n# Add density (per square kilometer) of each prefecture\nstats['density'] = stats[\"population\"] / stats[\"area_sqkm\"]\nstats.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      population\n      area_sqkm\n      density\n    \n    \n      prefecture\n      \n      \n      \n    \n  \n  \n    \n      Tokyo\n      12868000\n      1800.614782\n      7146.448049\n    \n    \n      Kanagawa\n      8943000\n      2383.038975\n      3752.771186\n    \n    \n      Osaka\n      8801000\n      1923.151529\n      4576.342460\n    \n    \n      Aichi\n      7418000\n      5164.400005\n      1436.372085\n    \n    \n      Saitama\n      7130000\n      3794.036890\n      1879.264806\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음 코드 셀을 사용하여 인구 밀도를 시각화하는 동적 지도를 생성하세요.\n\n# Create a base map\nm_3 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a choropleth map to visualize population density\nChoropleth(geo_data=prefectures['geometry'].__geo_interface__,\n           data=stats['density'],\n           key_on=\"feature.id\",\n           fill_color='YlGnBu',\n           legend_name='Population density (per square kilometer)'\n          ).add_to(m_3)\n\n# View the map\nembed_map(m_3, 'q_3.html')\n\n\n        \n        \n\n\n세 개의 지방은 다른 지역보다 상대적으로 더 높은 인구 밀도를 가지고 있나요? 그 지역은 전국적으로 분산되어 있나요, 아니면 거의 동일한 지리적 지역에 위치해 있나요? (일본 지리에 익숙하지 않은 경우, 이 링크를 사용하여 질문에 대답하는 데 도움이 될 수 있습니다.)\n\n\n4) 고밀도 지방 중에서 강진이 높은 지방은 어디인가요?\n방진 보강이 필요한 지방을 제안하기 위해 맵을 생성하세요. 맵은 인구 밀도와 지진 규모를 시각화해야 합니다.\n번역된 내용에 맞게 이해하셨다면, 다음 코드 셀을 실행하여 고밀도와 지진 규모를 시각화하는 맵을 생성하세요.\n\n# Create a base map\nm_4 = folium.Map(location=[35,136], tiles='cartodbpositron', zoom_start=5)\n\n# Your code here: create a map\n# Create a map\ndef color_producer(magnitude):\n    if magnitude > 6.5:\n        return 'red'\n    else:\n        return 'green'\n\nChoropleth(\n    geo_data=prefectures['geometry'].__geo_interface__,\n    data=stats['density'],\n    key_on=\"feature.id\",\n    fill_color='BuPu',\n    legend_name='Population density (per square kilometer)').add_to(m_4)\n\nfor i in range(0,len(earthquakes)):\n    folium.Circle(\n        location=[earthquakes.iloc[i]['Latitude'], earthquakes.iloc[i]['Longitude']],\n        popup=(\"{} ({})\").format(\n            earthquakes.iloc[i]['Magnitude'],\n            earthquakes.iloc[i]['DateTime'].year),\n        radius=earthquakes.iloc[i]['Magnitude']**5.5,\n        color=color_producer(earthquakes.iloc[i]['Magnitude'])).add_to(m_4)\n        \n# Uncomment to see a hint\n#q_4.a.hint()\n\n# View the map\nembed_map(m_4, 'q_4.html')\n\n\n        \n        \n\n\n추가 지진 보강을 위해 어떤 도를 추천하시겠습니까?\n인구 밀도와 지진 규모를 시각화한 지도를 기반으로, 저는 추가 지진 보강을 위해 도쿄도를 추천합니다. 도쿄는 인구 밀도가 높고 고규모 지진이 발생하기 쉬운 지역입니다. 따라서 도쿄에서 지진 보강 조치에 우선순위를 두어 인구의 안전과 회복력을 보장하는 것이 매우 중요합니다."
  },
  {
    "objectID": "posts/bigleader-01-12/index.html",
    "href": "posts/bigleader-01-12/index.html",
    "title": "빅리더 사이드 프로젝트",
    "section": "",
    "text": "빅리더 수업을 들으면서, 특히 여러 분야의 멘토분들의 특강을 들으며 사이드 프로젝트를 진행해야겠다고 마음먹었다.\n여러 멘토님들이 공통점으로 강조해주신 것은 프로젝트 경험. ‘하나를 하더라도 제대로 된 프로젝트를 할 것’ 이고 이말이 내게는 크게 다가왔다.\n컴공을 졸업할때까지 학교 과제나 팀프로젝트 말고는 따로 프로젝트를 진행한 경험이 하나도 없어서 내가 실제로 서비스를 만들 수 있는지도 의문이고 이번 빅리더를 통해 꼭 보여주고 싶은 모습들이 있어서 지금이라도 시작해야겠다는 생각이 들었다.\n\n뭘 만들어야 할까?\n본격적인 프로젝트에 들어가기 까지는 짧게는 1달, 길게는 2달 까지의 시간이 있다.\n이론 수업 이후에 어떤 프로젝트를 진행하게 될 진 모르겠지만 RPA 프로젝트가 아닌 이상 웹사이트를 통해 우리가 만든 프로젝트를 시각적으로 멋있게 누구나 자유롭게 접속해서 이용 할 수 있는걸 보여주고자 하는 목표가 있다.\n프로젝트 형태나 결과물에 따라 어떻게 보여줄지는 조금씩 바뀔지 모르겠지만 개인적인 욕심으로는 리엑트와 node.js 를 이용하여 웹 상으로 보여주는 것이 내 목표이다.\n작성중…"
  },
  {
    "objectID": "posts/new-post-01/index.html",
    "href": "posts/new-post-01/index.html",
    "title": "새로 추가한 포스팅3333",
    "section": "",
    "text": "새로 추가한 포스팅입니다."
  },
  {
    "objectID": "posts/new-post-08/exercise_proximity_analysis.html",
    "href": "posts/new-post-08/exercise_proximity_analysis.html",
    "title": "Kaggle_exercise_05",
    "section": "",
    "text": "kaggle 문제풀이 05 https://www.kaggle.com/code/lsiina/exercise-proximity-analysis/edit\n\n소개\n당신은 위기 대응 팀의 일원으로서 뉴욕시에서 교통사고 대응에 어떻게 병원들이 대응하고 있는지 파악하고자 합니다.\n\n\n\n시작하기 전에 아래의 코드 셀을 실행하여 준비를 완료하세요.\n\n!pip install geopandas\n\n\nimport math\nimport geopandas as gpd\nimport pandas as pd\nfrom shapely.geometry import MultiPolygon\n\nimport folium\nfrom folium import Choropleth, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\n\nembed_map() 함수를 사용하여 지도를 시각화할 것입니다.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\n연습\n\n1) 충돌 데이터 시각화하기\n아래의 코드 셀을 실행하여 2013-2018년 동안의 주요 차량 충돌을 추적하는 GeoDataFrame인 collisions을 로드하세요.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\ncollisions = gpd.read_file(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp\")\ncollisions.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      DATE\n      TIME\n      BOROUGH\n      ZIP CODE\n      LATITUDE\n      LONGITUDE\n      LOCATION\n      ON STREET\n      CROSS STRE\n      OFF STREET\n      ...\n      CONTRIBU_2\n      CONTRIBU_3\n      CONTRIBU_4\n      UNIQUE KEY\n      VEHICLE TY\n      VEHICLE _1\n      VEHICLE _2\n      VEHICLE _3\n      VEHICLE _4\n      geometry\n    \n  \n  \n    \n      0\n      07/30/2019\n      0:00\n      BRONX\n      10464\n      40.841100\n      -73.784960\n      (40.8411, -73.78496)\n      NaN\n      NaN\n      121       PILOT STREET\n      ...\n      Unspecified\n      NaN\n      NaN\n      4180045\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      POINT (1043750.211 245785.815)\n    \n    \n      1\n      07/30/2019\n      0:10\n      QUEENS\n      11423\n      40.710827\n      -73.770660\n      (40.710827, -73.77066)\n      JAMAICA AVENUE\n      188 STREET\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      4180007\n      Sedan\n      Sedan\n      NaN\n      NaN\n      NaN\n      POINT (1047831.185 198333.171)\n    \n    \n      2\n      07/30/2019\n      0:25\n      NaN\n      NaN\n      40.880318\n      -73.841286\n      (40.880318, -73.841286)\n      BOSTON ROAD\n      NaN\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      4179575\n      Sedan\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      NaN\n      POINT (1028139.293 260041.178)\n    \n    \n      3\n      07/30/2019\n      0:35\n      MANHATTAN\n      10036\n      40.756744\n      -73.984590\n      (40.756744, -73.98459)\n      NaN\n      NaN\n      155       WEST 44 STREET\n      ...\n      NaN\n      NaN\n      NaN\n      4179544\n      Box Truck\n      Station Wagon/Sport Utility Vehicle\n      NaN\n      NaN\n      NaN\n      POINT (988519.261 214979.320)\n    \n    \n      4\n      07/30/2019\n      10:00\n      BROOKLYN\n      11223\n      40.600090\n      -73.965910\n      (40.60009, -73.96591)\n      AVENUE T\n      OCEAN PARKWAY\n      NaN\n      ...\n      NaN\n      NaN\n      NaN\n      4180660\n      Station Wagon/Sport Utility Vehicle\n      Bike\n      NaN\n      NaN\n      NaN\n      POINT (993716.669 157907.212)\n    \n  \n\n5 rows × 30 columns\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n“LATITUDE” 및 “LONGITUDE” 열을 사용하여 충돌 데이터를 시각화하는 대화형 맵을 생성하세요. 어떤 유형의 맵이 가장 효과적인지 어떻게 생각하시나요?\n\nm_1 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the collision data\nHeatMap(data=collisions[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_1)\n\n<folium.plugins.heat_map.HeatMap at 0x7fedf36b9480>\n\n\n\n\n2) 병원 보호범위를 파악하기\n다음 코드 셀을 실행하여 병원 데이터를 불러옵니다.\n\nhospitals = gpd.read_file(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/nyu_2451_34494/nyu_2451_34494.shp\")\nhospitals.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      id\n      name\n      address\n      zip\n      factype\n      facname\n      capacity\n      capname\n      bcode\n      xcoord\n      ycoord\n      latitude\n      longitude\n      geometry\n    \n  \n  \n    \n      0\n      317000001H1178\n      BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI...\n      1650 Grand Concourse\n      10457\n      3102\n      Hospital\n      415\n      Beds\n      36005\n      1008872.0\n      246596.0\n      40.843490\n      -73.911010\n      POINT (1008872.000 246596.000)\n    \n    \n      1\n      317000001H1164\n      BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION\n      1276 Fulton Ave\n      10456\n      3102\n      Hospital\n      164\n      Beds\n      36005\n      1011044.0\n      242204.0\n      40.831429\n      -73.903178\n      POINT (1011044.000 242204.000)\n    \n    \n      2\n      317000011H1175\n      CALVARY HOSPITAL INC\n      1740-70 Eastchester Rd\n      10461\n      3102\n      Hospital\n      225\n      Beds\n      36005\n      1027505.0\n      248287.0\n      40.848060\n      -73.843656\n      POINT (1027505.000 248287.000)\n    \n    \n      3\n      317000002H1165\n      JACOBI MEDICAL CENTER\n      1400 Pelham Pkwy\n      10461\n      3102\n      Hospital\n      457\n      Beds\n      36005\n      1027042.0\n      251065.0\n      40.855687\n      -73.845311\n      POINT (1027042.000 251065.000)\n    \n    \n      4\n      317000008H1172\n      LINCOLN MEDICAL & MENTAL HEALTH CENTER\n      234 E 149 St\n      10451\n      3102\n      Hospital\n      362\n      Beds\n      36005\n      1005154.0\n      236853.0\n      40.816758\n      -73.924478\n      POINT (1005154.000 236853.000)\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n“latitude”와 “longitude” 열을 사용하여 병원 위치를 시각화하세요.\n\nm_2 = folium.Map(location=[40.7, -74], zoom_start=11) \n\n# Your code here: Visualize the hospital locations\nfor idx, row in hospitals.iterrows():\n    Marker([row['latitude'], row['longitude']], popup=row['name']).add_to(m_2)\n\n\n\n3) 가장 가까운 병원으로부터 10킬로미터 이상 떨어진 사고는 언제였나요?\n가장 가까운 병원으로부터 10킬로미터 이상 떨어진 위치에서 발생한 충돌 사고의 모든 행을 포함하는 DataFrame인 outside_range을 생성하세요. 이때, hospitals와 collisions 모두 좌표 참조 시스템으로 EPSG 2263을 사용하며, EPSG 2263은 미터 단위를 가지고 있습니다.\n\n# Your code here\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nmy_union = coverage.geometry.unary_union\n\noutside_range = collisions.loc[~collisions[\"geometry\"].apply(lambda x: my_union.contains(x))]\n\n다음 코드 셀은 가장 가까운 병원으로부터 10킬로미터 이상 떨어진 충돌의 범위를 계산합니다\n\npercentage = round(100*len(outside_range)/len(collisions), 2)\nprint(\"Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(percentage))\n\nPercentage of collisions more than 10 km away from the closest hospital: 15.12%\n\n\n\n\n4) 추천 시스템 만들기\n먼 위치에서 충돌이 발생할 때는 부상자를 가장 가까운 이용 가능한 병원으로 이송하는 것이 더욱 중요합니다.\n이를 고려하여 다음과 같은 추천 시스템을 만들기로 결정했습니다: - 충돌 위치 (EPSG 2263에서)를 입력으로 받습니다. - 가장 가까운 병원을 찾습니다 (거리 계산은 EPSG 2263에서 수행). - 가장 가까운 병원의 이름을 반환합니다.\n\ndef best_hospital(collision_location):\n    idx_min = hospitals.geometry.distance(collision_location).idxmin()\n    my_hospital = hospitals.iloc[idx_min]\n    name = my_hospital[\"name\"]\n    return name\n\n# Test your function: this should suggest CALVARY HOSPITAL INC\nprint(best_hospital(outside_range.geometry.iloc[0]))\n\nCALVARY HOSPITAL INC\n\n\n\n\n5) 가장 많은 수요를 받는 병원은 어디인가요?\noutside_range DataFrame에서만 고려할 때, 가장 추천되는 병원은 어디인가요? 답변은 4)에서 생성한 함수에 의해 반환된 병원 이름과 정확히 일치하는 Python 문자열이어야 합니다.\n\n# Your code here\nhighest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax()\n\n\n\n6) 도시는 어디에 새로운 병원을 건설해야 할까요?\n다음 코드 셀을 실행하여 병원 위치를 시각화합니다. 이에 추가로, 가장 가까운 병원으로부터 10킬로미터 이상 떨어진 위치에서 발생한 사고도 함께 표시됩니다.\n\nm_6 = folium.Map(location=[40.7, -74], zoom_start=11) \n\ncoverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000)\nfolium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6)\nHeatMap(data=outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m_6)\nfolium.LatLngPopup().add_to(m_6)\n\nembed_map(m_6, 'm_6.html')\n\n\n        \n        \n\n\n지도에서 아무 곳이나 클릭하여 해당 위치의 위도와 경도가 표시되는 팝업을 볼 수 있습니다. 뉴욕 시는 3) 단계에서 계산된 백분율을 10% 미만으로 낮추기 위해 두 개의 새로운 병원 위치를 식별하는 데 도움을 요청했습니다. 지도를 사용하여 (지역 법률이나 병원을 건설하기 위해 제거해야 할 잠재적인 건물에 대해 걱정하지 않고) 이 목표를 달성하는 데 도움이 될 수 있는 두 개의 위치를 찾을 수 있을까요? 병원 1의 제안된 위도와 경도를 각각 lat_1과 long_1에 입력해주세요. (병원 2도 마찬가지로 입력해주세요.) 그런 다음 나머지 셀을 그대로 실행하여 새로운 병원의 효과를 확인하세요. 답변이 정답으로 표시될 것입니다. 새로운 두 병원이 백분율을 10% 미만으로 낮추게 됩니다.\n\n# Your answer here: proposed location of hospital 1\nlat_1 = 40.6714\nlong_1 = -73.8492\n\n# Your answer here: proposed location of hospital 2\nlat_2 = 40.6702\nlong_2 = -73.7612\n\n# Do not modify the code below this line\ntry:\n    new_df = pd.DataFrame(\n        {'Latitude': [lat_1, lat_2],\n         'Longitude': [long_1, long_2]})\n    new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude))\n    new_gdf.crs = {'init' :'epsg:4326'}\n    new_gdf = new_gdf.to_crs(epsg=2263)\n    # get new percentage\n    new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000)\n    new_my_union = new_coverage.geometry.unary_union\n    new_outside_range = outside_range.loc[~outside_range[\"geometry\"].apply(lambda x: new_my_union.contains(x))]\n    new_percentage = round(100*len(new_outside_range)/len(collisions), 2)\n    print(\"(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%\".format(new_percentage))\n    # Did you help the city to meet its goal?\n    q_6.check()\n    # make the map\n    m = folium.Map(location=[40.7, -74], zoom_start=11) \n    folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m)\n    for idx, row in new_gdf.iterrows():\n        Marker([row['Latitude'], row['Longitude']]).add_to(m)\n    HeatMap(data=new_outside_range[['LATITUDE', 'LONGITUDE']], radius=9).add_to(m)\n    folium.LatLngPopup().add_to(m)\n    display(embed_map(m, 'q_6.html'))\nexcept:\n    print(\"error.\")\n    #q_6.hint()\n\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n(NEW) Percentage of collisions more than 10 km away from the closest hospital: 9.12%\nerror."
  },
  {
    "objectID": "posts/new-post-07/exercise_manipulating_geospatial_data.html",
    "href": "posts/new-post-07/exercise_manipulating_geospatial_data.html",
    "title": "Kaggle_exercise_04",
    "section": "",
    "text": "kaggle 문제풀이 04 https://www.kaggle.com/code/lsiina/exercise-manipulating-geospatial-data/edit\n\n소개\n당신은 스타벅스의 빅데이터 분석가입니다 (실제로 존재하는 직업입니다!). 당신은 스타벅스 리저브 로스터리에 다음 매장을 찾기 위해 캘리포니아 주의 다양한 군지역의 인구 통계를 조사할 것입니다.\n\n\n\n시작하기 전에, 아래의 코드 셀을 실행하여 모든 설정을 완료하세요.\n\n!pip install geopandas\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting geopandas\n  Downloading geopandas-0.13.0-py3-none-any.whl (1.1 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 15.7 MB/s eta 0:00:00\nCollecting fiona>=1.8.19 (from geopandas)\n  Downloading Fiona-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.5/16.5 MB 47.2 MB/s eta 0:00:00\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from geopandas) (23.1)\nRequirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (1.5.3)\nCollecting pyproj>=3.0.1 (from geopandas)\n  Downloading pyproj-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.7 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.7/7.7 MB 49.6 MB/s eta 0:00:00\nRequirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.0.1)\nRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (23.1.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (2022.12.7)\nRequirement already satisfied: click~=8.0 in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (8.1.3)\nCollecting click-plugins>=1.0 (from fiona>=1.8.19->geopandas)\n  Downloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\nCollecting cligj>=0.5 (from fiona>=1.8.19->geopandas)\n  Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (2022.7.1)\nRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.0->geopandas) (1.22.4)\nInstalling collected packages: pyproj, cligj, click-plugins, fiona, geopandas\nSuccessfully installed click-plugins-1.1.1 cligj-0.7.2 fiona-1.9.4 geopandas-0.13.0 pyproj-3.5.0\n\n\n\nimport math\nimport pandas as pd\nimport geopandas as gpd\nfrom geopy.geocoders import Nominatim            # What you'd normally run\n#from learntools.geospatial.tools import Nominatim # Just for this exercise\n\nimport folium \nfrom folium import Marker\nfrom folium.plugins import MarkerCluster\n\n이전 연습에서 사용한 embed_map() 함수를 사용하여 지도를 시각화합니다.\n\ndef embed_map(m, file_name):\n    from IPython.display import IFrame\n    m.save(file_name)\n    return IFrame(file_name, width='100%', height='500px')\n\n\n\n연습\n\n1) 누락된 위치에 대해 지오코딩하기\n다음 코드 셀을 실행하여 캘리포니아 주에 있는 스타벅스 매장을 포함하는 DataFrame starbucks를 생성합니다.\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n# Load and preview Starbucks locations in California\nstarbucks = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/starbucks_locations.csv\")\nstarbucks.head()\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      0\n      10429-100710\n      Palmdale & Hwy 395\n      14136 US Hwy 395 Adelanto CA\n      Adelanto\n      -117.40\n      34.51\n    \n    \n      1\n      635-352\n      Kanan & Thousand Oaks\n      5827 Kanan Road Agoura CA\n      Agoura\n      -118.76\n      34.16\n    \n    \n      2\n      74510-27669\n      Vons-Agoura Hills #2001\n      5671 Kanan Rd. Agoura Hills CA\n      Agoura Hills\n      -118.76\n      34.15\n    \n    \n      3\n      29839-255026\n      Target Anaheim T-0677\n      8148 E SANTA ANA CANYON ROAD AHAHEIM CA\n      AHAHEIM\n      -117.75\n      33.87\n    \n    \n      4\n      23463-230284\n      Safeway - Alameda 3281\n      2600 5th Street Alameda CA\n      Alameda\n      -122.28\n      37.79\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n대부분의 매장은 알려진 (위도, 경도) 위치를 가지고 있습니다. 그러나 버클리 시의 모든 위치는 누락되어 있습니다.\n\n# How many rows in each column have missing values?\nprint(starbucks.isnull().sum())\n\n# View rows with missing locations\nrows_with_missing = starbucks[starbucks[\"City\"]==\"Berkeley\"]\nrows_with_missing\n\nStore Number    0\nStore Name      0\nAddress         0\nCity            0\nLongitude       5\nLatitude        5\ndtype: int64\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      Store Number\n      Store Name\n      Address\n      City\n      Longitude\n      Latitude\n    \n  \n  \n    \n      153\n      5406-945\n      2224 Shattuck - Berkeley\n      2224 Shattuck Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      154\n      570-512\n      Solano Ave\n      1799 Solano Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      155\n      17877-164526\n      Safeway - Berkeley #691\n      1444 Shattuck Place Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      156\n      19864-202264\n      Telegraph & Ashby\n      3001 Telegraph Avenue Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n    \n      157\n      9217-9253\n      2128 Oxford St.\n      2128 Oxford Street Berkeley CA\n      Berkeley\n      NaN\n      NaN\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n아래 코드 셀을 사용하여 Nominatim 지오코더로 이러한 값을 채워 넣을 수 있습니다.\n참고로 튜토리얼에서는 geopy.geocoders의 Nominatim()을 사용하여 값을 지오코딩했습니다. 이것은 이 강의 외부에서도 사용할 수 있는 방법입니다.\n이 연습에서는 약간 다른 함수 learntools.geospatial.tools의 Nominatim()을 사용합니다. 이 함수는 노트북 상단에서 가져온 것이며, GeoPandas의 함수와 동일하게 작동합니다.\n다시 말해 다음 조건을 만족한다면: - 노트북 상단의 import 문을 변경하지 않고, - 아래 코드 셀에서 지오코딩 함수를 geocode()로 호출한다면,\n원하는 대로 코드가 작동할 것입니다!\n\n# Create the geocoder\ngeolocator = Nominatim(user_agent=\"kaggle_learn\")\n\n# Your code here\ndef my_geocoder(row):\n    point = geolocator.geocode(row).point\n    return pd.Series({'Latitude': point.latitude, 'Longitude': point.longitude})\n\nberkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x['Address']), axis=1)\nstarbucks.update(berkeley_locations)\n\n\n\n2) 버클리 위치를 봅시다.\n다음 코드 셀을 사용하여 방금 찾은 버클리의 (위도, 경도) 위치를 OpenStreetMap 스타일로 시각화해 봅시다.\n\n# Create a base map\nm_2 = folium.Map(location=[37.88,-122.26], zoom_start=13)\n\n# Your code here: Add a marker for each Berkeley location\n# Add a marker for each Berkeley location\nfor idx, row in starbucks[starbucks[\"City\"]=='Berkeley'].iterrows():\n    Marker([row['Latitude'], row['Longitude']]).add_to(m_2)\n\n# Uncomment to see a hint\n#q_2.a.hint()\n\n# Show the map\nembed_map(m_2, 'q_2.html')\n\n\n        \n        \n\n\n버클리의 다섯 개 위치 중 얼마나 많은 (위도, 경도) 위치가 잠재적으로 올바른 위치로 보입니다(올바른 도시에 위치해 있는지)?\n\n# Solution: All five locations appear to be correct!\n\n\n\n3) 데이터를 통합합시다.\n아래의 코드를 실행하여 캘리포니아 주의 각 카운티에 대한 이름, 면적(제곱 킬로미터 단위) 및 고유 ID(“GEOID” 열)를 포함한 GeoDataFrame ’CA_counties’를 로드하세요. “geometry” 열에는 카운티 경계를 나타내는 폴리곤이 포함되어 있습니다.\n\nCA_counties = gpd.read_file(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/CA_county_boundaries/CA_county_boundaries.shp\")\nCA_counties.crs = {'init': 'epsg:4326'}\nCA_counties.head()\n\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n\n\n  \n    \n      \n\n\n  \n    \n      \n      GEOID\n      name\n      area_sqkm\n      geometry\n    \n  \n  \n    \n      0\n      6091\n      Sierra County\n      2491.995494\n      POLYGON ((-120.65560 39.69357, -120.65554 39.6...\n    \n    \n      1\n      6067\n      Sacramento County\n      2575.258262\n      POLYGON ((-121.18858 38.71431, -121.18732 38.7...\n    \n    \n      2\n      6083\n      Santa Barbara County\n      9813.817958\n      MULTIPOLYGON (((-120.58191 34.09856, -120.5822...\n    \n    \n      3\n      6009\n      Calaveras County\n      2685.626726\n      POLYGON ((-120.63095 38.34111, -120.63058 38.3...\n    \n    \n      4\n      6111\n      Ventura County\n      5719.321379\n      MULTIPOLYGON (((-119.63631 33.27304, -119.6360...\n    \n  \n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n다음으로, 세 개의 DataFrame을 생성합니다: - CA_pop는 각 카운티의 인구 추정치를 포함합니다. - CA_high_earners는 연간 소득이 최소 $150,000인 가구 수를 포함합니다. - CA_median_age는 각 카운티의 중간 연령을 포함합니다.\n\nCA_pop = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/CA_county_population.csv\", index_col=\"GEOID\")\nCA_high_earners = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/CA_county_high_earners.csv\", index_col=\"GEOID\")\nCA_median_age = pd.read_csv(\"/content/drive/MyDrive/2023 데이터마이닝/dataset/CA_county_median_age.csv\", index_col=\"GEOID\")\n\n다음 코드 셀을 사용하여 CA_counties GeoDataFrame을 CA_pop, CA_high_earners, CA_median_age와 조인합니다.\n결과 GeoDataFrame의 이름을 CA_stats로 지정하고, “GEOID”, “name”, “area_sqkm”, “geometry”, “population”, “high_earners”, “median_age”의 8개 열이 있는지 확인하세요.\n\n# Your code here\ncols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index()\nCA_stats = CA_counties.merge(cols_to_add, on=\"GEOID\")\n\n이제 모든 데이터를 하나의 장소에 모았으므로, 여러 열을 조합하여 통계를 계산하는 것이 훨씬 쉬워졌습니다. 다음 코드 셀을 실행하여 “density” 열을 생성하여 인구 밀도를 계산하세요.\n\nCA_stats[\"density\"] = CA_stats[\"population\"] / CA_stats[\"area_sqkm\"]\n\n\n\n4) 어떤 지역이 유망해 보일까요?\n모든 정보를 하나의 GeoDataFrame으로 병합하면 특정 기준을 충족하는 지역을 선택하는 것이 훨씬 쉬워집니다.\n다음 코드 셀을 사용하여 CA_stats GeoDataFrame에서 일부 행(및 모든 열)을 포함하는 GeoDataFrame sel_counties를 생성하세요. 특히 다음 조건을 충족하는 지역을 선택해야 합니다. - 연간 $150,000 이상의 소득을 가진 가구가 적어도 100,000개 이상 있는 경우 - 중간 연령이 38.5보다 작은 경우 - 주민 밀도가 285 이상 (평방 킬로미터당)\n또한, 선택한 지역은 다음 기준 중 하나 이상을 충족해야 합니다. - 연간 $150,000 이상의 소득을 가진 가구가 적어도 500,000개 이상 있는 경우 - 중간 연령이 35.5보다 작은 경우 - 주민 밀도가 1400 이상 (평방 킬로미터당)\n\n# Your code here\nsel_counties = CA_stats[((CA_stats.high_earners > 100000) &\n                         (CA_stats.median_age < 38.5) &\n                         (CA_stats.density > 285) &\n                         ((CA_stats.median_age < 35.5) |\n                         (CA_stats.density > 1400) |\n                         (CA_stats.high_earners > 500000)))]\n\n\n\n5) 얼마나 많은 매장을 찾았나요?\n다음 Starbucks Reserve Roastery 위치를 찾을 때, 선택한 지역 내의 모든 매장을 고려하고 싶습니다. 따라서 선택한 지역 내에 몇 개의 매장이 있는지 알아보려고 합니다.\n이 질문에 답하기 위해, 다음 코드 셀을 실행하여 모든 스타벅스 위치를 포함하는 GeoDataFrame starbucks_gdf를 생성하세요.\n\nstarbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude))\nstarbucks_gdf.crs = {'init': 'epsg:4326'}\n\n/usr/local/lib/python3.10/dist-packages/pyproj/crs/crs.py:141: FutureWarning: '+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n  in_crs_string = _prepare_from_proj_string(in_crs_string)\n\n\n당신이 선택한 지역에는 몇 개의 스타벅스 매장이 있나요?\n\n# Fill in your answer\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nnum_stores = len(locations_of_interest)\nnum_stores\n\n1043\n\n\n\n\n6) 매장 위치 시각화하기.\n이전 질문에서 확인한 매장들의 위치를 보여주는 지도를 생성하세요.\n\n# Create a base map\nm_6 = folium.Map(location=[37,-120], zoom_start=6)\n\n# Your code here: show selected store locations\nmc = MarkerCluster()\n\nlocations_of_interest = gpd.sjoin(starbucks_gdf, sel_counties)\nfor idx, row in locations_of_interest.iterrows():\n    if not math.isnan(row['Longitude']) and not math.isnan(row['Latitude']):\n        mc.add_child(folium.Marker([row['Latitude'], row['Longitude']]))\n\nm_6.add_child(mc)\n\nembed_map(m_6, 'q_6.html')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "minstar",
    "section": "",
    "text": "기초 통계 정리\n\n\n\n\n\n\n\nbigleader\n\n\n기초통계\n\n\n\n\n\n\n\n\n\n\n\nJul 18, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n블로그 개발 방향\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 사이드 프로젝트\n\n\n\n\n\n\n\nbigleader\n\n\nsideproject\n\n\n\n\n\n\n\n\n\n\n\nJul 9, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n류성한 프로님 특강\n\n\n\n\n\n\n\nbigleader\n\n\n특강\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 시각화 체크리스트\n\n\n\n\n\n\n\nbigleader\n\n\n시각화\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 시각화 체크리스트\n\n\n\n\n\n\n\nbigleader\n\n\n시각화\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 시각화 체크리스트\n\n\n\n\n\n\n\nbigleader\n\n\n시각화\n\n\n\n\n\n\n\n\n\n\n\nJul 6, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 시각화 PowerBI & Tableau\n\n\n\n\n\n\n\nbigleader\n\n\n시각화\n\n\n\n\n\n\n\n\n\n\n\nJul 5, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 알고리즘 특강 1주차\n\n\n\n\n\n\n\nps\n\n\nbigleader\n\n\n\n\n\n\n\n\n\n\n\nJul 2, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 1주차 마무리 겸 생각정리\n\n\n\n\n\n\n\nbigleader\n\n\n\n\n\n\n\n\n\n\n\nJul 1, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n빅리더 1주차 02일 지도시각화\n\n\n\n\n\n\n\npython\n\n\nbigleader\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n빅리더 1주차 02일 크롤링\n\n\n\n\n\n\n\npython\n\n\nbigleader\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n빅리더 1주차 02일 타이타닉\n\n\n\n\n\n\n\nbigleader\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n빅리더 1주차 02일 Pandas\n\n\n\n\n\n\n\nbigleader\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJun 27, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n빅리더 프로젝트 목표\n\n\n\n\n\n\n\nbigleader\n\n\n\n\n\n\n\n\n\n\n\nJun 26, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n크롬 공룡 게임 AI\n\n\n\n\n\n\n\npython\n\n\ndl\n\n\n\n\n\n\n\n\n\n\n\nJun 3, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle_exercise_05\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle_exercise_04\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 18, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle_exercise_03\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 17, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle_exercise_02\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2023\n\n\nminstar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKaggle_exercise_01\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nMay 2, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n새로 추가한 포스팅3333\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nApr 4, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n새로 추가한 포스팅22222\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n새로 추가한 포스팅3333\n\n\n\n\n\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nMar 17, 2023\n\n\nminstar\n\n\n\n\n\n\n  \n\n\n\n\n마크다운 문법 연습\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nMar 16, 2023\n\n\nminstar\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "minstar_blog",
    "section": "",
    "text": "About this blog 수업 참여 겸 그날 공부한 내용들을 정리할 블로그.  아래로는 업로드할 내용들 빅리더 수업 코드(python), 코딩테스트 문제풀이(ps), 그냥 내 생각(etc)"
  },
  {
    "objectID": "profile.html",
    "href": "profile.html",
    "title": "Profile",
    "section": "",
    "text": "minstar 한남대 비즈니스통계학과 컴퓨터공학과 다전공…\n\n참여 활동\n\n\n2020 CPD Summer\n\n\n2022 데이터청년캠퍼스 (한남대)\n\n\n2023 빅리더 ai\n\n\nEmail : noveled.siina@gmail.com"
  }
]